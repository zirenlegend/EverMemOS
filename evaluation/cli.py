"""
CLI å…¥å£

è¯„æµ‹æ¡†æ¶çš„å‘½ä»¤è¡Œæ¥å£ã€‚

Usage:
    python -m evaluation.cli --dataset locomo --system evermemos
    python -m evaluation.cli --dataset locomo --system evermemos --smoke 10
    python -m evaluation.cli --dataset locomo --system evermemos --stages search answer evaluate
"""
import asyncio
import argparse
import os
import sys
from pathlib import Path

# ===== ç¯å¢ƒåˆå§‹åŒ– =====
# å¿…é¡»åœ¨å¯¼å…¥ä»»ä½• EverMemOS ç»„ä»¶ä¹‹å‰å®Œæˆ
# å‚è€ƒ src/bootstrap.py çš„åˆå§‹åŒ–é€»è¾‘

# 1. æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.resolve()
src_path = project_root / "src"
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))

# 2. åŠ è½½ç¯å¢ƒå˜é‡
from common_utils.load_env import setup_environment
setup_environment(load_env_file_name=".env", check_env_var="MONGODB_HOST")

# 3. åˆå§‹åŒ–ä¾èµ–æ³¨å…¥å®¹å™¨ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
# æ³¨ï¼šå½“å‰ adapter æ‰‹åŠ¨åˆ›å»ºå¯¹è±¡ï¼Œæš‚ä¸éœ€è¦ DI å®¹å™¨
# ä½†ä¿ç•™æ­¤æ³¨é‡Šï¼Œæœªæ¥å¦‚æœéœ€è¦å¯ä»¥å¯ç”¨ï¼š
# from application_startup import setup_all
# setup_all()

# ===== ç°åœ¨å¯ä»¥å®‰å…¨åœ°å¯¼å…¥ EverMemOS ç»„ä»¶ =====
from evaluation.src.core.loaders import load_dataset
from evaluation.src.core.pipeline import Pipeline
from evaluation.src.adapters.registry import create_adapter
from evaluation.src.evaluators.registry import create_evaluator
from evaluation.src.utils.config import load_yaml
from evaluation.src.utils.logger import get_console

from memory_layer.llm.llm_provider import LLMProvider


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Memory System Evaluation Framework")
    
    parser.add_argument(
        "--dataset",
        type=str,
        required=True,
        help="Dataset name (e.g., locomo)"
    )
    parser.add_argument(
        "--system",
        type=str,
        required=True,
        help="System name (e.g., evermemos)"
    )
    parser.add_argument(
        "--stages",
        nargs="+",
        default=None,
        help="Stages to run (add, search, answer, evaluate). Default: all"
    )
    parser.add_argument(
        "--smoke",
        action="store_true",
        help="Enable smoke test mode (process small dataset for quick validation)"
    )
    parser.add_argument(
        "--smoke-messages",
        type=int,
        default=10,
        help="Smoke test: number of messages to process (use 0 for all). Default: 10"
    )
    parser.add_argument(
        "--smoke-questions",
        type=int,
        default=3,
        help="Smoke test: number of questions to test (use 0 for all). Default: 3"
    )
    parser.add_argument(
        "--run-name",
        type=str,
        default=None,
        help="Run name/version for distinguishing multiple runs (e.g., 'v1', 'baseline', '20241104')"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default=None,
        help="Output directory. Default: results/{dataset}-{system}[-{run_name}]"
    )
    
    args = parser.parse_args()
    
    console = get_console()
    
    # ===== åŠ è½½é…ç½® =====
    console.print("\n[bold cyan]Loading configurations...[/bold cyan]")
    
    evaluation_root = Path(__file__).parent
    
    # åŠ è½½æ•°æ®é›†é…ç½®
    dataset_config_path = evaluation_root / "config" / "datasets" / f"{args.dataset}.yaml"
    if not dataset_config_path.exists():
        console.print(f"[red]âŒ Dataset config not found: {dataset_config_path}[/red]")
        return
    
    dataset_config = load_yaml(str(dataset_config_path))
    console.print(f"  âœ… Loaded dataset config: {args.dataset}")
    
    # åŠ è½½ç³»ç»Ÿé…ç½®
    system_config_path = evaluation_root / "config" / "systems" / f"{args.system}.yaml"
    if not system_config_path.exists():
        console.print(f"[red]âŒ System config not found: {system_config_path}[/red]")
        return
    
    system_config = load_yaml(str(system_config_path))
    console.print(f"  âœ… Loaded system config: {args.system}")
    
    # ===== åŠ è½½æ•°æ®é›† =====
    console.print(f"\n[bold cyan]Loading dataset: {args.dataset}[/bold cyan]")
    
    data_path = dataset_config["data"]["path"]
    if not Path(data_path).is_absolute():
        # ä¼˜å…ˆä» evaluation/data/ åŠ è½½ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä»é¡¹ç›®æ ¹ç›®å½•åŠ è½½
        eval_data_path = evaluation_root / "data" / data_path
        root_data_path = evaluation_root.parent / data_path
        
        if eval_data_path.exists():
            data_path = eval_data_path
            console.print(f"  ğŸ“‚ Using evaluation/data/{data_path}")
        elif root_data_path.exists():
            data_path = root_data_path
            console.print(f"  ğŸ“‚ Using project root data/{data_path}")
        else:
            console.print(f"[red]âŒ Data not found in evaluation/data/ or project root data/[/red]")
            return
    
    # æ™ºèƒ½åŠ è½½ï¼ˆè‡ªåŠ¨è½¬æ¢ï¼‰
    dataset = load_dataset(args.dataset, str(data_path))
    
    console.print(f"  âœ… Loaded {len(dataset.conversations)} conversations, {len(dataset.qa_pairs)} QA pairs")
    
    # ===== ç¡®å®šè¾“å‡ºç›®å½• =====
    if args.output_dir:
        output_dir = Path(args.output_dir)
    else:
        # æ ¹æ®æ˜¯å¦æœ‰ run_name ç”Ÿæˆè¾“å‡ºç›®å½•å
        if args.run_name:
            output_dir = evaluation_root / "results" / f"{args.dataset}-{args.system}-{args.run_name}"
        else:
            output_dir = evaluation_root / "results" / f"{args.dataset}-{args.system}"
    
    # ===== åˆ›å»ºç»„ä»¶ =====
    console.print(f"\n[bold cyan]Initializing components...[/bold cyan]")
    
    # åˆ›å»ºé€‚é…å™¨ï¼ˆä¼ é€’ output_dir ç”¨äºæŒä¹…åŒ–ï¼‰
    adapter = create_adapter(
        system_config["adapter"],
        system_config,
        output_dir=output_dir
    )
    console.print(f"  âœ… Created adapter: {adapter.get_system_info()['name']}")
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = create_evaluator(
        dataset_config["evaluation"]["type"],
        dataset_config["evaluation"]
    )
    console.print(f"  âœ… Created evaluator: {evaluator.get_name()}")
    
    # åˆ›å»º LLM Providerï¼ˆç”¨äºç­”æ¡ˆç”Ÿæˆï¼‰
    llm_config = system_config.get("llm", {})
    llm_provider = LLMProvider(
        provider_type=llm_config.get("provider", "openai"),
        model=llm_config.get("model"),
        api_key=llm_config.get("api_key"),
        base_url=llm_config.get("base_url"),
        temperature=llm_config.get("temperature", 0.0),
        max_tokens=llm_config.get("max_tokens", 32768),
    )
    console.print(f"  âœ… Created LLM provider: {llm_config.get('model')}")
    
    # ===== åˆ›å»º Pipeline =====
    pipeline = Pipeline(
        adapter=adapter,
        evaluator=evaluator,
        llm_provider=llm_provider,
        output_dir=output_dir
    )
    
    console.print(f"  âœ… Created pipeline, output: {output_dir}")
    
    # ===== è¿è¡Œ Pipeline =====
    try:
        results = await pipeline.run(
            dataset=dataset,
            stages=args.stages,
            smoke_test=args.smoke,
            smoke_messages=args.smoke_messages,
            smoke_questions=args.smoke_questions,
        )
        
        console.print(f"\n[bold green]âœ¨ Evaluation completed![/bold green]")
        console.print(f"Results saved to: [cyan]{output_dir}[/cyan]\n")
    
    finally:
        # ===== æ¸…ç†èµ„æº =====
        # å…³é—­ rerank_service çš„ HTTP sessionï¼ˆé¿å… unclosed client session è­¦å‘Šï¼‰
        try:
            from agentic_layer import rerank_service
            reranker = rerank_service.get_rerank_service()
            if hasattr(reranker, 'close') and callable(getattr(reranker, 'close')):
                await reranker.close()
                console.print("[dim]ğŸ§¹ Cleaned up rerank service resources[/dim]")
        except Exception as e:
            # å¦‚æœæ¸…ç†å¤±è´¥ä¹Ÿä¸å½±å“ä¸»æµç¨‹
            console.print(f"[dim]âš ï¸  Failed to cleanup resources: {e}[/dim]")


if __name__ == "__main__":
    asyncio.run(main())

