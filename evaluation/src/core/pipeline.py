"""
Pipeline æ ¸å¿ƒ

è¯„æµ‹æµç¨‹çš„ç¼–æŽ’å™¨ï¼Œè´Ÿè´£åè°ƒ Add â†’ Search â†’ Answer â†’ Evaluate å››ä¸ªé˜¶æ®µã€‚
"""
import asyncio
import time
from pathlib import Path
from typing import List, Dict, Any, Optional

from evaluation.src.core.data_models import (
    Dataset, SearchResult, AnswerResult, EvaluationResult
)
from evaluation.src.adapters.base import BaseAdapter
from evaluation.src.evaluators.base import BaseEvaluator
from evaluation.src.utils.logger import setup_logger, get_console
from evaluation.src.utils.saver import ResultSaver
from evaluation.src.utils.checkpoint import CheckpointManager

# å¯¼å…¥ç­”æ¡ˆç”Ÿæˆæ‰€éœ€çš„ç»„ä»¶
from memory_layer.llm.llm_provider import LLMProvider
# format_prompt å·²ç§»é™¤ï¼Œanswer é˜¶æ®µç”±å„ adapter è‡ªè¡Œå¤„ç†


class Pipeline:
    """
    è¯„æµ‹ Pipeline
    
    å››é˜¶æ®µæµç¨‹ï¼š
    1. Add: æ‘„å…¥å¯¹è¯æ•°æ®å¹¶æž„å»ºç´¢å¼•
    2. Search: æ£€ç´¢ç›¸å…³è®°å¿†
    3. Answer: ç”Ÿæˆç­”æ¡ˆ
    4. Evaluate: è¯„ä¼°ç­”æ¡ˆè´¨é‡
    """
    
    def __init__(
        self,
        adapter: BaseAdapter,
        evaluator: BaseEvaluator,
        llm_provider: LLMProvider,
        output_dir: Path,
        run_name: str = "default",
        use_checkpoint: bool = True,
    ):
        """
        åˆå§‹åŒ– Pipeline
        
        Args:
            adapter: ç³»ç»Ÿé€‚é…å™¨
            evaluator: è¯„ä¼°å™¨
            llm_provider: LLM Providerï¼ˆç”¨äºŽç­”æ¡ˆç”Ÿæˆï¼‰
            output_dir: è¾“å‡ºç›®å½•
            run_name: è¿è¡Œåç§°ï¼ˆç”¨äºŽåŒºåˆ†ä¸åŒè¿è¡Œï¼‰
            use_checkpoint: æ˜¯å¦å¯ç”¨æ–­ç‚¹ç»­ä¼ 
        """
        self.adapter = adapter
        self.evaluator = evaluator
        self.llm_provider = llm_provider
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger = setup_logger(self.output_dir / "pipeline.log")
        self.saver = ResultSaver(self.output_dir)
        self.console = get_console()
        
        # æ–­ç‚¹ç»­ä¼ æ”¯æŒ
        self.use_checkpoint = use_checkpoint
        self.checkpoint = CheckpointManager(output_dir=output_dir, run_name=run_name) if use_checkpoint else None
        self.completed_stages: set = set()
    
    async def run(
        self,
        dataset: Dataset,
        stages: Optional[List[str]] = None,
        smoke_test: bool = False,
        smoke_messages: int = 10,
        smoke_questions: int = 3,
    ) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´ Pipeline
        
        Args:
            dataset: æ ‡å‡†æ ¼å¼æ•°æ®é›†
            stages: è¦æ‰§è¡Œçš„é˜¶æ®µåˆ—è¡¨ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨
                   å¯é€‰å€¼: ["add", "search", "answer", "evaluate"]
            smoke_test: æ˜¯å¦ä¸ºå†’çƒŸæµ‹è¯•
            smoke_messages: å†’çƒŸæµ‹è¯•æ—¶çš„æ¶ˆæ¯æ•°é‡ï¼ˆé»˜è®¤10ï¼‰
            smoke_questions: å†’çƒŸæµ‹è¯•æ—¶çš„é—®é¢˜æ•°é‡ï¼ˆé»˜è®¤3ï¼‰
            
        Returns:
            è¯„æµ‹ç»“æžœå­—å…¸
        """
        start_time = time.time()
        
        self.console.print(f"\n{'='*60}", style="bold cyan")
        self.console.print("ðŸš€ Evaluation Pipeline", style="bold cyan")
        self.console.print(f"{'='*60}", style="bold cyan")
        self.console.print(f"Dataset: {dataset.dataset_name}")
        self.console.print(f"System: {self.adapter.get_system_info()['name']}")
        self.console.print(f"Stages: {stages or 'all'}")
        if smoke_test:
            self.console.print(f"[yellow]ðŸ§ª Smoke Test Mode: {smoke_messages} messages, {smoke_questions} questions[/yellow]")
        self.console.print(f"{'='*60}\n", style="bold cyan")
        
        # å†’çƒŸæµ‹è¯•ï¼šåªå¤„ç†ç¬¬ä¸€ä¸ªå¯¹è¯çš„å‰ K æ¡æ¶ˆæ¯å’Œå‰ K ä¸ªé—®é¢˜
        if smoke_test:
            dataset = self._apply_smoke_test(dataset, smoke_messages, smoke_questions)
            self.console.print(f"[yellow]âœ‚ï¸  Smoke test applied:[/yellow]")
            self.console.print(f"[yellow]   - Conversation: {dataset.conversations[0].conversation_id}[/yellow]")
            self.console.print(f"[yellow]   - Messages: {len(dataset.conversations[0].messages)}[/yellow]")
            self.console.print(f"[yellow]   - Questions: {len(dataset.qa_pairs)}[/yellow]\n")
        
        # è¿‡æ»¤æŽ‰ Category 5ï¼ˆå¯¹æŠ—æ€§é—®é¢˜ï¼‰
        original_qa_count = len(dataset.qa_pairs)
        dataset.qa_pairs = [qa for qa in dataset.qa_pairs if qa.category != 5]
        filtered_count = original_qa_count - len(dataset.qa_pairs)
        
        if filtered_count > 0:
            self.console.print(f"[dim]ðŸ” Filtered out {filtered_count} category-5 (adversarial) questions[/dim]")
            self.console.print(f"[dim]   Remaining questions: {len(dataset.qa_pairs)}[/dim]\n")
        
        # å°è¯•åŠ è½½ checkpoint
        search_results_data = None
        answer_results_data = None
        
        if self.use_checkpoint and self.checkpoint:
            checkpoint_data = self.checkpoint.load_checkpoint()
            if checkpoint_data:
                self.completed_stages = set(checkpoint_data.get('completed_stages', []))
                # åŠ è½½å·²ä¿å­˜çš„ä¸­é—´ç»“æžœ
                if 'search_results' in checkpoint_data:
                    search_results_data = checkpoint_data['search_results']
                if 'answer_results' in checkpoint_data:
                    answer_results_data = checkpoint_data['answer_results']
        
        # é»˜è®¤æ‰§è¡Œæ‰€æœ‰é˜¶æ®µ
        if stages is None:
            stages = ["add", "search", "answer", "evaluate"]
        
        results = {}
        
        # ===== Stage 1: Add =====
        if "add" in stages and "add" not in self.completed_stages:
            self.logger.info("Starting Stage 1: Add")
            
            # ðŸ”¥ ä¼ é€’ checkpoint_manager ä»¥æ”¯æŒç»†ç²’åº¦æ–­ç‚¹ç»­ä¼ 
            index = await self.adapter.add(
                conversations=dataset.conversations,
                output_dir=self.output_dir,
                checkpoint_manager=self.checkpoint
            )
            
            # ç´¢å¼•å…ƒæ•°æ®ï¼ˆå»¶è¿ŸåŠ è½½ï¼Œæ— éœ€æŒä¹…åŒ–ï¼‰
            results["index"] = index
            self.logger.info("âœ… Stage 1 completed")
            
            # ä¿å­˜ checkpoint
            self.completed_stages.add("add")
            if self.checkpoint:
                self.checkpoint.save_checkpoint(self.completed_stages)
        elif "add" in self.completed_stages:
            self.console.print("\n[yellow]â­ï¸  Skip Add stage (already completed)[/yellow]")
            # é‡æ–°æž„å»ºç´¢å¼•å…ƒæ•°æ®ï¼ˆå»¶è¿ŸåŠ è½½ä¸éœ€è¦ pkl æ–‡ä»¶ï¼‰
            index = {
                "type": "lazy_load",
                "memcells_dir": str(self.output_dir / "memcells"),
                "bm25_index_dir": str(self.output_dir / "bm25_index"),
                "emb_index_dir": str(self.output_dir / "vectors"),
                "conversation_ids": [conv.conversation_id for conv in dataset.conversations],
                "use_hybrid_search": True,
                "total_conversations": len(dataset.conversations),
            }
            results["index"] = index
        else:
            # é‡æ–°æž„å»ºç´¢å¼•å…ƒæ•°æ®
            index = {
                "type": "lazy_load",
                "memcells_dir": str(self.output_dir / "memcells"),
                "bm25_index_dir": str(self.output_dir / "bm25_index"),
                "emb_index_dir": str(self.output_dir / "vectors"),
                "conversation_ids": [conv.conversation_id for conv in dataset.conversations],
                "use_hybrid_search": True,
                "total_conversations": len(dataset.conversations),
            }
            results["index"] = index
            self.logger.info("â­ï¸  Skipped Stage 1, using lazy loading")
        
        # ===== Stage 2: Search =====
        if "search" in stages and "search" not in self.completed_stages:
            self.logger.info("Starting Stage 2: Search")
            
            search_results = await self._run_search(
                dataset.qa_pairs,
                index
            )
            
            self.saver.save_json(
                [self._search_result_to_dict(sr) for sr in search_results],
                "search_results.json"
            )
            results["search_results"] = search_results
            self.logger.info("âœ… Stage 2 completed")
            
            # ä¿å­˜ checkpoint
            self.completed_stages.add("search")
            if self.checkpoint:
                search_results_data = [self._search_result_to_dict(sr) for sr in search_results]
                self.checkpoint.save_checkpoint(
                    self.completed_stages,
                    search_results=search_results_data
                )
        elif "search" in self.completed_stages:
            self.console.print(f"\n[yellow]â­ï¸  Skip Search stage (already completed)[/yellow]")
            if search_results_data:
                # ä»Ž checkpoint åŠ è½½
                search_results = [self._dict_to_search_result(d) for d in search_results_data]
                results["search_results"] = search_results
            elif self.saver.file_exists("search_results.json"):
                # ä»Žæ–‡ä»¶åŠ è½½
                search_data = self.saver.load_json("search_results.json")
                search_results = [self._dict_to_search_result(d) for d in search_data]
                results["search_results"] = search_results
        else:
            if self.saver.file_exists("search_results.json"):
                search_data = self.saver.load_json("search_results.json")
                search_results = [self._dict_to_search_result(d) for d in search_data]
                results["search_results"] = search_results
                self.logger.info("â­ï¸  Skipped Stage 2, loaded existing results")
            else:
                raise FileNotFoundError("Search results not found. Please run 'search' stage first.")
        
        # ===== Stage 3: Answer =====
        if "answer" in stages and "answer" not in self.completed_stages:
            self.logger.info("Starting Stage 3: Answer")
            
            answer_results = await self._run_answer(
                dataset.qa_pairs,
                search_results
            )
            
            self.saver.save_json(
                [self._answer_result_to_dict(ar) for ar in answer_results],
                "answer_results.json"
            )
            results["answer_results"] = answer_results
            self.logger.info("âœ… Stage 3 completed")
            
            # ä¿å­˜ checkpoint
            self.completed_stages.add("answer")
            if self.checkpoint:
                answer_results_dict = [self._answer_result_to_dict(ar) for ar in answer_results]
                self.checkpoint.save_checkpoint(
                    self.completed_stages,
                    search_results=search_results_data if search_results_data else [self._search_result_to_dict(sr) for sr in search_results],
                    answer_results=answer_results_dict
                )
        elif "answer" in self.completed_stages:
            self.console.print(f"\n[yellow]â­ï¸  Skip Answer stage (already completed)[/yellow]")
            if answer_results_data:
                # ä»Ž checkpoint åŠ è½½
                answer_results = [self._dict_to_answer_result(d) for d in answer_results_data]
                results["answer_results"] = answer_results
            elif self.saver.file_exists("answer_results.json"):
                # ä»Žæ–‡ä»¶åŠ è½½
                answer_data = self.saver.load_json("answer_results.json")
                answer_results = [self._dict_to_answer_result(d) for d in answer_data]
                results["answer_results"] = answer_results
        else:
            if self.saver.file_exists("answer_results.json"):
                answer_data = self.saver.load_json("answer_results.json")
                answer_results = [self._dict_to_answer_result(d) for d in answer_data]
                results["answer_results"] = answer_results
                self.logger.info("â­ï¸  Skipped Stage 3, loaded existing results")
            else:
                raise FileNotFoundError("Answer results not found. Please run 'answer' stage first.")
        
        # ===== Stage 4: Evaluate =====
        if "evaluate" in stages and "evaluate" not in self.completed_stages:
            self.logger.info("Starting Stage 4: Evaluate")
            
            eval_result = await self.evaluator.evaluate(answer_results)
            
            self.saver.save_json(
                self._eval_result_to_dict(eval_result),
                "eval_results.json"
            )
            results["eval_result"] = eval_result
            self.logger.info("âœ… Stage 4 completed")
            
            # ä¿å­˜ checkpoint
            self.completed_stages.add("evaluate")
            if self.checkpoint:
                self.checkpoint.save_checkpoint(
                    self.completed_stages,
                    search_results=search_results_data if search_results_data else [self._search_result_to_dict(sr) for sr in search_results],
                    answer_results=answer_results_data if answer_results_data else [self._answer_result_to_dict(ar) for ar in answer_results],
                    eval_results=self._eval_result_to_dict(eval_result)
                )
        elif "evaluate" in self.completed_stages:
            self.console.print("\n[yellow]â­ï¸  Skip Evaluate stage (already completed)[/yellow]")
        
        # # æ‰€æœ‰é˜¶æ®µå®ŒæˆåŽï¼Œåˆ é™¤ checkpoint
        # if self.checkpoint and len(self.completed_stages) == 4:
        #     self.console.print("\n[dim]ðŸ§¹ Cleaning up checkpoint...[/dim]")
        #     self.checkpoint.delete_checkpoint()
        
        # ç”ŸæˆæŠ¥å‘Š
        elapsed_time = time.time() - start_time
        self._generate_report(results, elapsed_time)
        
        return results
    
    def _apply_smoke_test(
        self, 
        dataset: Dataset, 
        num_messages: int, 
        num_questions: int
    ) -> Dataset:
        """
        åº”ç”¨å†’çƒŸæµ‹è¯•ï¼šåªä¿ç•™ç¬¬ä¸€ä¸ªå¯¹è¯çš„å‰ N æ¡æ¶ˆæ¯å’Œå‰ M ä¸ªé—®é¢˜
        
        è¿™æ ·å¯ä»¥å¿«é€ŸéªŒè¯å®Œæ•´æµç¨‹ï¼ˆAdd â†’ Search â†’ Answer â†’ Evaluateï¼‰ï¼Œ
        ä½†åªä½¿ç”¨å°‘é‡æ•°æ®ï¼ŒèŠ‚çœæ—¶é—´ã€‚
        
        Args:
            dataset: åŽŸå§‹æ•°æ®é›†
            num_messages: ä¿ç•™çš„æ¶ˆæ¯æ•°é‡ï¼ˆç”¨äºŽ Add é˜¶æ®µï¼‰ï¼Œ0 è¡¨ç¤ºæ‰€æœ‰æ¶ˆæ¯
            num_questions: ä¿ç•™çš„é—®é¢˜æ•°é‡ï¼ˆç”¨äºŽ Search/Answer/Evaluate é˜¶æ®µï¼‰ï¼Œ0 è¡¨ç¤ºæ‰€æœ‰é—®é¢˜
            
        Returns:
            è£å‰ªåŽçš„æ•°æ®é›†
        """
        if not dataset.conversations:
            return dataset
        
        # åªä¿ç•™ç¬¬ä¸€ä¸ªå¯¹è¯
        first_conv = dataset.conversations[0]
        conv_id = first_conv.conversation_id
        
        # æˆªå–å‰ N æ¡æ¶ˆæ¯ï¼ˆç”¨äºŽ Addï¼‰
        # 0 è¡¨ç¤ºä¿ç•™æ‰€æœ‰æ¶ˆæ¯
        if num_messages > 0:
            total_messages = len(first_conv.messages)
            first_conv.messages = first_conv.messages[:num_messages]
            msg_desc = f"{len(first_conv.messages)}/{total_messages}"
        else:
            msg_desc = f"{len(first_conv.messages)} (all)"
        
        # åªä¿ç•™è¯¥å¯¹è¯çš„å‰ M ä¸ªé—®é¢˜ï¼ˆç”¨äºŽ Search/Answer/Evaluateï¼‰
        # 0 è¡¨ç¤ºä¿ç•™æ‰€æœ‰é—®é¢˜
        conv_qa_pairs = [
            qa for qa in dataset.qa_pairs 
            if qa.metadata.get("conversation_id") == conv_id
        ]
        if num_questions > 0:
            total_questions = len(conv_qa_pairs)
            selected_qa_pairs = conv_qa_pairs[:num_questions]
            qa_desc = f"{len(selected_qa_pairs)}/{total_questions}"
        else:
            selected_qa_pairs = conv_qa_pairs
            qa_desc = f"{len(selected_qa_pairs)} (all)"
        
        self.logger.info(
            f"Smoke test: Conv {conv_id} - "
            f"{msg_desc} messages, "
            f"{qa_desc} questions"
        )
        
        return Dataset(
            dataset_name=dataset.dataset_name + "_smoke",
            conversations=[first_conv],
            qa_pairs=selected_qa_pairs,
            metadata={
                **dataset.metadata, 
                "smoke_test": True, 
                "smoke_messages": num_messages if num_messages > 0 else len(first_conv.messages),
                "smoke_questions": num_questions if num_questions > 0 else len(selected_qa_pairs),
            }
        )
    
    async def _run_search(
        self,
        qa_pairs: List,
        index: Any
    ) -> List[SearchResult]:
        """
        å¹¶å‘æ‰§è¡Œæ£€ç´¢ï¼Œæ”¯æŒç»†ç²’åº¦ checkpoint
        
        æŒ‰ä¼šè¯åˆ†ç»„å¤„ç†ï¼Œæ¯å¤„ç†å®Œä¸€ä¸ªä¼šè¯å°±ä¿å­˜ checkpointï¼ˆå’Œ archive çš„ stage3 ä¸€è‡´ï¼‰
        """
        print(f"\n{'='*60}")
        print(f"Stage 2/4: Search")
        print(f"{'='*60}")
        
        # ðŸ”¥ åŠ è½½ç»†ç²’åº¦ checkpoint
        all_search_results_dict = {}
        if self.checkpoint:
            all_search_results_dict = self.checkpoint.load_search_progress()
        
        # æŒ‰ä¼šè¯åˆ†ç»„ QA å¯¹
        conv_to_qa = {}
        for qa in qa_pairs:
            conv_id = qa.metadata.get("conversation_id", "unknown")
            if conv_id not in conv_to_qa:
                conv_to_qa[conv_id] = []
            conv_to_qa[conv_id].append(qa)
        
        total_convs = len(conv_to_qa)
        processed_convs = set(all_search_results_dict.keys())
        remaining_convs = set(conv_to_qa.keys()) - processed_convs
        
        print(f"Total conversations: {total_convs}")
        print(f"Total questions: {len(qa_pairs)}")
        if processed_convs:
            print(f"Already processed: {len(processed_convs)} conversations (from checkpoint)")
            print(f"Remaining: {len(remaining_convs)} conversations")
        
        semaphore = asyncio.Semaphore(20)
        
        async def search_single(qa):
            async with semaphore:
                conv_id = qa.metadata.get("conversation_id", "0")
                return await self.adapter.search(qa.question, conv_id, index)
        
        # æŒ‰ä¼šè¯é€ä¸ªå¤„ç†ï¼ˆå’Œ archive ä¸€è‡´ï¼‰
        for idx, (conv_id, qa_list) in enumerate(sorted(conv_to_qa.items())):
            # ðŸ”¥ è·³è¿‡å·²å¤„ç†çš„ä¼šè¯
            if conv_id in processed_convs:
                print(f"\nâ­ï¸  Skipping Conversation ID: {conv_id} (already processed)")
                continue
            
            print(f"\n--- Processing Conversation ID: {conv_id} ({idx+1}/{total_convs}) ---")
            print(f"    Questions in this conversation: {len(qa_list)}")
            
            # å¹¶å‘å¤„ç†è¿™ä¸ªä¼šè¯çš„æ‰€æœ‰é—®é¢˜
            tasks = [search_single(qa) for qa in qa_list]
            results_for_conv = await asyncio.gather(*tasks)
            
            # å°†ç»“æžœä¿å­˜ä¸ºå­—å…¸æ ¼å¼
            results_for_conv_dict = [
                {
                    "question_id": qa.question_id,
                    "query": qa.question,
                    "conversation_id": conv_id,
                    "results": result.results,
                    "retrieval_metadata": result.retrieval_metadata
                }
                for qa, result in zip(qa_list, results_for_conv)
            ]
            
            all_search_results_dict[conv_id] = results_for_conv_dict
            
            # ðŸ”¥ æ¯å¤„ç†å®Œä¸€ä¸ªä¼šè¯å°±ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆå’Œ archive ä¸€è‡´ï¼‰
            if self.checkpoint:
                self.checkpoint.save_search_progress(all_search_results_dict)
        
        # ðŸ”¥ å®ŒæˆåŽåˆ é™¤ç»†ç²’åº¦æ£€æŸ¥ç‚¹
        if self.checkpoint:
            self.checkpoint.delete_search_checkpoint()
        
        # å°†å­—å…¸æ ¼å¼è½¬æ¢ä¸º SearchResult å¯¹è±¡åˆ—è¡¨ï¼ˆä¿æŒåŽŸæœ‰è¿”å›žæ ¼å¼ï¼‰
        all_results = []
        for conv_id in sorted(conv_to_qa.keys()):
            if conv_id in all_search_results_dict:
                for result_dict in all_search_results_dict[conv_id]:
                    all_results.append(SearchResult(
                        query=result_dict["query"],
                        conversation_id=result_dict["conversation_id"],
                        results=result_dict["results"],
                        retrieval_metadata=result_dict.get("retrieval_metadata", {})
                    ))
        
        print(f"\n{'='*60}")
        print(f"ðŸŽ‰ All conversations processed!")
        print(f"{'='*60}")
        print(f"âœ… Search completed: {len(all_results)} results\n")
        return all_results
    
    async def _run_answer(
        self,
        qa_pairs: List,
        search_results: List[SearchResult]
    ) -> List[AnswerResult]:
        """
        ç”Ÿæˆç­”æ¡ˆï¼Œæ”¯æŒç»†ç²’åº¦ checkpoint
        
        æ¯ SAVE_INTERVAL ä¸ªé—®é¢˜ä¿å­˜ä¸€æ¬¡ checkpointï¼ˆå’Œ archive çš„ stage4 ä¸€è‡´ï¼‰
        """
        print(f"\n{'='*60}")
        print(f"Stage 3/4: Answer")
        print(f"{'='*60}")
        
        SAVE_INTERVAL = 400  # ðŸ”¥ å’Œ archive ä¿æŒä¸€è‡´ï¼šæ¯ 400 ä¸ªä»»åŠ¡ä¿å­˜ä¸€æ¬¡
        MAX_CONCURRENT = 50  # æœ€å¤§å¹¶å‘æ•°
        
        # ðŸ”¥ åŠ è½½ç»†ç²’åº¦ checkpoint
        all_answer_results = {}
        if self.checkpoint:
            loaded_results = self.checkpoint.load_answer_progress()
            # è½¬æ¢ä¸º {question_id: AnswerResult} æ ¼å¼
            for result in loaded_results.values():
                all_answer_results[result["question_id"]] = result
        
        total_qa_count = len(qa_pairs)
        processed_count = len(all_answer_results)
        
        print(f"Total questions: {total_qa_count}")
        if processed_count > 0:
            print(f"Already processed: {processed_count} questions (from checkpoint)")
            print(f"Remaining: {total_qa_count - processed_count} questions")
        
        # å‡†å¤‡å¾…å¤„ç†çš„ä»»åŠ¡
        pending_tasks = []
        for qa, sr in zip(qa_pairs, search_results):
            if qa.question_id not in all_answer_results:
                pending_tasks.append((qa, sr))
        
        if not pending_tasks:
            print(f"âœ… All questions already processed!")
            # è½¬æ¢ä¸º AnswerResult å¯¹è±¡åˆ—è¡¨ï¼ˆæŒ‰åŽŸå§‹é¡ºåºï¼‰
            results = []
            for qa in qa_pairs:
                if qa.question_id in all_answer_results:
                    result_dict = all_answer_results[qa.question_id]
                    results.append(AnswerResult(
                        question_id=result_dict["question_id"],
                        question=result_dict["question"],
                        answer=result_dict["answer"],
                        golden_answer=result_dict["golden_answer"],
                        category=result_dict.get("category"),
                        conversation_id=result_dict.get("conversation_id", ""),
                        search_results=result_dict.get("search_results", []),
                    ))
            return results
        
        semaphore = asyncio.Semaphore(MAX_CONCURRENT)
        completed = processed_count
        failed = 0
        start_time = time.time()
        
        async def answer_single_with_tracking(qa, search_result):
            nonlocal completed, failed
            
            async with semaphore:
                try:
                    # æž„å»º context
                    context = self._build_context(search_result)
                    
                    # ðŸ”¥ ç›´æŽ¥è°ƒç”¨ adapter çš„ answer æ–¹æ³•
                    answer = await self.adapter.answer(
                        query=qa.question,
                        context=context,
                        conversation_id=search_result.conversation_id,
                    )
                    
                    answer = answer.strip()
                
                except Exception as e:
                    print(f"  âš ï¸ Answer generation failed for {qa.question_id}: {e}")
                    answer = "Error: Failed to generate answer"
                    failed += 1
                
                result = AnswerResult(
                    question_id=qa.question_id,
                    question=qa.question,
                    answer=answer,
                    golden_answer=qa.answer,
                    category=qa.category,
                    conversation_id=search_result.conversation_id,
                    search_results=search_result.results,
                )
                
                # ä¿å­˜ç»“æžœ
                all_answer_results[qa.question_id] = {
                    "question_id": result.question_id,
                    "question": result.question,
                    "answer": result.answer,
                    "golden_answer": result.golden_answer,
                    "category": result.category,
                    "conversation_id": result.conversation_id,
                    "search_results": result.search_results,
                }
                
                completed += 1
                
                # ðŸ”¥ å®šæœŸä¿å­˜ checkpointï¼ˆå’Œ archive ä¸€è‡´ï¼‰
                if self.checkpoint and (completed % SAVE_INTERVAL == 0 or completed == total_qa_count):
                    elapsed = time.time() - start_time
                    speed = completed / elapsed if elapsed > 0 else 0
                    eta = (total_qa_count - completed) / speed if speed > 0 else 0
                    
                    print(f"Progress: {completed}/{total_qa_count} ({completed/total_qa_count*100:.1f}%) | "
                          f"Speed: {speed:.1f} qa/s | Failed: {failed} | ETA: {eta/60:.1f} min")
                    
                    self.checkpoint.save_answer_progress(all_answer_results, completed, total_qa_count)
                
                return result
        
        # åˆ›å»ºæ‰€æœ‰å¾…å¤„ç†çš„ä»»åŠ¡
        tasks = [
            answer_single_with_tracking(qa, sr)
            for qa, sr in pending_tasks
        ]
        
        # å¹¶å‘æ‰§è¡Œ
        await asyncio.gather(*tasks)
        
        # ç»Ÿè®¡ä¿¡æ¯
        elapsed_time = time.time() - start_time
        success_rate = (completed - failed) / completed * 100 if completed > 0 else 0
        
        print(f"\n{'='*60}")
        print(f"âœ… All responses generated!")
        print(f"   - Total questions: {total_qa_count}")
        print(f"   - Successful: {completed - failed}")
        print(f"   - Failed: {failed}")
        print(f"   - Success rate: {success_rate:.1f}%")
        print(f"   - Time elapsed: {elapsed_time/60:.1f} minutes ({elapsed_time:.0f}s)")
        print(f"   - Average speed: {total_qa_count/elapsed_time:.1f} qa/s")
        print(f"{'='*60}\n")
        
        # ðŸ”¥ å®ŒæˆåŽåˆ é™¤ç»†ç²’åº¦æ£€æŸ¥ç‚¹ï¼ˆå’Œ archive ä¸€è‡´ï¼‰
        if self.checkpoint:
            self.checkpoint.delete_answer_checkpoints()
        
        # è½¬æ¢ä¸º AnswerResult å¯¹è±¡åˆ—è¡¨ï¼ˆæŒ‰åŽŸå§‹é¡ºåºï¼‰
        results = []
        for qa in qa_pairs:
            if qa.question_id in all_answer_results:
                result_dict = all_answer_results[qa.question_id]
                results.append(AnswerResult(
                    question_id=result_dict["question_id"],
                    question=result_dict["question"],
                    answer=result_dict["answer"],
                    golden_answer=result_dict["golden_answer"],
                    category=result_dict.get("category"),
                    conversation_id=result_dict.get("conversation_id", ""),
                    search_results=result_dict.get("search_results", []),
                ))
        
        return results
    
    def _build_context(self, search_result: SearchResult) -> str:
        """ä»Žæ£€ç´¢ç»“æžœæž„å»ºä¸Šä¸‹æ–‡"""
        context_parts = []
        
        for idx, result in enumerate(search_result.results[:10], 1):
            content = result.get("content", "")
            context_parts.append(f"{idx}. {content}")
        
        return "\n\n".join(context_parts)
    
    def _generate_report(self, results: Dict[str, Any], elapsed_time: float):
        """ç”Ÿæˆè¯„æµ‹æŠ¥å‘Š"""
        report_lines = []
        report_lines.append("=" * 60)
        report_lines.append("ðŸ“Š Evaluation Report")
        report_lines.append("=" * 60)
        report_lines.append("")
        
        # ç³»ç»Ÿä¿¡æ¯
        system_info = self.adapter.get_system_info()
        report_lines.append(f"System: {system_info['name']}")
        report_lines.append(f"Time Elapsed: {elapsed_time:.2f}s")
        report_lines.append("")
        
        # è¯„ä¼°ç»“æžœ
        if "eval_result" in results:
            eval_result = results["eval_result"]
            report_lines.append(f"Total Questions: {eval_result.total_questions}")
            report_lines.append(f"Correct: {eval_result.correct}")
            report_lines.append(f"Accuracy: {eval_result.accuracy:.2%}")
            report_lines.append("")
        
        report_lines.append("=" * 60)
        
        report_text = "\n".join(report_lines)
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = self.output_dir / "report.txt"
        with open(report_path, "w") as f:
            f.write(report_text)
        
        # æ‰“å°åˆ°æŽ§åˆ¶å°
        self.console.print("\n" + report_text, style="bold green")
        self.logger.info(f"Report saved to: {report_path}")
    
    # åºåˆ—åŒ–è¾…åŠ©æ–¹æ³•
    def _search_result_to_dict(self, sr: SearchResult) -> dict:
        return {
            "query": sr.query,
            "conversation_id": sr.conversation_id,
            "results": sr.results,
            "retrieval_metadata": sr.retrieval_metadata,
        }
    
    def _dict_to_search_result(self, d: dict) -> SearchResult:
        return SearchResult(**d)
    
    def _answer_result_to_dict(self, ar: AnswerResult) -> dict:
        return {
            "question_id": ar.question_id,
            "question": ar.question,
            "answer": ar.answer,
            "golden_answer": ar.golden_answer,
            "category": ar.category,
            "conversation_id": ar.conversation_id,
            "search_results": ar.search_results,
            "metadata": ar.metadata,
        }
    
    def _dict_to_answer_result(self, d: dict) -> AnswerResult:
        return AnswerResult(**d)
    
    def _eval_result_to_dict(self, er: EvaluationResult) -> dict:
        return {
            "total_questions": er.total_questions,
            "correct": er.correct,
            "accuracy": er.accuracy,
            "detailed_results": er.detailed_results,
            "metadata": er.metadata,
        }

