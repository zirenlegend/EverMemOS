"""
EverMemOS Adapter

é€‚é…å±‚ï¼Œè´Ÿè´£å°†è¯„æµ‹æ¡†æ¶ä¸ EverMemOS å®ç°è¿æ¥èµ·æ¥ã€‚
"""
import asyncio
import json
import pickle
import time
from pathlib import Path
from typing import Any, Dict, List

from rich.progress import (
    Progress,
    SpinnerColumn,
    TextColumn,
    BarColumn,
    TaskProgressColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
    MofNCompleteColumn,
)
from rich.console import Console

from evaluation.src.adapters.base import BaseAdapter
from evaluation.src.adapters.registry import register_adapter
from evaluation.src.core.data_models import Conversation, SearchResult
from common_utils.datetime_utils import to_iso_format

# å¯¼å…¥ EverMemOS å®ç°
from evaluation.src.adapters.evermemos import (
    stage1_memcells_extraction,
    stage2_index_building,
    stage3_memory_retrivel,
    stage4_response,
)

# å¯¼å…¥ Memory Layer ç»„ä»¶
from memory_layer.llm.llm_provider import LLMProvider
from memory_layer.memory_extractor.event_log_extractor import EventLogExtractor


@register_adapter("evermemos")
class EverMemOSAdapter(BaseAdapter):
    """
    EverMemOS é€‚é…å™¨
    
    èŒè´£ï¼š
    1. æ¥æ”¶è¯„æµ‹æ¡†æ¶çš„è°ƒç”¨
    2. è½¬æ¢æ•°æ®æ ¼å¼ï¼ˆè¯„æµ‹æ¡†æ¶ â†” EverMemOSï¼‰
    3. è°ƒç”¨ stage*.py å®ç°
    4. è¿”å›è¯„æµ‹æ¡†æ¶éœ€è¦çš„ç»“æœæ ¼å¼
    
    å®ç°ç»†èŠ‚ï¼š
    - MemCell æå–ï¼ˆstage1ï¼‰
    - ç´¢å¼•æ„å»ºï¼ˆstage2ï¼‰
    - æ£€ç´¢é€»è¾‘ï¼ˆstage3ï¼‰
    - ç­”æ¡ˆç”Ÿæˆï¼ˆstage4ï¼‰
    """
    
    def __init__(self, config: dict, output_dir: Path = None):
        super().__init__(config)
        self.output_dir = Path(output_dir) if output_dir else Path(".")
        
        # åˆå§‹åŒ– LLM Providerï¼ˆå…±äº«ç»™æ‰€æœ‰ stageï¼‰
        # ä» YAML çš„ llm é…ç½®ä¸­è¯»å–
        llm_config = config.get("llm", {})
        
        self.llm_provider = LLMProvider(
            provider_type=llm_config.get("provider", "openai"),
            model=llm_config.get("model", "gpt-4o-mini"),
            api_key=llm_config.get("api_key", ""),
            base_url=llm_config.get("base_url", "https://api.openai.com/v1"),
            temperature=llm_config.get("temperature", 0.3),
            max_tokens=llm_config.get("max_tokens", 32768),
        )
        
        # åˆå§‹åŒ– Event Log Extractor
        self.event_log_extractor = EventLogExtractor(llm_provider=self.llm_provider)
        
        # ç¡®ä¿ NLTK æ•°æ®å¯ç”¨
        stage2_index_building.ensure_nltk_data()
        
        print(f"âœ… EverMemOS Adapter initialized")
        print(f"   LLM Model: {llm_config.get('model')}")
        print(f"   Output Dir: {self.output_dir}")
    
    async def add(
        self, 
        conversations: List[Conversation],
        output_dir: Path = None,
        checkpoint_manager = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Add é˜¶æ®µï¼šæå– MemCells å¹¶æ„å»ºç´¢å¼•
        
        è°ƒç”¨æµç¨‹ï¼š
        1. Stage 1: æå– MemCells (stage1_memcells_extraction.py) - å¹¶å‘å¤„ç†
        2. Stage 2: æ„å»º BM25 å’Œ Embedding ç´¢å¼• (stage2_index_building.py)
        
        è¿”å›ï¼šç´¢å¼•å…ƒæ•°æ®ï¼ˆæ–¹æ¡ˆ Aï¼šå»¶è¿ŸåŠ è½½ï¼‰
        """
        output_dir = Path(output_dir) if output_dir else self.output_dir
        output_dir.mkdir(parents=True, exist_ok=True)
        
        memcells_dir = output_dir / "memcells"
        memcells_dir.mkdir(parents=True, exist_ok=True)
        bm25_index_dir = output_dir / "bm25_index"
        emb_index_dir = output_dir / "vectors"
        bm25_index_dir.mkdir(parents=True, exist_ok=True)
        emb_index_dir.mkdir(parents=True, exist_ok=True)
        
        console = Console()
        
        # ========== Stage 1: MemCell Extraction (å¹¶å‘å¤„ç†) ==========
        console.print(f"\n{'='*60}", style="bold cyan")
        console.print(f"Stage 1: MemCell Extraction", style="bold cyan")
        console.print(f"{'='*60}", style="bold cyan")
        
        # è½¬æ¢æ•°æ®æ ¼å¼ï¼šè¯„æµ‹æ¡†æ¶ â†’ EverMemOS
        raw_data_dict = {}
        for conv in conversations:
            conv_id = conv.conversation_id
            raw_data = []
            
            for idx, msg in enumerate(conv.messages):
                # å¤„ç†æ—¶é—´æˆ³ï¼šå¦‚æœä¸º Noneï¼Œä½¿ç”¨åŸºäºç´¢å¼•çš„ä¼ªæ—¶é—´æˆ³
                if msg.timestamp is not None:
                    timestamp_str = to_iso_format(msg.timestamp)
                else:
                    # ä½¿ç”¨æ¶ˆæ¯ç´¢å¼•ç”Ÿæˆä¼ªæ—¶é—´æˆ³ï¼ˆä¿æŒç›¸å¯¹é¡ºåºï¼‰
                    # åŸºå‡†æ—¶é—´: 2023-01-01 00:00:00ï¼Œæ¯æ¡æ¶ˆæ¯é—´éš” 30 ç§’
                    from datetime import datetime, timedelta
                    base_time = datetime(2023, 1, 1, 0, 0, 0)
                    pseudo_time = base_time + timedelta(seconds=idx * 30)
                    timestamp_str = to_iso_format(pseudo_time)
                
                raw_data.append({
                    "speaker_id": msg.speaker_id,
                    "user_name": msg.speaker_name or msg.speaker_id,
                    "speaker_name": msg.speaker_name or msg.speaker_id,
                    "content": msg.content,
                    "timestamp": timestamp_str,
                })
            
            raw_data_dict[conv_id] = raw_data
        
        # æ£€æŸ¥å·²å®Œæˆçš„ä¼šè¯ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
        completed_convs = set()
        if checkpoint_manager:
            all_conv_ids = [conv.conversation_id for conv in conversations]
            completed_convs = checkpoint_manager.load_add_progress(memcells_dir, all_conv_ids)
        
        # è¿‡æ»¤å‡ºå¾…å¤„ç†çš„ä¼šè¯
        pending_conversations = [
            conv for conv in conversations
            if conv.conversation_id not in completed_convs
        ]
        
        console.print(f"\nğŸ“Š æ€»ä¼šè¯æ•°: {len(conversations)}", style="bold cyan")
        console.print(f"âœ… å·²å®Œæˆ: {len(completed_convs)}", style="bold green")
        console.print(f"â³ å¾…å¤„ç†: {len(pending_conversations)}", style="bold yellow")
        
        if len(pending_conversations) == 0:
            console.print(f"\nğŸ‰ æ‰€æœ‰ä¼šè¯å·²å®Œæˆï¼Œè·³è¿‡ MemCell æå–ï¼", style="bold green")
        else:
            total_messages = sum(len(raw_data_dict[c.conversation_id]) for c in pending_conversations)
            console.print(f"ğŸ“ å¾…å¤„ç†æ¶ˆæ¯æ•°: {total_messages}", style="bold blue")
            console.print(f"ğŸš€ å¼€å§‹å¹¶å‘å¤„ç†...\n", style="bold green")
            
            # ä½¿ç”¨ Rich è¿›åº¦æ¡å¹¶å‘å¤„ç†
            start_time = time.time()
            
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                MofNCompleteColumn(),
                TextColumn("â€¢"),
                TaskProgressColumn(),
                TextColumn("â€¢"),
                TimeElapsedColumn(),
                TextColumn("â€¢"),
                TimeRemainingColumn(),
                TextColumn("â€¢"),
                TextColumn("[bold blue]{task.fields[status]}"),
                console=console,
                transient=False,
                refresh_per_second=1,
            ) as progress:
                # åˆ›å»ºä¸»è¿›åº¦ä»»åŠ¡
                main_task = progress.add_task(
                    "[bold cyan]ğŸ¯ æ€»è¿›åº¦",
                    total=len(conversations),
                    completed=len(completed_convs),
                    status="å¤„ç†ä¸­",
                )
                
                # ä¸ºå·²å®Œæˆçš„ä¼šè¯åˆ›å»ºè¿›åº¦æ¡ï¼ˆæ˜¾ç¤ºä¸ºå®Œæˆï¼‰
                conversation_tasks = {}
                for conv_id in completed_convs:
                    conv_task_id = progress.add_task(
                        f"[green]Conv-{conv_id}",
                        total=len(raw_data_dict.get(conv_id, [])),
                        completed=len(raw_data_dict.get(conv_id, [])),
                        status="âœ… (å·²è·³è¿‡)",
                    )
                    conversation_tasks[conv_id] = conv_task_id
                
                # ä¸ºå¾…å¤„ç†çš„ä¼šè¯åˆ›å»ºè¿›åº¦æ¡å’Œä»»åŠ¡
                processing_tasks = []
                for conv in pending_conversations:
                    conv_id = conv.conversation_id
                    conv_task_id = progress.add_task(
                        f"[yellow]Conv-{conv_id}",
                        total=len(raw_data_dict[conv_id]),
                        completed=0,
                        status="ç­‰å¾…",
                    )
                    conversation_tasks[conv_id] = conv_task_id
                    
                    # åˆ›å»ºå¤„ç†ä»»åŠ¡
                    task = stage1_memcells_extraction.process_single_conversation(
                        conv_id=conv_id,
                        conversation=raw_data_dict[conv_id],
                        save_dir=str(memcells_dir),
                        llm_provider=self.llm_provider,
                        event_log_extractor=self.event_log_extractor,
                        progress_counter=None,
                        progress=progress,
                        task_id=conv_task_id,
                        config=self._convert_config_to_experiment_config(),
                    )
                    processing_tasks.append((conv_id, task))
                
                # å®šä¹‰å®Œæˆæ—¶æ›´æ–°å‡½æ•°
                async def run_with_completion(conv_id, task):
                    result = await task
                    progress.update(
                        conversation_tasks[conv_id],
                        status="âœ…",
                        completed=progress.tasks[conversation_tasks[conv_id]].total,
                    )
                    progress.update(main_task, advance=1)
                    return result
                
                # ğŸ”¥ å¹¶å‘æ‰§è¡Œæ‰€æœ‰å¾…å¤„ç†çš„ä»»åŠ¡
                if processing_tasks:
                    results = await asyncio.gather(
                        *[run_with_completion(conv_id, task) for conv_id, task in processing_tasks]
                    )
                else:
                    results = []
                
                progress.update(main_task, status="âœ… å®Œæˆ")
            
            end_time = time.time()
            elapsed = end_time - start_time
            
            # ç»Ÿè®¡ç»“æœ
            successful_convs = sum(1 for _, memcell_list in results if memcell_list)
            total_memcells = sum(len(memcell_list) for _, memcell_list in results)
            
            console.print("\n" + "=" * 60, style="dim")
            console.print("ğŸ“Š MemCell æå–å®Œæˆç»Ÿè®¡:", style="bold")
            console.print(f"   âœ… æˆåŠŸå¤„ç†: {successful_convs}/{len(pending_conversations)}", style="green")
            console.print(f"   ğŸ“ æ€» memcells: {total_memcells}", style="blue")
            console.print(f"   â±ï¸  æ€»è€—æ—¶: {elapsed:.2f} ç§’", style="yellow")
            if len(pending_conversations) > 0:
                console.print(f"   ğŸš€ å¹³å‡æ¯ä¼šè¯: {elapsed/len(pending_conversations):.2f} ç§’", style="cyan")
            console.print("=" * 60, style="dim")
        
        # ========== Stage 2: Index Building ==========
        console.print(f"\n{'='*60}", style="bold cyan")
        console.print(f"Stage 2: Index Building", style="bold cyan")
        console.print(f"{'='*60}", style="bold cyan")
        
        # è°ƒç”¨ stage2 å®ç°æ„å»ºç´¢å¼•
        exp_config = self._convert_config_to_experiment_config()
        exp_config.num_conv = len(conversations)  # è®¾ç½®ä¼šè¯æ•°é‡
        
        # æ„å»º BM25 ç´¢å¼•
        console.print("ğŸ”¨ æ„å»º BM25 ç´¢å¼•...", style="yellow")
        stage2_index_building.build_bm25_index(
            config=exp_config,
            data_dir=memcells_dir,
            bm25_save_dir=bm25_index_dir,
        )
        console.print("âœ… BM25 ç´¢å¼•æ„å»ºå®Œæˆ", style="green")
        
        # æ„å»º Embedding ç´¢å¼•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        use_hybrid = self.config.get("search", {}).get("use_hybrid_search", True)
        if use_hybrid:
            console.print("ğŸ”¨ æ„å»º Embedding ç´¢å¼•...", style="yellow")
            await stage2_index_building.build_emb_index(
                config=exp_config,
                data_dir=memcells_dir,
                emb_save_dir=emb_index_dir,
            )
            console.print("âœ… Embedding ç´¢å¼•æ„å»ºå®Œæˆ", style="green")
        
        # ========== æ–¹æ¡ˆ Aï¼šè¿”å›ç´¢å¼•å…ƒæ•°æ®ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰ ==========
        # ä¸åŠ è½½ç´¢å¼•åˆ°å†…å­˜ï¼Œåªè¿”å›è·¯å¾„å’Œå…ƒæ•°æ®
        index_metadata = {
            "type": "lazy_load",  # æ ‡è®°ä¸ºå»¶è¿ŸåŠ è½½
            "memcells_dir": str(memcells_dir),
            "bm25_index_dir": str(bm25_index_dir),
            "emb_index_dir": str(emb_index_dir),
            "conversation_ids": [conv.conversation_id for conv in conversations],
            "use_hybrid_search": use_hybrid,
            "total_conversations": len(conversations),
        }
        
        console.print(f"\n{'='*60}", style="dim")
        console.print(f"âœ… Add é˜¶æ®µå®Œæˆ", style="bold green")
        console.print(f"   ğŸ“ MemCells: {memcells_dir}", style="dim")
        console.print(f"   ğŸ“ BM25 ç´¢å¼•: {bm25_index_dir}", style="dim")
        if use_hybrid:
            console.print(f"   ğŸ“ Embedding ç´¢å¼•: {emb_index_dir}", style="dim")
        console.print(f"   ğŸ’¡ ä½¿ç”¨å»¶è¿ŸåŠ è½½ç­–ç•¥ï¼ˆå†…å­˜å‹å¥½ï¼‰", style="cyan")
        console.print(f"{'='*60}\n", style="dim")
        
        return index_metadata
    
    async def search(self, query: str, conversation_id: str, index: Any, **kwargs) -> SearchResult:
        """
        Search é˜¶æ®µï¼šæ£€ç´¢ç›¸å…³ MemCells
        
        å»¶è¿ŸåŠ è½½ï¼šæŒ‰éœ€ä»æ–‡ä»¶åŠ è½½ç´¢å¼•ï¼ˆå†…å­˜å‹å¥½ï¼‰
        """
        # å»¶è¿ŸåŠ è½½ - ä»æ–‡ä»¶è¯»å–ç´¢å¼•
        bm25_index_dir = Path(index["bm25_index_dir"])
        emb_index_dir = Path(index["emb_index_dir"])
        
        # æŒ‰éœ€åŠ è½½ BM25 ç´¢å¼•
        bm25_file = bm25_index_dir / f"bm25_index_conv_{conversation_id}.pkl"
        if not bm25_file.exists():
            return SearchResult(
                query=query,
                conversation_id=conversation_id,
                results=[],
                retrieval_metadata={"error": "BM25 index not found"}
            )
        
        with open(bm25_file, "rb") as f:
            bm25_index_data = pickle.load(f)
        
        bm25 = bm25_index_data.get("bm25")
        docs = bm25_index_data.get("docs")
        
        # æŒ‰éœ€åŠ è½½ Embedding ç´¢å¼•
        emb_index = None
        if index.get("use_hybrid_search"):
            emb_file = emb_index_dir / f"embedding_index_conv_{conversation_id}.pkl"
            if emb_file.exists():
                with open(emb_file, "rb") as f:
                    emb_index = pickle.load(f)
        
        # è°ƒç”¨ stage3 æ£€ç´¢å®ç°
        search_config = self.config.get("search", {})
        retrieval_mode = search_config.get("mode", "agentic")
        
        exp_config = self._convert_config_to_experiment_config()
        # ä» exp_config è·å–æ­£ç¡®æ ¼å¼çš„ llm_config
        llm_config = exp_config.llm_config.get(exp_config.llm_service, {})
        
        if retrieval_mode == "agentic":
            # Agentic æ£€ç´¢
            top_results, metadata = await stage3_memory_retrivel.agentic_retrieval(
                query=query,
                config=exp_config,
                llm_provider=self.llm_provider,
                llm_config=llm_config,
                emb_index=emb_index,
                bm25=bm25,
                docs=docs,
            )
        elif retrieval_mode == "lightweight":
            # è½»é‡çº§æ£€ç´¢
            top_results, metadata = await stage3_memory_retrivel.lightweight_retrieval(
                query=query,
                emb_index=emb_index,
                bm25=bm25,
                docs=docs,
                config=exp_config,
            )
        else:
            # é»˜è®¤ä½¿ç”¨æ··åˆæ£€ç´¢
            top_results = await stage3_memory_retrivel.hybrid_search_with_rrf(
                query=query,
                emb_index=emb_index,
                bm25=bm25,
                docs=docs,
                top_n=20,
                emb_candidates=search_config.get("hybrid_emb_candidates", 100),
                bm25_candidates=search_config.get("hybrid_bm25_candidates", 100),
                rrf_k=search_config.get("hybrid_rrf_k", 60),
            )
            metadata = {}
        
        # è½¬æ¢ä¸ºè¯„æµ‹æ¡†æ¶éœ€è¦çš„æ ¼å¼
        results = []
        for doc, score in top_results:
            results.append({
                "content": doc.get("episode", ""),
                "score": float(score),
                "metadata": {
                    "subject": doc.get("subject", ""),
                    "summary": doc.get("summary", ""),
                }
            })
        
        return SearchResult(
            query=query,
            conversation_id=conversation_id,
            results=results,
            retrieval_metadata=metadata
        )
    
    async def answer(self, query: str, context: str, **kwargs) -> str:
        """
        Answer é˜¶æ®µï¼šç”Ÿæˆç­”æ¡ˆ
        
        è°ƒç”¨ stage4_response.py çš„å®ç°
        """
        # è°ƒç”¨ stage4 ç­”æ¡ˆç”Ÿæˆå®ç°
        exp_config = self._convert_config_to_experiment_config()
        
        answer = await stage4_response.locomo_response(
            llm_provider=self.llm_provider,
            context=context,
            question=query,
            experiment_config=exp_config,
        )
        
        return answer
    
    def get_system_info(self) -> Dict[str, Any]:
        """è¿”å›ç³»ç»Ÿä¿¡æ¯"""
        return {
            "name": "EverMemOS",
            "version": "1.0",
            "description": "EverMemOS memory system with agentic retrieval",
            "adapter": "Adapter connecting framework to EverMemOS implementation",
        }
    
    def _convert_config_to_experiment_config(self):
        """
        å°†è¯„æµ‹æ¡†æ¶çš„ config è½¬æ¢ä¸º ExperimentConfig æ ¼å¼
        """
        from evaluation.src.adapters.evermemos.config import ExperimentConfig
        import os
        
        exp_config = ExperimentConfig()
        
        # æ˜ å°„ LLM é…ç½®ï¼šå°† YAML çš„ llm è½¬æ¢ä¸º ExperimentConfig çš„ llm_config æ ¼å¼
        llm_cfg = self.config.get("llm", {})
        provider = llm_cfg.get("provider", "openai")
        
        exp_config.llm_service = provider
        exp_config.llm_config = {
            provider: {
                "llm_provider": provider,
                "model": llm_cfg.get("model", "gpt-4o-mini"),
                "api_key": llm_cfg.get("api_key") or os.getenv("LLM_API_KEY", ""),
                "base_url": llm_cfg.get("base_url") or os.getenv("LLM_BASE_URL", "https://api.openai.com/v1"),
                "temperature": llm_cfg.get("temperature", 0.3),
                "max_tokens": llm_cfg.get("max_tokens", 32768),
            }
        }
        
        # æ˜ å°„ Add é˜¶æ®µé…ç½®ï¼ˆåªè¦†ç›– YAML ä¸­æ˜¾å¼æŒ‡å®šçš„ï¼‰
        add_config = self.config.get("add", {})
        if "enable_semantic_extraction" in add_config:
            exp_config.enable_semantic_extraction = add_config["enable_semantic_extraction"]
        if "enable_clustering" in add_config:
            exp_config.enable_clustering = add_config["enable_clustering"]
        if "enable_profile_extraction" in add_config:
            exp_config.enable_profile_extraction = add_config["enable_profile_extraction"]
        
        # æ˜ å°„ Search é˜¶æ®µé…ç½®ï¼ˆåªè¦†ç›– YAML ä¸­æ˜¾å¼æŒ‡å®šçš„ï¼‰
        search_config = self.config.get("search", {})
        if "mode" in search_config:
            exp_config.retrieval_mode = search_config["mode"]
            exp_config.use_agentic_retrieval = (exp_config.retrieval_mode == "agentic")
        
        # å…¶ä»– search å‚æ•°ï¼ˆå¦‚æœ YAML ä¸­æœ‰æŒ‡å®šæ‰è¦†ç›–ï¼‰
        search_param_mapping = {
            "use_hybrid_search": "use_hybrid_search",
            "use_reranker": "use_reranker",
            "hybrid_emb_candidates": "hybrid_emb_candidates",
            "hybrid_bm25_candidates": "hybrid_bm25_candidates",
            "hybrid_rrf_k": "hybrid_rrf_k",
            "reranker_top_n": "reranker_top_n",
            "reranker_batch_size": "reranker_batch_size",
            "reranker_max_retries": "reranker_max_retries",
            "reranker_retry_delay": "reranker_retry_delay",
            "reranker_timeout": "reranker_timeout",
            "reranker_fallback_threshold": "reranker_fallback_threshold",
            "reranker_instruction": "reranker_instruction",
            "reranker_concurrent_batches": "reranker_concurrent_batches",
        }
        for yaml_key, config_attr in search_param_mapping.items():
            if yaml_key in search_config:
                setattr(exp_config, config_attr, search_config[yaml_key])
        
        # ç‰¹æ®Šå¤„ç†ï¼šuse_emb ä¸ use_hybrid_search å…³è”
        if "use_hybrid_search" in search_config:
            exp_config.use_emb = search_config["use_hybrid_search"]
        
        # æ˜ å°„ Answer é˜¶æ®µé…ç½®ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        answer_config = self.config.get("answer", {})
        if "max_retries" in answer_config:
            exp_config.max_retries = answer_config["max_retries"]
        
        return exp_config
