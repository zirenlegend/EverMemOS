from typing import Dict, List
import json
import os
import sys
import uuid
import asyncio
import time
from rich.progress import (
    Progress,
    SpinnerColumn,
    TextColumn,
    BarColumn,
    TaskProgressColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
    MofNCompleteColumn,
)
from rich.console import Console


from common_utils.datetime_utils import (
    to_iso_format,
    from_iso_format,
    from_timestamp,
    get_now_with_timezone,
)
from memory_layer.llm.llm_provider import LLMProvider
from memory_layer.memcell_extractor.base_memcell_extractor import RawData, MemCell
from memory_layer.memcell_extractor.conv_memcell_extractor import (
    ConvMemCellExtractor,
    ConversationMemCellExtractRequest,
)
from memory_layer.memory_extractor.episode_memory_extractor import (
    EpisodeMemoryExtractRequest,
    EpisodeMemoryExtractor,
)
from memory_layer.memory_extractor.event_log_extractor import EventLogExtractor
from memory_layer.types import RawDataType

# æ–°å¢žï¼šèšç±»å’Œ Profile ç®¡ç†ç»„ä»¶
from memory_layer.cluster_manager import (
    ClusterManager,
    ClusterManagerConfig,
    InMemoryClusterStorage,
)
from memory_layer.profile_manager import (
    ProfileManager,
    ProfileManagerConfig,
    ScenarioType,
    InMemoryProfileStorage,
)

from evaluation.src.adapters.evermemos.config import ExperimentConfig
from datetime import datetime, timedelta
from pathlib import Path


def parse_locomo_timestamp(timestamp_str: str) -> datetime:
    """Parse LoComo timestamp format to datetime object."""
    timestamp_str = timestamp_str.replace("\\s+", " ").strip()
    dt = datetime.strptime(timestamp_str, "%I:%M %p on %d %B, %Y")
    return dt


def raw_data_load(locomo_data_path: str) -> Dict[str, List[RawData]]:
    with open(locomo_data_path, "r") as f:
        data = json.load(f)

    # data = [data[2]]
    # data = [data[0], data[1], data[2]]
    raw_data_dict = {}

    conversations = [data[i]['conversation'] for i in range(len(data))]
    print(f"   ðŸ“… Found {len(conversations)} conversations")
    for con_id, conversation in enumerate(conversations):
        messages = []
        # print(conversation.keys())
        session_keys = sorted(
            [
                key
                for key in conversation
                if key.startswith("session_") and not key.endswith("_date_time")
            ]
        )

        print(f"   ðŸ“… Found {len(session_keys)} sessions")
        print(
            f"   ðŸŽ­ Speakers: {conversation.get('speaker_a', 'Unknown')} & {conversation.get('speaker_b', 'Unknown')}"
        )
        speaker_name_to_id = {}
        for session_key in session_keys:
            session_messages = conversation[session_key]
            session_time_key = f"{session_key}_date_time"

            if session_time_key in conversation:
                # Parse session timestamp
                session_time = parse_locomo_timestamp(conversation[session_time_key])

                # Process each message in this session
                for i, msg in enumerate(session_messages):
                    # Generate timestamp for this message (session time + message offset)
                    msg_timestamp = session_time + timedelta(
                        seconds=i * 30
                    )  # 30 seconds between messages
                    iso_timestamp = to_iso_format(msg_timestamp)

                    # Generate unique speaker_id for this conversation
                    speaker_name = msg["speaker"]
                    if speaker_name not in speaker_name_to_id:
                        # Generate unique ID: {name}_{conversation_index}
                        unique_id = f"{speaker_name.lower().replace(' ', '_')}_{con_id}"
                        speaker_name_to_id[speaker_name] = unique_id

                    # Process content with image information if present
                    content = msg["text"]
                    if msg.get("img_url"):
                        blip_caption = msg.get("blip_caption", "an image")
                        content = f"[{speaker_name} shared an image: {blip_caption}] {content}"

                    message = {
                        "speaker_id": speaker_name_to_id[speaker_name],
                        "user_name": speaker_name,
                        "speaker_name": speaker_name,
                        "content": content,
                        "timestamp": iso_timestamp,
                        "original_timestamp": conversation[session_time_key],
                        "dia_id": msg["dia_id"],
                        "session": session_key,
                    }
                    # Add optional fields if present
                    for optional_field in ["img_url", "blip_caption", "query"]:
                        if optional_field in msg:
                            message[optional_field] = msg[optional_field]
                    messages.append(message)
            # messages = messages[:30]
        raw_data_dict[str(con_id)] = messages

        print(
            f"   âœ… Converted {len(messages)} messages from {len(session_keys)} sessions"
        )

    return raw_data_dict


def convert_conversation_to_raw_data_list(conversation: list) -> List[RawData]:
    raw_data_list = []
    for msg in conversation:
        raw_data_list.append(RawData(content=msg, data_id=str(uuid.uuid4())))
    return raw_data_list


async def memcell_extraction_from_conversation(
    raw_data_list: List[RawData],
    llm_provider: LLMProvider = None,
    memcell_extractor: ConvMemCellExtractor = None,
    smart_mask: bool = True,
    conv_id: str = None,  # æ·»åŠ ä¼šè¯IDç”¨äºŽè¿›åº¦æ¡æè¿°
    progress: Progress = None,  # æ·»åŠ è¿›åº¦æ¡å¯¹è±¡
    task_id: int = None,  # æ·»åŠ ä»»åŠ¡ID
    use_semantic_extraction: bool = False,  # æ–°å¢žï¼šæ˜¯å¦å¯ç”¨è¯­ä¹‰è®°å¿†æå–
) -> list:

    episode_extractor = EpisodeMemoryExtractor(llm_provider=llm_provider, use_eval_prompts=True)
    memcell_list = []
    speakers = {
        raw_data.content["speaker_id"]
        for raw_data in raw_data_list
        if isinstance(raw_data.content, dict) and "speaker_id" in raw_data.content
    }
    history_raw_data_list = []
    # raw_data_list = raw_data_list[:100]

    # å¤„ç†æ¶ˆæ¯
    total_messages = len(raw_data_list)
    smart_mask_flag = False

    for idx, raw_data in enumerate(raw_data_list):
        # æ›´æ–°è¿›åº¦æ¡ï¼ˆåœ¨å¤„ç†å‰æ›´æ–°ï¼Œæ˜¾ç¤ºæ­£åœ¨å¤„ç†ç¬¬å‡ æ¡ï¼‰
        if progress and task_id is not None:
            progress.update(task_id, completed=idx)

        if history_raw_data_list == [] or len(history_raw_data_list) == 1:
            history_raw_data_list.append(raw_data)
            continue

        if smart_mask and len(history_raw_data_list) > 5:
            smart_mask_flag = True
            # analysis_history = history_raw_data_list[:-1]
        else:
            # analysis_history = history_raw_data_list
            smart_mask_flag = False
        request = ConversationMemCellExtractRequest(
            history_raw_data_list=history_raw_data_list,
            new_raw_data_list=[raw_data],
            user_id_list=list(speakers),
            smart_mask_flag=smart_mask_flag,
            # group_id="group_1",
        )
        for i in range(5):
            try:
                result = await memcell_extractor.extract_memcell(
                    request,
                    use_semantic_extraction=use_semantic_extraction  # ä¼ é€’å¼€å…³
                )
                break
            except Exception as e:
                print('retry: ', i)
                if i == 4:
                    raise Exception("Memcell extraction failed")
                continue
        memcell_result = result[0]
        # print(f"   âœ… Memcell result: {memcell_result}")  # æ³¨é‡ŠæŽ‰é¿å…å¹²æ‰°è¿›åº¦æ¡
        if memcell_result is None:
            history_raw_data_list.append(raw_data)
        elif isinstance(memcell_result, MemCell):
            if smart_mask_flag:
                history_raw_data_list = [history_raw_data_list[-1], raw_data]
            else:
                history_raw_data_list = [raw_data]
            memcell_result.summary = memcell_result.episode[:200] + "..."
            memcell_list.append(memcell_result)
        else:
            console = Console()
            console.print("--------------------------------")
            console.print(f"   âŒ Memcell result: {memcell_result}", style="bold red")
            raise Exception("Memcell extraction failed")

    # å¤„ç†å®Œæˆï¼Œæ›´æ–°è¿›åº¦ä¸º100%
    if progress and task_id is not None:
        progress.update(task_id, completed=total_messages)

    if history_raw_data_list:
        memcell = MemCell(
            type=RawDataType.CONVERSATION,
            event_id=str(uuid.uuid4()),
            user_id_list=list(speakers),
            original_data=history_raw_data_list,
            timestamp=(memcell_list[-1].timestamp),
            summary="111",
        )
        episode_request = EpisodeMemoryExtractRequest(
            memcell_list=[memcell],
            user_id_list=request.user_id_list,
            participants=list(speakers),
            group_id=request.group_id,
        )

        episode_result = await episode_extractor.extract_memory(
            episode_request, use_group_prompt=True
        )
        memcell.episode = episode_result.episode
        memcell.subject = episode_result.subject
        memcell.summary = episode_result.episode[:200] + "..."
        memcell.original_data = episode_extractor.get_conversation_text(
            history_raw_data_list
        )
        original_data_list = []
        for raw_data in history_raw_data_list:
            original_data_list.append(memcell_extractor._data_process(raw_data))
        memcell.original_data = original_data_list
        memcell_list.append(memcell)

    return memcell_list


async def process_single_conversation(
    conv_id: str,
    conversation: list,
    save_dir: str,
    llm_provider: LLMProvider = None,
    event_log_extractor: EventLogExtractor = None,
    progress_counter: dict = None,
    progress: Progress = None,
    task_id: int = None,
    config: ExperimentConfig = None,  # æ–°å¢žï¼šä¼ å…¥é…ç½®
) -> tuple:
    """å¤„ç†å•ä¸ªä¼šè¯å¹¶è¿”å›žç»“æžœï¼ˆæ–°å¢žï¼šèšç±»å’Œ Profile æå–ï¼‰

    Args:
        conv_id: ä¼šè¯ID
        conversation: ä¼šè¯æ•°æ®
        save_dir: ä¿å­˜ç›®å½•
        llm_provider: å…±äº«çš„LLMæä¾›è€…å®žä¾‹
        event_log_extractor: äº‹ä»¶æ—¥å¿—æå–å™¨å®žä¾‹
        progress: è¿›åº¦æ¡å¯¹è±¡
        task_id: è¿›åº¦ä»»åŠ¡ID
        config: å®žéªŒé…ç½®ï¼ˆç”¨äºŽè¯»å–å¼€å…³ï¼‰

    Returns:
        tuple: (conv_id, memcell_list)
    """
    try:
        # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
        if progress and task_id is not None:
            progress.update(task_id, status="å¤„ç†ä¸­")

        # ===== æ ¹æ®é…ç½®åˆ›å»ºç»„ä»¶ =====
        cluster_mgr = None
        profile_mgr = None
        
        # åˆ›å»º MemCellExtractor
        raw_data_list = convert_conversation_to_raw_data_list(conversation)
        memcell_extractor = ConvMemCellExtractor(llm_provider=llm_provider, use_eval_prompts=True)
        
        # æ¡ä»¶åˆ›å»ºï¼šèšç±»ç®¡ç†å™¨ï¼ˆæ¯ä¸ªå¯¹è¯ç‹¬ç«‹ï¼‰
        if config and config.enable_clustering:
            cluster_storage = InMemoryClusterStorage(
                enable_persistence=True,
                persist_dir=Path(save_dir) / "clusters" / f"conv_{conv_id}"
            )
            cluster_config = ClusterManagerConfig(
                similarity_threshold=config.cluster_similarity_threshold,
                max_time_gap_days=config.cluster_max_time_gap_days,
                enable_persistence=True,
                persist_dir=str(Path(save_dir) / "clusters" / f"conv_{conv_id}"),
                clustering_algorithm="centroid"
            )
            cluster_mgr = ClusterManager(config=cluster_config, storage=cluster_storage)
            cluster_mgr.attach_to_extractor(memcell_extractor)
        
        # æ¡ä»¶åˆ›å»ºï¼šProfile ç®¡ç†å™¨
        if config and config.enable_profile_extraction and cluster_mgr:
            profile_storage = InMemoryProfileStorage(
                enable_persistence=True,
                persist_dir=Path(save_dir) / "profiles" / f"conv_{conv_id}",
                enable_versioning=True
            )
            
            # åŠ¨æ€è®¾ç½®åœºæ™¯ç±»åž‹
            scenario = ScenarioType.ASSISTANT if config.profile_scenario.lower() == "assistant" else ScenarioType.GROUP_CHAT
            
            profile_config = ProfileManagerConfig(
                scenario=scenario,
                min_confidence=config.profile_min_confidence,
                enable_versioning=True,
                auto_extract=True,
                batch_size=50,
            )
            
            profile_mgr = ProfileManager(
                llm_provider=llm_provider,
                config=profile_config,
                storage=profile_storage,
                group_id=f"locomo_conv_{conv_id}",
                group_name=f"LoComo Conversation {conv_id}"
            )
            
            # è®¾ç½®æœ€å° MemCells é˜ˆå€¼
            profile_mgr._min_memcells_threshold = config.profile_min_memcells
            
            # è¿žæŽ¥ç»„ä»¶
            profile_mgr.attach_to_cluster_manager(cluster_mgr)
        
        # æå– MemCellsï¼ˆæ ¹æ®é…ç½®å†³å®šæ˜¯å¦å¯ç”¨è¯­ä¹‰è®°å¿†ï¼‰
        use_semantic = config.enable_semantic_extraction if config else False
        memcell_list = await memcell_extraction_from_conversation(
            raw_data_list,
            llm_provider=llm_provider,
            memcell_extractor=memcell_extractor,
            conv_id=conv_id,
            progress=progress,
            task_id=task_id,
            use_semantic_extraction=use_semantic,  # ä¼ é€’è¯­ä¹‰è®°å¿†å¼€å…³
        )
        # print(f"   âœ… ä¼šè¯ {conv_id}: {len(memcell_list)} memcells extracted")  # æ³¨é‡ŠæŽ‰é¿å…å¹²æ‰°è¿›åº¦æ¡

        # åœ¨ä¿å­˜å‰è½¬æ¢æ—¶é—´æˆ³ä¸º datetime å¯¹è±¡
        for memcell in memcell_list:
            if hasattr(memcell, 'timestamp'):
                ts = memcell.timestamp
                if isinstance(ts, (int, float)):
                    # å°† int/float æ—¶é—´æˆ³è½¬æ¢ä¸ºå¸¦æ—¶åŒºçš„ datetime
                    memcell.timestamp = from_timestamp(ts)
                elif isinstance(ts, str):
                    # å°†å­—ç¬¦ä¸²æ—¶é—´æˆ³è½¬æ¢ä¸ºå¸¦æ—¶åŒºçš„ datetime
                    memcell.timestamp = from_iso_format(ts)
                elif not isinstance(ts, datetime):
                    # å¦‚æžœä¸æ˜¯é¢„æœŸçš„ç±»åž‹ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
                    memcell.timestamp = get_now_with_timezone()

        # ðŸ”¥ ä¼˜åŒ–ï¼šå¹¶å‘ç”Ÿæˆ event logï¼ˆæå‡é€Ÿåº¦ 10-20 å€ï¼‰
        if event_log_extractor:
            # å‡†å¤‡æ‰€æœ‰éœ€è¦æå– event log çš„ memcells
            memcells_with_episode = [
                (idx, memcell) 
                for idx, memcell in enumerate(memcell_list)
                if hasattr(memcell, 'episode') and memcell.episode
            ]
            
            # å®šä¹‰å•ä¸ª event log æå–ä»»åŠ¡
            async def extract_single_event_log(idx: int, memcell):
                try:
                    event_log = await event_log_extractor.extract_event_log(
                        episode_text=memcell.episode, 
                        timestamp=memcell.timestamp
                    )
                    return idx, event_log
                except Exception as e:
                    console = Console()
                    console.print(
                        f"\nâš ï¸  ç”Ÿæˆevent logå¤±è´¥ (Conv {conv_id}, Memcell {idx}): {e}",
                        style="yellow",
                    )
                    return idx, None
            
            # ðŸ”¥ å¹¶å‘æå–æ‰€æœ‰ event logsï¼ˆä½¿ç”¨ Semaphore æŽ§åˆ¶å¹¶å‘æ•°ï¼‰
            sem = asyncio.Semaphore(20)  # é™åˆ¶å¹¶å‘æ•°ä¸º 20ï¼ˆé¿å… API é™æµï¼‰
            
            async def extract_with_semaphore(idx, memcell):
                async with sem:
                    return await extract_single_event_log(idx, memcell)
            
            print(f"\nðŸ”¥ å¼€å§‹å¹¶å‘æå– {len(memcells_with_episode)} ä¸ª event logs...")
            event_log_tasks = [
                extract_with_semaphore(idx, memcell) 
                for idx, memcell in memcells_with_episode
            ]
            event_log_results = await asyncio.gather(*event_log_tasks)
            
            # å°† event logs å…³è”å›žå¯¹åº”çš„ memcells
            for original_idx, event_log in event_log_results:
                if event_log:
                    memcell_list[original_idx].event_log = event_log
            
            print(f"âœ… Event log æå–å®Œæˆ: {sum(1 for _, el in event_log_results if el)}/{len(event_log_results)} æˆåŠŸ")

        # ä¿å­˜å•ä¸ªä¼šè¯çš„ç»“æžœ
        memcell_dicts = []
        for memcell in memcell_list:
            memcell_dict = memcell.to_dict()
            # å¦‚æžœæœ‰event_logï¼Œæ·»åŠ åˆ°å­—å…¸ä¸­
            if hasattr(memcell, 'event_log') and memcell.event_log:
                memcell_dict['event_log'] = memcell.event_log.to_dict()
            memcell_dicts.append(memcell_dict)

        memcell_dicts = [memcell_dict for memcell_dict in memcell_dicts]
        # print(memcell_dicts)  # æ³¨é‡ŠæŽ‰å¤§é‡è¾“å‡º
        output_file = os.path.join(save_dir, f"memcell_list_conv_{conv_id}.json")
        with open(output_file, "w") as f:
            json.dump(memcell_dicts, f, ensure_ascii=False, indent=2)

        # ===== æ¡ä»¶å¯¼å‡ºï¼šèšç±»å’Œ Profile ç»“æžœ =====
        cluster_stats = {}
        profile_stats = {}
        profile_count = 0
        
        if cluster_mgr or profile_mgr:
            await asyncio.sleep(2)  # ç»™å¼‚æ­¥ä»»åŠ¡æ—¶é—´å®Œæˆ
        
        # å¯¼å‡ºèšç±»ç»“æžœï¼ˆå¦‚æžœå¯ç”¨ï¼‰
        if cluster_mgr:
            cluster_output_dir = Path(save_dir) / "clusters" / f"conv_{conv_id}"
            cluster_output_dir.mkdir(parents=True, exist_ok=True)
            await cluster_mgr.export_clusters(cluster_output_dir)
            cluster_stats = cluster_mgr.get_stats()
        
        # å¯¼å‡º Profilesï¼ˆå¦‚æžœå¯ç”¨ï¼‰
        if profile_mgr:
            profile_output_dir = Path(save_dir) / "profiles" / f"conv_{conv_id}"
            profile_count = await profile_mgr.export_profiles(profile_output_dir, include_history=True)
            profile_stats = profile_mgr.get_stats()
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_output = {
            "conv_id": conv_id,
            "memcells": len(memcell_list),
            "clustering_enabled": config.enable_clustering if config else False,
            "profile_enabled": config.enable_profile_extraction if config else False,
            "semantic_enabled": config.enable_semantic_extraction if config else False,
        }
        
        if cluster_stats:
            stats_output["clustering"] = cluster_stats
        if profile_stats:
            stats_output["profiles"] = profile_stats
            stats_output["profile_count"] = profile_count
        
        stats_file = Path(save_dir) / "stats" / f"conv_{conv_id}_stats.json"
        stats_file.parent.mkdir(parents=True, exist_ok=True)
        with open(stats_file, "w") as f:
            json.dump(stats_output, f, ensure_ascii=False, indent=2)

        # æ›´æ–°è¿›åº¦ï¼ˆé™é»˜ï¼Œé¿å…å¹²æ‰°è¿›åº¦æ¡ï¼‰
        if progress_counter:
            progress_counter['completed'] += 1
            # ä¸æ‰“å°ï¼Œé¿å…å¹²æ‰°è¿›åº¦æ¡

        return conv_id, memcell_list

    except Exception as e:
        # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯ï¼Œè¿™æ ·æˆ‘ä»¬èƒ½çŸ¥é“å…·ä½“é—®é¢˜
        console = Console()
        console.print(f"\nâŒ å¤„ç†ä¼šè¯ {conv_id} æ—¶å‡ºé”™: {e}", style="bold red")
        if progress_counter:
            progress_counter['completed'] += 1
            progress_counter['failed'] += 1
        import traceback

        traceback.print_exc()
        return conv_id, []


async def main():
    """ä¸»å‡½æ•° - å¹¶å‘å¤„ç†æ‰€æœ‰ä¼šè¯"""

    config = ExperimentConfig()
    llm_service = config.llm_service
    dataset_path = config.datase_path
    raw_data_dict = raw_data_load(dataset_path)

    CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
    os.makedirs(os.path.join(CURRENT_DIR, "results"), exist_ok=True)
    os.makedirs(
        os.path.join(CURRENT_DIR, "results", config.experiment_name), exist_ok=True
    )
    os.makedirs(
        os.path.join(CURRENT_DIR, "results", config.experiment_name, "memcells"),
        exist_ok=True,
    )
    save_dir = os.path.join(CURRENT_DIR, "results", config.experiment_name, "memcells")

    console = Console()
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    console.print("\n" + "=" * 60, style="bold cyan")
    console.print("å®žéªŒé…ç½®", style="bold cyan")
    console.print("=" * 60, style="bold cyan")
    console.print(f"å®žéªŒåç§°: {config.experiment_name}", style="cyan")
    console.print(f"æ•°æ®è·¯å¾„: {config.datase_path}", style="cyan")
    console.print(f"\nåŠŸèƒ½å¼€å…³:", style="bold yellow")
    console.print(f"  - è¯­ä¹‰è®°å¿†æå–: {'âœ… å¯ç”¨' if config.enable_semantic_extraction else 'âŒ ç¦ç”¨'}", 
                  style="green" if config.enable_semantic_extraction else "dim")
    console.print(f"  - èšç±»: {'âœ… å¯ç”¨' if config.enable_clustering else 'âŒ ç¦ç”¨'}", 
                  style="green" if config.enable_clustering else "dim")
    console.print(f"  - Profile æå–: {'âœ… å¯ç”¨' if config.enable_profile_extraction else 'âŒ ç¦ç”¨'}", 
                  style="green" if config.enable_profile_extraction else "dim")
    
    if config.enable_clustering:
        console.print(f"\nèšç±»é…ç½®:", style="bold")
        console.print(f"  - ç›¸ä¼¼åº¦é˜ˆå€¼: {config.cluster_similarity_threshold}", style="dim")
        console.print(f"  - æœ€å¤§æ—¶é—´é—´éš”: {config.cluster_max_time_gap_days} å¤©", style="dim")
    
    if config.enable_profile_extraction:
        console.print(f"\nProfile é…ç½®:", style="bold")
        console.print(f"  - åœºæ™¯: {config.profile_scenario}", style="dim")
        console.print(f"  - æœ€å°ç½®ä¿¡åº¦: {config.profile_min_confidence}", style="dim")
        console.print(f"  - æœ€å° MemCells: {config.profile_min_memcells}", style="dim")
    console.print("=" * 60 + "\n", style="bold cyan")
    
    # ðŸ”¥ æ–­ç‚¹ç»­ä¼ ï¼šæ£€æŸ¥å·²å®Œæˆçš„å¯¹è¯
    completed_convs = set()
    for conv_id in raw_data_dict.keys():
        output_file = os.path.join(save_dir, f"memcell_list_conv_{conv_id}.json")
        if os.path.exists(output_file):
            # éªŒè¯æ–‡ä»¶æœ‰æ•ˆæ€§ï¼ˆéžç©ºä¸”å¯è§£æžï¼‰
            try:
                with open(output_file, "r") as f:
                    data = json.load(f)
                    if data and len(data) > 0:  # ç¡®ä¿æœ‰æ•°æ®
                        completed_convs.add(conv_id)
                        console.print(f"âœ… è·³è¿‡å·²å®Œæˆçš„ä¼šè¯: {conv_id} ({len(data)} memcells)", style="green")
            except Exception as e:
                console.print(f"âš ï¸  ä¼šè¯ {conv_id} æ–‡ä»¶æŸåï¼Œå°†é‡æ–°å¤„ç†: {e}", style="yellow")
    
    # è¿‡æ»¤å‡ºéœ€è¦å¤„ç†çš„å¯¹è¯
    pending_raw_data_dict = {
        conv_id: conv_data 
        for conv_id, conv_data in raw_data_dict.items() 
        if conv_id not in completed_convs
    }
    
    console.print(f"\nðŸ“Š æ€»å…±å‘çŽ° {len(raw_data_dict)} ä¸ªä¼šè¯", style="bold cyan")
    console.print(f"âœ… å·²å®Œæˆ: {len(completed_convs)} ä¸ª", style="bold green")
    console.print(f"â³ å¾…å¤„ç†: {len(pending_raw_data_dict)} ä¸ª", style="bold yellow")
    
    if len(pending_raw_data_dict) == 0:
        console.print(f"\nðŸŽ‰ æ‰€æœ‰ä¼šè¯å·²å®Œæˆï¼Œæ— éœ€å¤„ç†ï¼", style="bold green")
        return
    
    total_messages = sum(len(conv) for conv in pending_raw_data_dict.values())
    console.print(f"ðŸ“ å¾…å¤„ç†æ¶ˆæ¯æ•°: {total_messages}", style="bold blue")
    console.print(f"ðŸš€ å¼€å§‹å¹¶å‘å¤„ç†å‰©ä½™ä¼šè¯...\n", style="bold green")

    # åˆ›å»ºå…±äº«çš„ LLM Provider å’Œ MemCell Extractor å®žä¾‹ï¼ˆè§£å†³è¿žæŽ¥ç«žäº‰é—®é¢˜ï¼‰
    console.print("âš™ï¸ åˆå§‹åŒ– LLM Provider...", style="yellow")
    console.print(f"   æ¨¡åž‹: {config.llm_config[llm_service]['model']}", style="dim")
    console.print(
        f"   Base URL: {config.llm_config[llm_service]['base_url']}", style="dim"
    )

    shared_llm_provider = LLMProvider(
        provider_type="openai",
        model=config.llm_config[llm_service]["model"],
        api_key=config.llm_config[llm_service]["api_key"],
        base_url=config.llm_config[llm_service]["base_url"],
        temperature=config.llm_config[llm_service]["temperature"],
        max_tokens=config.llm_config[llm_service]["max_tokens"],
    )

    # åˆ›å»ºå…±äº«çš„ Event Log Extractor
    console.print("âš™ï¸ åˆå§‹åŒ– Event Log Extractor...", style="yellow")
    shared_event_log_extractor = EventLogExtractor(llm_provider=shared_llm_provider)

    # ðŸ”¥ ä½¿ç”¨å¾…å¤„ç†çš„å¯¹è¯å­—å…¸ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
    # åˆ›å»ºè¿›åº¦è®¡æ•°å™¨
    progress_counter = {'total': len(pending_raw_data_dict), 'completed': 0, 'failed': 0}

    # ä½¿ç”¨ Rich è¿›åº¦æ¡
    start_time = time.time()

    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        MofNCompleteColumn(),  # æ˜¾ç¤º "3/10" æ ¼å¼
        TextColumn("â€¢"),
        TaskProgressColumn(),  # æ˜¾ç¤ºç™¾åˆ†æ¯”
        TextColumn("â€¢"),
        TimeElapsedColumn(),  # å·²ç”¨æ—¶é—´
        TextColumn("â€¢"),
        TimeRemainingColumn(),  # é¢„è®¡å‰©ä½™æ—¶é—´
        TextColumn("â€¢"),
        TextColumn("[bold blue]{task.fields[status]}"),
        console=console,
        transient=False,
        refresh_per_second=1,
    ) as progress:
        # åˆ›å»ºä¸»è¿›åº¦ä»»åŠ¡
        main_task = progress.add_task(
            "[bold cyan]ðŸŽ¯ æ€»è¿›åº¦",
            total=len(raw_data_dict),
            completed=len(completed_convs),  # ðŸ”¥ å·²å®Œæˆçš„æ•°é‡
            status="å¤„ç†ä¸­",
        )

        # ðŸ”¥ å…ˆæ·»åŠ å·²å®Œæˆçš„ä¼šè¯åˆ°è¿›åº¦æ¡ï¼ˆæ˜¾ç¤ºä¸ºå·²å®Œæˆï¼‰
        conversation_tasks = {}
        for conv_id in completed_convs:
            conv_task_id = progress.add_task(
                f"[green]Conv-{conv_id}",
                total=len(raw_data_dict[conv_id]),
                completed=len(raw_data_dict[conv_id]),  # 100%
                status="âœ… (å·²è·³è¿‡)",
            )
            conversation_tasks[conv_id] = conv_task_id

        # ðŸ”¥ ä¸ºå¾…å¤„ç†çš„ä¼šè¯åˆ›å»ºè¿›åº¦æ¡ä»»åŠ¡
        updated_tasks = []
        for conv_id, conversation in pending_raw_data_dict.items():
            # åˆ›å»ºæ¯ä¸ªä¼šè¯çš„è¿›åº¦æ¡
            conv_task_id = progress.add_task(
                f"[yellow]Conv-{conv_id}",  # ç®€åŒ–åç§°
                total=len(conversation),  # æ¶ˆæ¯æ€»æ•°
                completed=0,  # åˆå§‹åŒ–ä¸º0
                status="ç­‰å¾…",
            )
            conversation_tasks[conv_id] = conv_task_id

            # åˆ›å»ºå¤„ç†ä»»åŠ¡
            task = process_single_conversation(
                conv_id,
                conversation,
                save_dir,
                llm_provider=shared_llm_provider,
                event_log_extractor=shared_event_log_extractor,
                progress_counter=progress_counter,
                progress=progress,
                task_id=conv_task_id,
                config=config,  # ä¼ å…¥é…ç½®
            )
            updated_tasks.append(task)

        # å®šä¹‰å®Œæˆæ—¶æ›´æ–°å‡½æ•°
        async def run_with_completion(task, conv_id):
            result = await task
            progress.update(
                conversation_tasks[conv_id],
                status="âœ…",
                completed=progress.tasks[conversation_tasks[conv_id]].total,
            )
            progress.update(main_task, advance=1)
            return result

        # ðŸ”¥ å¹¶å‘æ‰§è¡Œæ‰€æœ‰å¾…å¤„ç†çš„ä»»åŠ¡
        if updated_tasks:
            results = await asyncio.gather(
                *[
                    run_with_completion(task, conv_id)
                    for (conv_id, _), task in zip(pending_raw_data_dict.items(), updated_tasks)
                ]
            )
        else:
            results = []
        # with open(os.path.join(save_dir, "response_info.json"), "w") as f:
        #     json.dump(shared_llm_provider.provider.response_info, f, ensure_ascii=False, indent=2)
        # æ›´æ–°ä¸»è¿›åº¦ä¸ºå®Œæˆ
        progress.update(main_task, status="âœ… å®Œæˆ")

    end_time = time.time()

    # ç»Ÿè®¡ç»“æžœ
    all_memcells = []
    successful_convs = 0
    for conv_id, memcell_list in results:
        if memcell_list:
            successful_convs += 1
            all_memcells.extend(memcell_list)

    console.print("\n" + "=" * 60, style="dim")
    console.print("ðŸ“Š å¤„ç†å®Œæˆç»Ÿè®¡:", style="bold")
    console.print(
        f"   âœ… æˆåŠŸå¤„ç†ä¼šè¯æ•°: {successful_convs}/{len(raw_data_dict)}", style="green"
    )
    console.print(f"   ðŸ“ æ€»å…±æå–çš„ memcells: {len(all_memcells)}", style="blue")
    console.print(f"   â±ï¸  æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’", style="yellow")
    console.print(
        f"   ðŸš€ å¹³å‡æ¯ä¼šè¯è€—æ—¶: {(end_time - start_time)/len(raw_data_dict):.2f} ç§’",
        style="cyan",
    )
    console.print("=" * 60, style="dim")

    # ä¿å­˜æ±‡æ€»ç»“æžœ
    all_memcells_dicts = [memcell.to_dict() for memcell in all_memcells]
    summary_file = os.path.join(save_dir, "memcell_list_all.json")
    with open(summary_file, "w") as f:
        json.dump(all_memcells_dicts, f, ensure_ascii=False, indent=2)
    console.print(f"\nðŸ’¾ æ±‡æ€»ç»“æžœå·²ä¿å­˜åˆ°: {summary_file}", style="green")

    # ===== æ–°å¢žï¼šæ±‡æ€»èšç±»å’Œ Profile ç»Ÿè®¡ =====
    # ç»Ÿè®¡æ‰€æœ‰ä¼šè¯çš„èšç±»å’Œ Profile ä¿¡æ¯
    total_clusters = 0
    total_profiles = 0
    cluster_stats_list = []
    profile_stats_list = []
    
    stats_dir = Path(save_dir) / "stats"
    if stats_dir.exists():
        for stats_file in stats_dir.glob("conv_*_stats.json"):
            try:
                with open(stats_file) as f:
                    conv_stats = json.load(f)
                total_clusters += conv_stats.get("clustering", {}).get("total_clusters", 0)
                total_profiles += conv_stats.get("profile_count", 0)
                cluster_stats_list.append(conv_stats.get("clustering", {}))
                profile_stats_list.append(conv_stats.get("profiles", {}))
            except Exception:
                pass
    
    # ä¿å­˜å¤„ç†æ‘˜è¦ï¼ˆæ–°å¢žèšç±»å’Œ Profile ç»Ÿè®¡ï¼‰
    summary = {
        "total_conversations": len(raw_data_dict),
        "successful_conversations": successful_convs,
        "total_memcells": len(all_memcells),
        "total_clusters": total_clusters,
        "total_profiles": total_profiles,
        "processing_time_seconds": end_time - start_time,
        "average_time_per_conversation": (end_time - start_time) / len(raw_data_dict),
        "conversation_results": {
            conv_id: len(memcell_list) for conv_id, memcell_list in results
        },
        "clustering_summary": {
            "total_clusters": total_clusters,
            "avg_clusters_per_conv": total_clusters / successful_convs if successful_convs > 0 else 0,
        },
        "profile_summary": {
            "total_profiles": total_profiles,
            "avg_profiles_per_conv": total_profiles / successful_convs if successful_convs > 0 else 0,
        },
    }
    summary_info_file = os.path.join(save_dir, "processing_summary.json")
    with open(summary_info_file, "w") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    console.print(f"ðŸ“Š å¤„ç†æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_info_file}", style="green")
    
    # æ‰“å°èšç±»å’Œ Profile ç»Ÿè®¡
    console.print(f"\nðŸ“Š èšç±»ç»Ÿè®¡:", style="bold cyan")
    console.print(f"   - æ€»èšç±»æ•°: {total_clusters}", style="cyan")
    console.print(f"   - å¹³å‡æ¯ä¼šè¯: {total_clusters / successful_convs if successful_convs > 0 else 0:.1f}", style="cyan")
    console.print(f"\nðŸ‘¤ Profile ç»Ÿè®¡:", style="bold green")
    console.print(f"   - æ€» Profiles: {total_profiles}", style="green")
    console.print(f"   - å¹³å‡æ¯ä¼šè¯: {total_profiles / successful_convs if successful_convs > 0 else 0:.1f}\n", style="green")


if __name__ == "__main__":
    asyncio.run(main())
