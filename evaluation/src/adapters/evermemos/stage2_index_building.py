import json
import os
import sys
import pickle
from pathlib import Path

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from rank_bm25 import BM25Okapi
import asyncio




from evaluation.src.adapters.evermemos.config import ExperimentConfig
from agentic_layer import vectorize_service


def ensure_nltk_data():
    """Ensure required NLTK data is downloaded."""
    try:
        nltk.data.find("tokenizers/punkt")
    except LookupError:
        print("Downloading punkt...")
        nltk.download("punkt", quiet=True)
    
    try:
        nltk.data.find("tokenizers/punkt_tab")
    except LookupError:
        print("Downloading punkt_tab...")
        nltk.download("punkt_tab", quiet=True)

    try:
        nltk.data.find("corpora/stopwords")
    except LookupError:
        print("Downloading stopwords...")
        nltk.download("stopwords", quiet=True)
    
    # ğŸ”¥ éªŒè¯ stopwords æ˜¯å¦å¯ç”¨
    try:
        from nltk.corpus import stopwords
        test_stopwords = stopwords.words("english")
        if not test_stopwords:
            raise ValueError("Stopwords is empty")
    except Exception as e:
        print(f"Warning: NLTK stopwords error: {e}")
        print("Re-downloading stopwords...")
        nltk.download("stopwords", quiet=False, force=True)


def build_searchable_text(doc: dict) -> str:
    """
    Build searchable text from a document with weighted fields.

    Priority:
    1. If event_log exists, use atomic_fact for indexing
    2. Otherwise, fall back to original fields:
       - "subject" corresponds to "title" (weight * 3)
       - "summary" corresponds to "summary" (weight * 2)
       - "episode" corresponds to "content" (weight * 1)
    """
    parts = []

    # ä¼˜å…ˆä½¿ç”¨event_logçš„atomic_factï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if doc.get("event_log") and doc["event_log"].get("atomic_fact"):
        atomic_facts = doc["event_log"]["atomic_fact"]
        if isinstance(atomic_facts, list):
            # ğŸ”¥ ä¿®å¤ï¼šå¤„ç†åµŒå¥—çš„ atomic_fact ç»“æ„
            # atomic_fact å¯èƒ½æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨æˆ–å­—å…¸åˆ—è¡¨ï¼ˆåŒ…å« "fact" å’Œ "embedding"ï¼‰
            for fact in atomic_facts:
                if isinstance(fact, dict) and "fact" in fact:
                    # æ–°æ ¼å¼ï¼š{"fact": "...", "embedding": [...]}
                    parts.append(fact["fact"])
                elif isinstance(fact, str):
                    # æ—§æ ¼å¼ï¼šçº¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼ˆå‘åå…¼å®¹ï¼‰
                    parts.append(fact)
            return " ".join(str(fact) for fact in parts if fact)

    # å›é€€åˆ°åŸæœ‰å­—æ®µï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
    # Title has highest weight (repeat 3 times)
    if doc.get("subject"):
        parts.extend([doc["subject"]] * 3)

    # Summary (repeat 2 times)
    if doc.get("summary"):
        parts.extend([doc["summary"]] * 2)

    # Content
    if doc.get("episode"):
        parts.append(doc["episode"])

    return " ".join(str(part) for part in parts if part)


def tokenize(text: str, stemmer, stop_words: set) -> list[str]:
    """
    NLTK-based tokenization with stemming and stopword removal.
    """
    if not text:
        return []

    tokens = word_tokenize(text.lower())

    processed_tokens = [
        stemmer.stem(token)
        for token in tokens
        if token.isalpha() and len(token) >= 2 and token not in stop_words
    ]

    return processed_tokens


def build_bm25_index(
    config: ExperimentConfig, data_dir: Path, bm25_save_dir: Path
) -> list[list[float]]:
    # --- NLTK Setup ---
    print("Ensuring NLTK data is available...")
    ensure_nltk_data()
    stemmer = PorterStemmer()
    stop_words = set(stopwords.words("english"))

    # --- Data Loading and Processing ---
    # corpus = [] # This line is removed as per the new_code
    # original_docs = [] # This line is removed as per the new_code

    print(f"Reading data from: {data_dir}")

    for i in range(config.num_conv):
        file_path = data_dir / f"memcell_list_conv_{i}.json"
        if not file_path.exists():
            print(f"Warning: File not found, skipping: {file_path}")
            continue

        print(f"\nProcessing {file_path.name}...")

        corpus = []
        original_docs = []

        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)

            for doc in data:
                original_docs.append(doc)
                searchable_text = build_searchable_text(doc)
                tokenized_text = tokenize(searchable_text, stemmer, stop_words)
                corpus.append(tokenized_text)

        if not corpus:
            print(
                f"Warning: No documents found in {file_path.name}. Skipping index creation."
            )
            continue

        print(f"Processed {len(original_docs)} documents from {file_path.name}.")

        # --- BM25 Indexing ---
        print(f"Building BM25 index for {file_path.name}...")
        bm25 = BM25Okapi(corpus)

        # --- Saving the Index ---
        index_data = {"bm25": bm25, "docs": original_docs}

        output_path = bm25_save_dir / f"bm25_index_conv_{i}.pkl"
        print(f"Saving index to: {output_path}")
        with open(output_path, "wb") as f:
            pickle.dump(index_data, f)


async def build_emb_index(config: ExperimentConfig, data_dir: Path, emb_save_dir: Path):
    """
    æ„å»º Embedding ç´¢å¼•ï¼ˆç¨³å®šç‰ˆï¼‰
    
    æ€§èƒ½ä¼˜åŒ–ç­–ç•¥ï¼š
    1. å—æ§å¹¶å‘ï¼šä¸¥æ ¼éµå®ˆ API Semaphore(5) é™åˆ¶
    2. ä¿å®ˆæ‰¹æ¬¡å¤§å°ï¼š256 ä¸ªæ–‡æœ¬/æ‰¹æ¬¡ï¼ˆé¿å…è¶…æ—¶ï¼‰
    3. ä¸²è¡Œæ‰¹æ¬¡æäº¤ï¼šåˆ†ç»„æäº¤ï¼Œé¿å…é˜Ÿåˆ—å †ç§¯
    4. è¿›åº¦ç›‘æ§ï¼šå®æ—¶æ˜¾ç¤ºå¤„ç†è¿›åº¦å’Œé€Ÿåº¦
    
    ä¼˜åŒ–æ•ˆæœï¼š
    - ç¨³å®šæ€§ä¼˜å…ˆï¼Œé¿å…è¶…æ—¶å’Œ API è¿‡è½½
    - API å¹¶å‘æ•°ï¼š5ï¼ˆå— vectorize_service.Semaphore æ§åˆ¶ï¼‰
    - æ‰¹æ¬¡å¤§å°ï¼š256ï¼ˆå¹³è¡¡ç¨³å®šæ€§å’Œæ•ˆç‡ï¼‰
    """
    # ğŸ”¥ ä¼˜åŒ–1ï¼šä¿å®ˆçš„æ‰¹æ¬¡å¤§å°ï¼ˆé¿å…è¶…æ—¶ï¼‰
    BATCH_SIZE = 256  # ä½¿ç”¨è¾ƒå¤§æ‰¹æ¬¡ï¼ˆå•æ¬¡ API è°ƒç”¨å¤„ç†æ›´å¤šï¼Œå‡å°‘è¯·æ±‚æ•°ï¼‰
    MAX_CONCURRENT_BATCHES = 5  # ğŸ”¥ ä¸¥æ ¼æ§åˆ¶å¹¶å‘æ•°ï¼ˆä¸ Semaphore(5) åŒ¹é…ï¼‰
    
    import time  # ç”¨äºæ€§èƒ½ç»Ÿè®¡
    
    for i in range(config.num_conv):
        file_path = data_dir / f"memcell_list_conv_{i}.json"
        if not file_path.exists():
            print(f"Warning: File not found, skipping: {file_path}")
            continue

        print(f"\n{'='*60}")
        print(f"Processing {file_path.name} for embedding...")
        print(f"{'='*60}")

        with open(file_path, "r", encoding="utf-8") as f:
            original_docs = json.load(f)

        texts_to_embed = []
        doc_field_map = []
        for doc_idx, doc in enumerate(original_docs):
            # ä¼˜å…ˆä½¿ç”¨event_logï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if doc.get("event_log") and doc["event_log"].get("atomic_fact"):
                atomic_facts = doc["event_log"]["atomic_fact"]
                if isinstance(atomic_facts, list) and atomic_facts:
                    # ğŸ”¥ å…³é”®æ”¹åŠ¨ï¼šæ¯ä¸ªatomic_factå•ç‹¬è®¡ç®—embeddingï¼ˆMaxSimç­–ç•¥ï¼‰
                    # è¿™æ ·å¯ä»¥ç²¾ç¡®åŒ¹é…åˆ°æŸä¸ªå…·ä½“çš„åŸå­äº‹å®ï¼Œé¿å…è¯­ä¹‰ç¨€é‡Š
                    for fact_idx, fact in enumerate(atomic_facts):
                        # ğŸ”¥ ä¿®å¤ï¼šå…¼å®¹ä¸¤ç§æ ¼å¼ï¼ˆå­—ç¬¦ä¸² / å­—å…¸ï¼‰
                        fact_text = None
                        if isinstance(fact, dict) and "fact" in fact:
                            # æ–°æ ¼å¼ï¼š{"fact": "...", "embedding": [...]}
                            fact_text = fact["fact"]
                        elif isinstance(fact, str):
                            # æ—§æ ¼å¼ï¼šçº¯å­—ç¬¦ä¸²
                            fact_text = fact
                        
                        # ç¡®ä¿factéç©º
                        if fact_text and fact_text.strip():
                            texts_to_embed.append(fact_text)
                            doc_field_map.append((doc_idx, f"atomic_fact_{fact_idx}"))
                    continue

            # å›é€€åˆ°åŸæœ‰å­—æ®µï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
            for field in ["subject", "summary", "episode"]:
                if text := doc.get(field):
                    texts_to_embed.append(text)
                    doc_field_map.append((doc_idx, field))

        if not texts_to_embed:
            print(
                f"Warning: No documents found in {file_path.name}. Skipping embedding creation."
            )
            continue

        total_texts = len(texts_to_embed)
        total_batches = (total_texts + BATCH_SIZE - 1) // BATCH_SIZE
        print(f"Total texts to embed: {total_texts}")
        print(f"Batch size: {BATCH_SIZE}")
        print(f"Total batches: {total_batches}")
        print(f"Max concurrent batches: {MAX_CONCURRENT_BATCHES}")
        print(f"\nStarting parallel embedding generation...")
        
        # ğŸ”¥ ä¼˜åŒ–2ï¼šç¨³å®šçš„æ‰¹æ¬¡å¤„ç†ï¼ˆé¿å…è¶…æ—¶ï¼‰
        start_time = time.time()
        
        async def process_batch_with_retry(batch_idx: int, batch_texts: list, max_retries: int = 3) -> tuple[int, list]:
            """å¤„ç†å•ä¸ªæ‰¹æ¬¡ï¼ˆå¼‚æ­¥ + é‡è¯•ï¼‰"""
            for attempt in range(max_retries):
                try:
                    # è°ƒç”¨ API è·å– embeddingsï¼ˆå— Semaphore(5) æ§åˆ¶å¹¶å‘æ•°ï¼‰
                    batch_embeddings = await vectorize_service.get_text_embeddings(batch_texts)
                    return (batch_idx, batch_embeddings)
                except Exception as e:
                    if attempt < max_retries - 1:
                        wait_time = 2.0 * (2 ** attempt)  # æŒ‡æ•°é€€é¿ï¼š2s, 4s
                        print(f"  âš ï¸  Batch {batch_idx + 1}/{total_batches} failed (attempt {attempt + 1}), retrying in {wait_time:.1f}s: {e}")
                        await asyncio.sleep(wait_time)
                    else:
                        print(f"  âŒ Batch {batch_idx + 1}/{total_batches} failed after {max_retries} attempts: {e}")
                        return (batch_idx, [])
        
        # ğŸ”¥ ä¼˜åŒ–3ï¼šåˆ†ç»„ä¸²è¡Œæäº¤ï¼ˆé¿å…é˜Ÿåˆ—å †ç§¯å¯¼è‡´è¶…æ—¶ï¼‰
        print(f"Processing {total_batches} batches in groups of {MAX_CONCURRENT_BATCHES}...")
        
        batch_results = []
        completed = 0
        
        # ğŸ”¥ å…³é”®ï¼šåˆ†ç»„æäº¤ï¼Œæ¯ç»„æœ€å¤š MAX_CONCURRENT_BATCHES ä¸ªå¹¶å‘
        for group_start in range(0, total_texts, BATCH_SIZE * MAX_CONCURRENT_BATCHES):
            # è®¡ç®—å½“å‰ç»„çš„æ‰¹æ¬¡èŒƒå›´
            group_end = min(group_start + BATCH_SIZE * MAX_CONCURRENT_BATCHES, total_texts)
            group_tasks = []
            
            for j in range(group_start, group_end, BATCH_SIZE):
                batch_idx = j // BATCH_SIZE
                batch_texts = texts_to_embed[j : j + BATCH_SIZE]
                task = process_batch_with_retry(batch_idx, batch_texts)
                group_tasks.append(task)
            
            # ğŸ”¥ å¹¶å‘å¤„ç†å½“å‰ç»„ï¼ˆæœ€å¤š MAX_CONCURRENT_BATCHES ä¸ªï¼‰
            print(f"  Group {group_start//BATCH_SIZE//MAX_CONCURRENT_BATCHES + 1}: Processing {len(group_tasks)} batches concurrently...")
            group_results = await asyncio.gather(*group_tasks, return_exceptions=False)
            batch_results.extend(group_results)
            
            completed += len(group_tasks)
            progress = (completed / total_batches) * 100
            print(f"  Progress: {completed}/{total_batches} batches ({progress:.1f}%)")
            
            # ğŸ”¥ ç»„é—´å»¶è¿Ÿï¼ˆç»™ API æœåŠ¡å™¨å–˜æ¯æ—¶é—´ï¼‰
            if group_end < total_texts:
                await asyncio.sleep(1.0)  # 1ç§’ç»„é—´å»¶è¿Ÿ
        
        # æŒ‰æ‰¹æ¬¡é¡ºåºé‡ç»„ç»“æœ
        all_embeddings = []
        for batch_idx, batch_embeddings in sorted(batch_results, key=lambda x: x[0]):
            all_embeddings.extend(batch_embeddings)
        
        elapsed_time = time.time() - start_time
        speed = total_texts / elapsed_time if elapsed_time > 0 else 0
        print(f"\nâœ… Embedding generation complete!")
        print(f"   - Total texts: {total_texts}")
        print(f"   - Total embeddings: {len(all_embeddings)}")
        print(f"   - Time elapsed: {elapsed_time:.2f}s")
        print(f"   - Speed: {speed:.1f} texts/sec")
        print(f"   - Average batch time: {elapsed_time/total_batches:.2f}s")
        
        # éªŒè¯ç»“æœå®Œæ•´æ€§
        if len(all_embeddings) != total_texts:
            print(f"   âš ï¸  Warning: Expected {total_texts} embeddings, got {len(all_embeddings)}")
        else:
            print(f"   âœ“ All embeddings generated successfully")

        # Re-associate embeddings with their original documents and fields
        # ğŸ”¥ æ”¹è¿›ï¼šæ”¯æŒæ¯ä¸ªæ–‡æ¡£æœ‰å¤šä¸ªatomic_fact embeddingsï¼ˆç”¨äºMaxSimç­–ç•¥ï¼‰
        doc_embeddings = [{"doc": doc, "embeddings": {}} for doc in original_docs]
        
        for (doc_idx, field), emb in zip(doc_field_map, all_embeddings):
            # å¦‚æœæ˜¯atomic_factå­—æ®µï¼Œä¿å­˜ä¸ºåˆ—è¡¨ï¼ˆæ”¯æŒå¤šä¸ªatomic_factï¼‰
            if field.startswith("atomic_fact_"):
                if "atomic_facts" not in doc_embeddings[doc_idx]["embeddings"]:
                    doc_embeddings[doc_idx]["embeddings"]["atomic_facts"] = []
                doc_embeddings[doc_idx]["embeddings"]["atomic_facts"].append(emb)
            else:
                # å…¶ä»–å­—æ®µç›´æ¥ä¿å­˜
                doc_embeddings[doc_idx]["embeddings"][field] = emb

        # The final structure of the saved .pkl file will be a list of dicts:
        # [
        #     {
        #         "doc": { ... original document ... },
        #         "embeddings": {
        #             "atomic_facts": [  # ğŸ”¥ æ–°å¢ï¼šatomic_fact embeddingsåˆ—è¡¨ï¼ˆç”¨äºMaxSimï¼‰
        #                 [ ... embedding vector for fact 0 ... ],
        #                 [ ... embedding vector for fact 1 ... ],
        #                 ...
        #             ],
        #             "subject": [ ... embedding vector ... ],  # å‘åå…¼å®¹çš„ä¼ ç»Ÿå­—æ®µ
        #             "summary": [ ... embedding vector ... ],
        #             "episode": [ ... embedding vector ... ]
        #         }
        #     },
        #     ...
        # ]
        output_path = emb_save_dir / f"embedding_index_conv_{i}.pkl"
        emb_save_dir.mkdir(parents=True, exist_ok=True)
        print(f"Saving embeddings to: {output_path}")
        with open(output_path, "wb") as f:
            pickle.dump(doc_embeddings, f)


async def main():
    """Main function to build and save the BM25 index."""
    # --- Configuration ---
    # The directory containing the JSON files
    config = ExperimentConfig()
    data_dir = Path(__file__).parent / "results" / config.experiment_name / "memcells"
    bm25_save_dir = (
        Path(__file__).parent / "results" / config.experiment_name / "bm25_index"
    )
    emb_save_dir = (
        Path(__file__).parent / "results" / config.experiment_name / "vectors"
    )
    os.makedirs(bm25_save_dir, exist_ok=True)
    os.makedirs(emb_save_dir, exist_ok=True)
    build_bm25_index(config, data_dir, bm25_save_dir)
    if config.use_emb:
        await build_emb_index(config, data_dir, emb_save_dir)
    # data_dir = Path("/Users/admin/Documents/Projects/b001-memsys/evaluation/locomo_evaluation/results/locomo_evaluation_0/")

    # Where to save the final index file
    # output_path = data_dir / "bm25_index.pkl" # This line is removed as per the new_code

    print("\nAll indexing complete!")


if __name__ == "__main__":
    asyncio.run(main())
