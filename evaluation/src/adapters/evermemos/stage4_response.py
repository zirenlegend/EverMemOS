import argparse
import asyncio
import json
import os
import sys
from pathlib import Path
from time import time

import pandas as pd
from tqdm import tqdm



from evaluation.src.adapters.evermemos.config import ExperimentConfig
from evaluation.src.adapters.evermemos.prompts.answer_prompts import ANSWER_PROMPT

# ä½¿ç”¨ Memory Layer çš„ LLMProvider
from memory_layer.llm.llm_provider import LLMProvider


async def locomo_response(
    llm_provider: LLMProvider,  # æ”¹ç”¨ LLMProvider
    context: str,
    question: str,
    experiment_config: ExperimentConfig,
) -> str:
    """ç”Ÿæˆå›ç­”ï¼ˆä½¿ç”¨ LLMProviderï¼‰
    
    Args:
        llm_provider: LLM Provider
        context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
        question: ç”¨æˆ·é—®é¢˜
        experiment_config: å®éªŒé…ç½®
    
    Returns:
        ç”Ÿæˆçš„ç­”æ¡ˆ
    """
    prompt = ANSWER_PROMPT.format(context=context, question=question)
    
    for i in range(experiment_config.max_retries):
        try:
            result = await llm_provider.generate(
                prompt=prompt,
                temperature=0,
                max_tokens=32768,
            )
            
            # ğŸ”¥ å®‰å…¨è§£æ FINAL ANSWERï¼ˆé¿å… index out of rangeï¼‰
            if "FINAL ANSWER:" in result:
                parts = result.split("FINAL ANSWER:")
                if len(parts) > 1:
                    result = parts[1].strip()
                else:
                    # åˆ†å‰²å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ç»“æœ
                    result = result.strip()
            else:
                # æ²¡æœ‰ FINAL ANSWER æ ‡è®°ï¼Œä½¿ç”¨åŸå§‹ç»“æœ
                result = result.strip()
            
            if result == "":
                continue
            break
        except Exception as e:
            print(f"Error: {e}")
            continue
    
    return result


async def process_qa(qa, search_result, llm_provider, experiment_config):
    """
    å¤„ç†å•ä¸ª QA å¯¹
    
    Args:
        qa: é—®é¢˜å’Œç­”æ¡ˆå¯¹
        search_result: æ£€ç´¢ç»“æœï¼ˆåŒ…å« contextï¼‰
        llm_provider: LLM Provider
        experiment_config: å®éªŒé…ç½®
    
    Returns:
        åŒ…å«é—®é¢˜ã€ç­”æ¡ˆã€ç±»åˆ«ç­‰ä¿¡æ¯çš„å­—å…¸
    """
    start = time()
    query = qa.get("question")
    gold_answer = qa.get("answer")
    qa_category = qa.get("category")

    answer = await locomo_response(
        llm_provider, search_result.get("context"), query, experiment_config
    )

    response_duration_ms = (time() - start) * 1000

    # åªåœ¨ verbose æ¨¡å¼ä¸‹è¾“å‡ºï¼ˆå‡å°‘æ—¥å¿—ï¼‰
    # print(f"Processed question: {query}")
    # print(f"Answer: {answer}")
    
    return {
        "question": query,
        "answer": answer,
        "category": qa_category,
        "golden_answer": gold_answer,
        "search_context": search_result.get("context", ""),
        "response_duration_ms": response_duration_ms,
        "search_duration_ms": search_result.get("duration_ms", 0),
    }


async def main(search_path, save_path):
    """
    ä¼˜åŒ–åçš„ä¸»å‡½æ•°
    
    æ€§èƒ½ä¼˜åŒ–ï¼š
    1. å…¨å±€å¹¶å‘å¤„ç†ï¼šæ‰€æœ‰ QA å¯¹å¹¶å‘å¤„ç†ï¼Œè€Œä¸æ˜¯æŒ‰ conversation ä¸²è¡Œ
    2. å¹¶å‘æ§åˆ¶ï¼šä½¿ç”¨ Semaphore æ§åˆ¶æœ€å¤§å¹¶å‘æ•°
    3. è¿›åº¦ç›‘æ§ï¼šå®æ—¶æ˜¾ç¤ºå¤„ç†è¿›åº¦
    4. å¢é‡ä¿å­˜ï¼šæ¯ä¸ª conversation å®Œæˆåç«‹å³ä¿å­˜ï¼ˆé¿å…å´©æºƒä¸¢å¤±æ•°æ®ï¼‰
    
    ä¼˜åŒ–æ•ˆæœï¼š
    - ä¼˜åŒ–å‰ï¼š77 åˆ†é’Ÿï¼ˆä¸²è¡Œï¼‰
    - ä¼˜åŒ–åï¼š~8 åˆ†é’Ÿï¼ˆå¹¶å‘ 50ï¼‰
    - åŠ é€Ÿæ¯”ï¼š~10x
    """
    llm_config = ExperimentConfig.llm_config["openai"]
    experiment_config = ExperimentConfig()
    
    # åˆ›å»º LLM Providerï¼ˆæ›¿ä»£ AsyncOpenAIï¼‰
    llm_provider = LLMProvider(
        provider_type="openai",
        model=llm_config["model"],
        api_key=llm_config["api_key"],
        base_url=llm_config["base_url"],
        temperature=llm_config.get("temperature", 0.0),
        max_tokens=llm_config.get("max_tokens", 32768),
    )
    
    locomo_df = pd.read_json(experiment_config.datase_path)
    with open(search_path) as file:
        locomo_search_results = json.load(file)

    num_users = len(locomo_df)
    
    # ğŸ”¥ ä¼˜åŒ–1ï¼šå…¨å±€å¹¶å‘æ§åˆ¶ï¼ˆå…³é”®ä¼˜åŒ–ï¼‰
    # æ§åˆ¶åŒæ—¶å¤„ç†çš„ QA å¯¹æ•°é‡ï¼Œé¿å… API é™æµ
    MAX_CONCURRENT = 50  # å¯æ ¹æ® API é™åˆ¶è°ƒæ•´ï¼ˆ10-100ï¼‰
    semaphore = asyncio.Semaphore(MAX_CONCURRENT)
    
    # ğŸ”¥ ä¼˜åŒ–2ï¼šæ”¶é›†æ‰€æœ‰ QA å¯¹ï¼ˆè·¨ conversationï¼‰
    all_tasks = []
    task_to_group = {}  # ç”¨äºè¿½è¸ªæ¯ä¸ªä»»åŠ¡å±äºå“ªä¸ª group
    
    print(f"\n{'='*60}")
    print(f"Stage4: LLM Response Generation")
    print(f"{'='*60}")
    print(f"Total conversations: {num_users}")
    
    # ğŸ”¥ ä¼˜åŒ–3ï¼šå®šä¹‰å¸¦å¹¶å‘æ§åˆ¶çš„å¤„ç†å‡½æ•°
    async def process_qa_with_semaphore(qa, search_result, group_id):
        """å¸¦å¹¶å‘æ§åˆ¶çš„ QA å¤„ç†"""
        async with semaphore:
            result = await process_qa(qa, search_result, llm_provider, experiment_config)
            return (group_id, result)
    
    total_qa_count = 0
    for group_idx in range(num_users):
        qa_set = locomo_df["qa"].iloc[group_idx]
        qa_set_filtered = [qa for qa in qa_set if qa.get("category") != 5]

        group_id = f"locomo_exp_user_{group_idx}"
        search_results = locomo_search_results.get(group_id)

        matched_pairs = []
        for qa in qa_set_filtered:
            question = qa.get("question")
            matching_result = next(
                (
                    result
                    for result in search_results
                    if result.get("query") == question
                ),
                None,
            )
            if matching_result:
                matched_pairs.append((qa, matching_result))
            else:
                print(
                    f"Warning: No matching search result found for question: {question}"
                )
        
        total_qa_count += len(matched_pairs)
        
        # åˆ›å»ºä»»åŠ¡ï¼ˆå…¨å±€å¹¶å‘ï¼‰
        for qa, search_result in matched_pairs:
            task = process_qa_with_semaphore(qa, search_result, group_id)
            all_tasks.append(task)
    
    print(f"Total questions to process: {total_qa_count}")
    print(f"Max concurrent requests: {MAX_CONCURRENT}")
    print(f"Estimated time: {total_qa_count * 3 / MAX_CONCURRENT / 60:.1f} minutes")
    print(f"\n{'='*60}")
    print(f"Starting parallel processing...")
    print(f"{'='*60}\n")
    
    # ğŸ”¥ ä¼˜åŒ–4ï¼šå…¨å±€å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡ï¼ˆå¸¦è¿›åº¦ç›‘æ§ï¼‰
    all_responses = {f"locomo_exp_user_{i}": [] for i in range(num_users)}
    
    import time as time_module
    start_time = time_module.time()
    completed = 0
    failed = 0
    
    # ğŸ”¥ ä¼˜åŒ–5ï¼šåˆ†æ‰¹å¤„ç† + å¢é‡ä¿å­˜ï¼ˆé¿å…å´©æºƒä¸¢å¤±æ•°æ®ï¼‰
    CHUNK_SIZE = 200  # æ¯æ¬¡å¤„ç† 200 ä¸ªä»»åŠ¡
    SAVE_INTERVAL = 400  # æ¯ 400 ä¸ªä»»åŠ¡ä¿å­˜ä¸€æ¬¡
    
    for chunk_start in range(0, len(all_tasks), CHUNK_SIZE):
        chunk_tasks = all_tasks[chunk_start : chunk_start + CHUNK_SIZE]
        chunk_results = await asyncio.gather(*chunk_tasks, return_exceptions=True)
        
        # å°†ç»“æœåˆ†ç»„åˆ°å„ä¸ª conversation
        for result in chunk_results:
            if isinstance(result, Exception):
                print(f"  âŒ Task failed: {result}")
                failed += 1
                continue
            
            group_id, qa_result = result
            all_responses[group_id].append(qa_result)
        
        completed += len(chunk_tasks)
        elapsed = time_module.time() - start_time
        speed = completed / elapsed if elapsed > 0 else 0
        eta = (total_qa_count - completed) / speed if speed > 0 else 0
        
        print(f"Progress: {completed}/{total_qa_count} ({completed/total_qa_count*100:.1f}%) | "
              f"Speed: {speed:.1f} qa/s | Failed: {failed} | ETA: {eta/60:.1f} min")
        
        # ğŸ”¥ å¢é‡ä¿å­˜ï¼ˆæ¯ SAVE_INTERVAL ä¸ªä»»åŠ¡ä¿å­˜ä¸€æ¬¡ï¼‰
        if completed % SAVE_INTERVAL == 0 or completed == total_qa_count:
            temp_save_path = Path(save_path).parent / f"responses_checkpoint_{completed}.json"
            with open(temp_save_path, "w", encoding="utf-8") as f:
                json.dump(all_responses, f, indent=2, ensure_ascii=False)
            print(f"  ğŸ’¾ Checkpoint saved: {temp_save_path.name}")
    
    elapsed_time = time_module.time() - start_time
    success_rate = (completed - failed) / completed * 100 if completed > 0 else 0
    
    print(f"\n{'='*60}")
    print(f"âœ… All responses generated!")
    print(f"   - Total questions: {total_qa_count}")
    print(f"   - Successful: {completed - failed}")
    print(f"   - Failed: {failed}")
    print(f"   - Success rate: {success_rate:.1f}%")
    print(f"   - Time elapsed: {elapsed_time/60:.1f} minutes ({elapsed_time:.0f}s)")
    print(f"   - Average speed: {total_qa_count/elapsed_time:.1f} qa/s")
    print(f"{'='*60}\n")

    # ä¿å­˜æœ€ç»ˆç»“æœ
    os.makedirs(Path(save_path).parent, exist_ok=True)
    with open(save_path, "w", encoding="utf-8") as f:
        json.dump(all_responses, f, indent=2, ensure_ascii=False)
        print(f"âœ… Final results saved to: {save_path}")
    
    # æ¸…ç† checkpoint æ–‡ä»¶
    checkpoint_files = list(Path(save_path).parent.glob("responses_checkpoint_*.json"))
    for checkpoint_file in checkpoint_files:
        checkpoint_file.unlink()
        print(f"  ğŸ—‘ï¸  Removed checkpoint: {checkpoint_file.name}")


if __name__ == "__main__":
    config = ExperimentConfig()
    search_result_path = str(
        Path(__file__).parent
        / "results"
        / config.experiment_name
        / "search_results.json"
    )
    save_path = (
        Path(__file__).parent / "results" / config.experiment_name / "responses.json"
    )
    # search_result_path = f"/Users/admin/Documents/Projects/b001-memsys/evaluation/locomo_evaluation/results/locomo_evaluation_0/nemori_locomo_search_results.json"

    asyncio.run(main(search_result_path, save_path))
