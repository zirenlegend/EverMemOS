import json
import os
import sys
import pickle
from pathlib import Path
from typing import List, Tuple, Optional
import time

import nltk
import numpy as np
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import asyncio
from tqdm import tqdm



from evaluation.src.adapters.evermemos.config import ExperimentConfig

# from evaluation.src.adapters.evermemos.tools.embedding_provider import EmbeddingProvider
# from evaluation.src.adapters.evermemos.tools.reranker_provider import RerankerProvider
from agentic_layer import vectorize_service
from agentic_layer import rerank_service

from evaluation.src.adapters.evermemos.tools import agentic_utils

# æ–°å¢žï¼šä½¿ç”¨ Memory Layer çš„ LLMProvider
from memory_layer.llm.llm_provider import LLMProvider


# This file depends on the rank_bm25 library.
# If you haven't installed it yet, run: pip install rank_bm25
TEMPLATE = """Episodes memories for conversation between {speaker_1} and {speaker_2}:

    {speaker_memories}
"""


def ensure_nltk_data():
    """Ensure required NLTK data is downloaded."""
    try:
        nltk.data.find("tokenizers/punkt")
    except LookupError:
        print("Downloading punkt...")
        nltk.download("punkt", quiet=True)
    
    try:
        nltk.data.find("tokenizers/punkt_tab")
    except LookupError:
        print("Downloading punkt_tab...")
        nltk.download("punkt_tab", quiet=True)

    try:
        nltk.data.find("corpora/stopwords")
    except LookupError:
        print("Downloading stopwords...")
        nltk.download("stopwords", quiet=True)
    
    # ðŸ”¥ éªŒè¯ stopwords æ˜¯å¦å¯ç”¨
    try:
        from nltk.corpus import stopwords
        test_stopwords = stopwords.words("english")
        if not test_stopwords:
            raise ValueError("Stopwords is empty")
    except Exception as e:
        print(f"Warning: NLTK stopwords error: {e}")
        print("Re-downloading stopwords...")
        nltk.download("stopwords", quiet=False, force=True)


def cosine_similarity(query_vec: np.ndarray, doc_vecs: np.ndarray) -> np.ndarray:
    """
    Calculates cosine similarity between a query vector and multiple document vectors.

    Args:
        query_vec: A 1D numpy array for the query.
        doc_vecs: A 2D numpy array where each row is a document vector.

    Returns:
        A 1D numpy array of cosine similarity scores.
    """
    # Calculate dot product
    dot_product = np.dot(doc_vecs, query_vec)

    # Calculate norms
    query_norm = np.linalg.norm(query_vec)
    doc_norms = np.linalg.norm(doc_vecs, axis=1)

    # Calculate cosine similarity, handling potential division by zero
    denominator = query_norm * doc_norms
    # Replace 0s in denominator with a small number to avoid division by zero
    denominator[denominator == 0] = 1e-9

    similarity_scores = dot_product / denominator

    return similarity_scores


def compute_maxsim_score(query_emb: np.ndarray, atomic_fact_embs: List[np.ndarray]) -> float:
    """
    è®¡ç®— query ä¸Žå¤šä¸ª atomic_fact embeddings çš„æœ€å¤§ç›¸ä¼¼åº¦ï¼ˆMaxSimç­–ç•¥ï¼‰
    
    MaxSim ç­–ç•¥çš„æ ¸å¿ƒæ€æƒ³ï¼š
    - åªè¦æœ‰ä¸€ä¸ª atomic_fact ä¸Ž query å¼ºç›¸å…³ï¼Œå°±è®¤ä¸ºæ•´ä¸ª event_log ç›¸å…³
    - é¿å…è¢«ä¸ç›¸å…³çš„ fact ç¨€é‡Šåˆ†æ•°
    - é€‚ç”¨äºŽè®°å¿†æ£€ç´¢åœºæ™¯ï¼Œç”¨æˆ·é€šå¸¸åªå…³æ³¨æŸä¸€ä¸ªæ–¹é¢
    
    ä¼˜åŒ–ï¼šä½¿ç”¨å‘é‡åŒ–çŸ©é˜µè¿ç®—ï¼Œé€Ÿåº¦æå‡ 2-3 å€
    
    Args:
        query_emb: query çš„ embedding å‘é‡ï¼ˆ1D numpy arrayï¼‰
        atomic_fact_embs: å¤šä¸ª atomic_fact çš„ embedding å‘é‡åˆ—è¡¨
    
    Returns:
        æœ€å¤§ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆfloatï¼ŒèŒƒå›´ [-1, 1]ï¼Œé€šå¸¸ä¸º [0, 1]ï¼‰
    """
    if not atomic_fact_embs:
        return 0.0
    
    query_norm = np.linalg.norm(query_emb)
    if query_norm == 0:
        return 0.0
    
    # ðŸ”¥ ä¼˜åŒ–ï¼šä½¿ç”¨çŸ©é˜µè¿ç®—ä»£æ›¿å¾ªçŽ¯ï¼ˆ2-3å€åŠ é€Ÿï¼‰
    try:
        # å°† list è½¬æ¢ä¸ºçŸ©é˜µï¼šshape = (n_facts, embedding_dim)
        fact_matrix = np.array(atomic_fact_embs)
        
        # æ‰¹é‡è®¡ç®—æ‰€æœ‰ fact çš„èŒƒæ•°
        fact_norms = np.linalg.norm(fact_matrix, axis=1)
        
        # è¿‡æ»¤é›¶å‘é‡
        valid_mask = fact_norms > 0
        if not np.any(valid_mask):
            return 0.0
        
        # å‘é‡åŒ–è®¡ç®—æ‰€æœ‰ç›¸ä¼¼åº¦
        # sims = (fact_matrix @ query_emb) / (query_norm * fact_norms)
        dot_products = np.dot(fact_matrix[valid_mask], query_emb)
        sims = dot_products / (query_norm * fact_norms[valid_mask])
        
        # è¿”å›žæœ€å¤§ç›¸ä¼¼åº¦
        return float(np.max(sims))
    
    except Exception as e:
        # å›žé€€åˆ°å¾ªçŽ¯æ–¹å¼ï¼ˆå…¼å®¹æ€§ä¿è¯ï¼‰
        similarities = []
        for fact_emb in atomic_fact_embs:
            fact_norm = np.linalg.norm(fact_emb)
            if fact_norm == 0:
                continue
            sim = np.dot(query_emb, fact_emb) / (query_norm * fact_norm)
            similarities.append(sim)
        return max(similarities) if similarities else 0.0


def tokenize(text: str, stemmer, stop_words: set) -> list[str]:
    """
    NLTK-based tokenization with stemming and stopword removal.
    This must be identical to the tokenization used during indexing.
    """
    if not text:
        return []

    tokens = word_tokenize(text.lower())

    processed_tokens = [
        stemmer.stem(token)
        for token in tokens
        if token.isalpha() and len(token) >= 2 and token not in stop_words
    ]

    return processed_tokens


def search_with_bm25_index(query: str, bm25, docs, top_n: int = 5):
    """
    Performs BM25 search using a pre-loaded index.
    """
    stemmer = PorterStemmer()
    stop_words = set(stopwords.words("english"))
    tokenized_query = tokenize(query, stemmer, stop_words)

    if not tokenized_query:
        print("Warning: Query is empty after tokenization.")
        return []

    doc_scores = bm25.get_scores(tokenized_query)
    results_with_scores = list(zip(docs, doc_scores))
    sorted_results = sorted(results_with_scores, key=lambda x: x[1], reverse=True)
    return sorted_results[:top_n]


def reciprocal_rank_fusion(
    emb_results: List[Tuple[dict, float]],
    bm25_results: List[Tuple[dict, float]],
    k: int = 60
) -> List[Tuple[dict, float]]:
    """
    ä½¿ç”¨ RRF (Reciprocal Rank Fusion) èžåˆ Embedding å’Œ BM25 æ£€ç´¢ç»“æžœ
    
    RRF æ˜¯ä¸€ç§æ— éœ€å½’ä¸€åŒ–çš„èžåˆç­–ç•¥ï¼Œå¯¹æŽ’åºä½ç½®æ•æ„Ÿã€‚
    å…¬å¼ï¼šRRF_score(doc) = Î£(1 / (k + rank_i))
    
    ä¼˜åŠ¿ï¼š
    1. æ— éœ€å½’ä¸€åŒ–åˆ†æ•°ï¼ˆEmbedding å’Œ BM25 åˆ†æ•°èŒƒå›´ä¸åŒï¼‰
    2. ç®€å•æœ‰æ•ˆï¼Œå·¥ä¸šç•Œå¹¿æ³›éªŒè¯ï¼ˆElasticsearch ç­‰ï¼‰
    3. å¯¹å¤´éƒ¨ç»“æžœæ›´æ•æ„Ÿï¼ˆé«˜æŽ’åè´¡çŒ®æ›´å¤§ï¼‰
    4. æ— éœ€è°ƒå‚ï¼ˆk=60 æ˜¯ç»éªŒæœ€ä¼˜å€¼ï¼‰
    
    Args:
        emb_results: Embedding æ£€ç´¢ç»“æžœ [(doc, score), ...]
        bm25_results: BM25 æ£€ç´¢ç»“æžœ [(doc, score), ...]
        k: RRF å¸¸æ•°ï¼Œé€šå¸¸ä½¿ç”¨ 60ï¼ˆç»éªŒå€¼ï¼‰
    
    Returns:
        èžåˆåŽçš„ç»“æžœ [(doc, rrf_score), ...]ï¼ŒæŒ‰ RRF åˆ†æ•°é™åºæŽ’åˆ—
    
    Example:
        emb_results = [(doc1, 0.92), (doc2, 0.87), (doc3, 0.81)]
        bm25_results = [(doc2, 15.3), (doc1, 12.7), (doc4, 10.2)]
        
        Doc1: 1/(60+1) + 1/(60+2) = 0.0323
        Doc2: 1/(60+2) + 1/(60+1) = 0.0323  
        Doc3: 1/(60+3) + 0        = 0.0159
        Doc4: 0        + 1/(60+3) = 0.0159
        
        èžåˆç»“æžœ: [(doc1, 0.0323), (doc2, 0.0323), (doc3, 0.0159), (doc4, 0.0159)]
    """
    # ä½¿ç”¨æ–‡æ¡£çš„å†…å­˜åœ°å€ä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼ˆé¿å…åºåˆ—åŒ–å¼€é”€ï¼‰
    # åŒæ—¶ä¿å­˜æ–‡æ¡£å¼•ç”¨ï¼Œç”¨äºŽæœ€åŽè¿”å›ž
    doc_rrf_scores = {}  # {doc_id: rrf_score}
    doc_map = {}         # {doc_id: doc}
    
    # å¤„ç† Embedding æ£€ç´¢ç»“æžœ
    for rank, (doc, score) in enumerate(emb_results, start=1):
        doc_id = id(doc)  # ä½¿ç”¨ Python å¯¹è±¡çš„å†…å­˜åœ°å€ä½œä¸ºå”¯ä¸€ ID
        if doc_id not in doc_map:
            doc_map[doc_id] = doc
        doc_rrf_scores[doc_id] = doc_rrf_scores.get(doc_id, 0.0) + 1.0 / (k + rank)
    
    # å¤„ç† BM25 æ£€ç´¢ç»“æžœ
    for rank, (doc, score) in enumerate(bm25_results, start=1):
        doc_id = id(doc)
        if doc_id not in doc_map:
            doc_map[doc_id] = doc
        doc_rrf_scores[doc_id] = doc_rrf_scores.get(doc_id, 0.0) + 1.0 / (k + rank)
    
    # æŒ‰ RRF åˆ†æ•°æŽ’åº
    sorted_docs = sorted(doc_rrf_scores.items(), key=lambda x: x[1], reverse=True)
    
    # è½¬æ¢å›ž (doc, score) æ ¼å¼
    fused_results = [(doc_map[doc_id], rrf_score) for doc_id, rrf_score in sorted_docs]
    
    return fused_results


def multi_rrf_fusion(
    results_list: List[List[Tuple[dict, float]]],
    k: int = 60
) -> List[Tuple[dict, float]]:
    """
    ä½¿ç”¨ RRF èžåˆå¤šä¸ªæŸ¥è¯¢çš„æ£€ç´¢ç»“æžœï¼ˆå¤šæŸ¥è¯¢èžåˆï¼‰
    
    ä¸ŽåŒè·¯ RRF ç±»ä¼¼ï¼Œä½†æ”¯æŒèžåˆä»»æ„æ•°é‡çš„æ£€ç´¢ç»“æžœã€‚
    æ¯ä¸ªç»“æžœé›†è´¡çŒ®çš„åˆ†æ•°ï¼š1 / (k + rank)
    
    åŽŸç†ï¼š
    - åœ¨å¤šä¸ªæŸ¥è¯¢ä¸­éƒ½æŽ’åé å‰çš„æ–‡æ¡£ â†’ åˆ†æ•°ç´¯ç§¯é«˜ â†’ æœ€ç»ˆæŽ’åé å‰
    - è¿™æ˜¯ä¸€ç§"æŠ•ç¥¨æœºåˆ¶"ï¼šå¤šä¸ªæŸ¥è¯¢éƒ½è®¤ä¸ºç›¸å…³çš„æ–‡æ¡£æ›´å¯èƒ½çœŸæ­£ç›¸å…³
    
    Args:
        results_list: å¤šä¸ªæ£€ç´¢ç»“æžœåˆ—è¡¨ [
            [(doc1, score), (doc2, score), ...],  # Query 1 ç»“æžœ
            [(doc3, score), (doc1, score), ...],  # Query 2 ç»“æžœ
            [(doc4, score), (doc2, score), ...],  # Query 3 ç»“æžœ
        ]
        k: RRF å¸¸æ•°ï¼ˆé»˜è®¤ 60ï¼‰
    
    Returns:
        èžåˆåŽçš„ç»“æžœ [(doc, rrf_score), ...]ï¼ŒæŒ‰ RRF åˆ†æ•°é™åºæŽ’åˆ—
    
    Example:
        Query 1 ç»“æžœ: [(doc_A, 0.9), (doc_B, 0.8), (doc_C, 0.7)]
        Query 2 ç»“æžœ: [(doc_B, 0.88), (doc_D, 0.82), (doc_A, 0.75)]
        Query 3 ç»“æžœ: [(doc_A, 0.92), (doc_E, 0.85), (doc_B, 0.80)]
        
        RRF åˆ†æ•°è®¡ç®—ï¼š
        doc_A: 1/(60+1) + 1/(60+3) + 1/(60+1) = 0.0323  â† åœ¨ Q1,Q2,Q3 éƒ½å‡ºçŽ°
        doc_B: 1/(60+2) + 1/(60+1) + 1/(60+3) = 0.0323  â† åœ¨ Q1,Q2,Q3 éƒ½å‡ºçŽ°
        doc_C: 1/(60+3) + 0        + 0        = 0.0159  â† åªåœ¨ Q1 å‡ºçŽ°
        doc_D: 0        + 1/(60+2) + 0        = 0.0161  â† åªåœ¨ Q2 å‡ºçŽ°
        doc_E: 0        + 0        + 1/(60+2) = 0.0161  â† åªåœ¨ Q3 å‡ºçŽ°
        
        èžåˆç»“æžœ: doc_A å’Œ doc_B æŽ’åæœ€é«˜ï¼ˆè¢«å¤šä¸ªæŸ¥è¯¢è®¤å¯ï¼‰
    """
    if not results_list:
        return []
    
    # å¦‚æžœåªæœ‰ä¸€ä¸ªç»“æžœé›†ï¼Œç›´æŽ¥è¿”å›ž
    if len(results_list) == 1:
        return results_list[0]
    
    # ä½¿ç”¨æ–‡æ¡£çš„å†…å­˜åœ°å€ä½œä¸ºå”¯ä¸€æ ‡è¯†
    doc_rrf_scores = {}  # {doc_id: rrf_score}
    doc_map = {}         # {doc_id: doc}
    
    # éåŽ†æ¯ä¸ªæŸ¥è¯¢çš„æ£€ç´¢ç»“æžœ
    for query_results in results_list:
        for rank, (doc, score) in enumerate(query_results, start=1):
            doc_id = id(doc)
            if doc_id not in doc_map:
                doc_map[doc_id] = doc
            # ç´¯åŠ  RRF åˆ†æ•°
            doc_rrf_scores[doc_id] = doc_rrf_scores.get(doc_id, 0.0) + 1.0 / (k + rank)
    
    # æŒ‰ RRF åˆ†æ•°æŽ’åº
    sorted_docs = sorted(doc_rrf_scores.items(), key=lambda x: x[1], reverse=True)
    
    # è½¬æ¢å›ž (doc, score) æ ¼å¼
    fused_results = [(doc_map[doc_id], rrf_score) for doc_id, rrf_score in sorted_docs]
    
    return fused_results


async def lightweight_retrieval(
    query: str,
    emb_index,
    bm25,
    docs,
    config: ExperimentConfig,
) -> Tuple[List[Tuple[dict, float]], dict]:
    """
    è½»é‡çº§å¿«é€Ÿæ£€ç´¢ï¼ˆæ—  LLM è°ƒç”¨ï¼Œçº¯ç®—æ³•æ£€ç´¢ï¼‰
    
    æµç¨‹ï¼š
    1. å¹¶è¡Œæ‰§è¡Œ Embedding å’Œ BM25 æ£€ç´¢
    2. å„å– Top-50 å€™é€‰
    3. ä½¿ç”¨ RRF èžåˆ
    4. è¿”å›ž Top-20 ç»“æžœ
    
    ä¼˜åŠ¿ï¼š
    - é€Ÿåº¦å¿«ï¼šæ—  LLM è°ƒç”¨ï¼Œçº¯å‘é‡/è¯æ³•æ£€ç´¢
    - æˆæœ¬ä½Žï¼šä¸æ¶ˆè€— LLM API è´¹ç”¨
    - ç¨³å®šï¼šæ— ç½‘ç»œä¾èµ–ï¼Œçº¯æœ¬åœ°è®¡ç®—
    
    é€‚ç”¨åœºæ™¯ï¼š
    - å¯¹å»¶è¿Ÿæ•æ„Ÿçš„åœºæ™¯
    - é¢„ç®—æœ‰é™çš„åœºæ™¯
    - æŸ¥è¯¢ç®€å•æ˜Žç¡®çš„åœºæ™¯
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        emb_index: Embedding ç´¢å¼•
        bm25: BM25 ç´¢å¼•
        docs: æ–‡æ¡£åˆ—è¡¨
        config: å®žéªŒé…ç½®
    
    Returns:
        (final_results, metadata)
    """
    start_time = time.time()
    
    metadata = {
        "retrieval_mode": "lightweight",
        "emb_count": 0,
        "bm25_count": 0,
        "final_count": 0,
        "total_latency_ms": 0.0,
    }
    
    # ========== å¹¶è¡Œæ‰§è¡Œ Embedding å’Œ BM25 æ£€ç´¢ ==========
    emb_task = search_with_emb_index(
        query, 
        emb_index, 
        top_n=config.lightweight_emb_top_n  # é»˜è®¤ 50
    )
    bm25_task = asyncio.to_thread(
        search_with_bm25_index, 
        query, 
        bm25, 
        docs, 
        config.lightweight_bm25_top_n  # é»˜è®¤ 50
    )
    
    emb_results, bm25_results = await asyncio.gather(emb_task, bm25_task)
    
    metadata["emb_count"] = len(emb_results)
    metadata["bm25_count"] = len(bm25_results)
    
    # ========== RRF èžåˆ ==========
    if not emb_results and not bm25_results:
        metadata["total_latency_ms"] = (time.time() - start_time) * 1000
        return [], metadata
    elif not emb_results:
        final_results = bm25_results[:config.lightweight_final_top_n]
    elif not bm25_results:
        final_results = emb_results[:config.lightweight_final_top_n]
    else:
        # ä½¿ç”¨ RRF èžåˆ
        fused_results = reciprocal_rank_fusion(
            emb_results, 
            bm25_results, 
            k=60  # æ ‡å‡† RRF å‚æ•°
        )
        final_results = fused_results[:config.lightweight_final_top_n]  # é»˜è®¤ 20
    
    metadata["final_count"] = len(final_results)
    metadata["total_latency_ms"] = (time.time() - start_time) * 1000
    
    return final_results, metadata


async def search_with_emb_index(
    query: str, 
    emb_index, 
    top_n: int = 5,
    query_embedding: Optional[np.ndarray] = None  # ðŸ”¥ æ”¯æŒé¢„è®¡ç®—çš„ embedding
):
    """
    ä½¿ç”¨ MaxSim ç­–ç•¥æ‰§è¡Œ embedding æ£€ç´¢
    
    å¯¹äºŽåŒ…å« atomic_facts çš„æ–‡æ¡£ï¼š
    - è®¡ç®— query ä¸Žæ¯ä¸ª atomic_fact çš„ç›¸ä¼¼åº¦
    - å–æœ€å¤§ç›¸ä¼¼åº¦ä½œä¸ºæ–‡æ¡£åˆ†æ•°ï¼ˆMaxSimç­–ç•¥ï¼‰
    
    å¯¹äºŽä¼ ç»Ÿæ–‡æ¡£ï¼š
    - å›žé€€åˆ°ä½¿ç”¨ subject/summary/episode å­—æ®µ
    - å–è¿™äº›å­—æ®µä¸­çš„æœ€å¤§ç›¸ä¼¼åº¦
    
    ä¼˜åŒ–ï¼šæ”¯æŒé¢„è®¡ç®—çš„ query embeddingï¼Œé¿å…é‡å¤ API è°ƒç”¨
    
    Args:
        query: æŸ¥è¯¢æ–‡æœ¬
        emb_index: é¢„æž„å»ºçš„ embedding ç´¢å¼•
        top_n: è¿”å›žçš„ç»“æžœæ•°é‡
        query_embedding: å¯é€‰çš„é¢„è®¡ç®— query embeddingï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
    
    Returns:
        æŽ’åºåŽçš„ (æ–‡æ¡£, åˆ†æ•°) åˆ—è¡¨
    """
    # èŽ·å– query çš„ embeddingï¼ˆå¦‚æžœæœªæä¾›åˆ™è°ƒç”¨ APIï¼‰
    if query_embedding is not None:
        query_vec = query_embedding
    else:
        query_vec = np.array(await vectorize_service.get_text_embedding(query))
    
    query_norm = np.linalg.norm(query_vec)
    
    # å¦‚æžœ query å‘é‡ä¸ºé›¶ï¼Œè¿”å›žç©ºç»“æžœ
    if query_norm == 0:
        return []
    
    # å­˜å‚¨æ¯ä¸ªæ–‡æ¡£çš„ MaxSim åˆ†æ•°
    doc_scores = []
    
    for item in emb_index:
        doc = item.get("doc")
        embeddings = item.get("embeddings", {})
        
        if not embeddings:
            continue
        
        # ä¼˜å…ˆä½¿ç”¨ atomic_factsï¼ˆMaxSimç­–ç•¥ï¼‰
        if "atomic_facts" in embeddings:
            atomic_fact_embs = embeddings["atomic_facts"]
            if atomic_fact_embs:
                # ðŸ”¥ æ ¸å¿ƒï¼šä½¿ç”¨ MaxSim è®¡ç®—åˆ†æ•°
                score = compute_maxsim_score(query_vec, atomic_fact_embs)
                doc_scores.append((doc, score))
                continue
        
        # å›žé€€åˆ°ä¼ ç»Ÿå­—æ®µï¼ˆä¿æŒå‘åŽå…¼å®¹ï¼‰
        # å¯¹äºŽä¼ ç»Ÿå­—æ®µï¼Œä¹Ÿä½¿ç”¨ MaxSim ç­–ç•¥ï¼ˆå–æœ€å¤§å€¼ï¼‰
        field_scores = []
        for field in ["subject", "summary", "episode"]:
            if field in embeddings:
                field_emb = embeddings[field]
                field_norm = np.linalg.norm(field_emb)
                
                if field_norm > 0:
                    sim = np.dot(query_vec, field_emb) / (query_norm * field_norm)
                    field_scores.append(sim)
        
        if field_scores:
            score = max(field_scores)
            doc_scores.append((doc, score))
    
    if not doc_scores:
        return []
    
    # æŒ‰åˆ†æ•°é™åºæŽ’åºå¹¶è¿”å›ž Top-N
    sorted_results = sorted(doc_scores, key=lambda x: x[1], reverse=True)
    return sorted_results[:top_n]


async def hybrid_search_with_rrf(
    query: str,
    emb_index,
    bm25,
    docs,
    top_n: int = 40,
    emb_candidates: int = 100,
    bm25_candidates: int = 100,
    rrf_k: int = 60,
    query_embedding: Optional[np.ndarray] = None  # ðŸ”¥ æ”¯æŒé¢„è®¡ç®—çš„ embedding
) -> List[Tuple[dict, float]]:
    """
    ä½¿ç”¨ RRF èžåˆ Embedding å’Œ BM25 æ£€ç´¢ç»“æžœï¼ˆæ··åˆæ£€ç´¢ï¼‰
    
    æ‰§è¡Œæµç¨‹ï¼š
    1. å¹¶è¡Œæ‰§è¡Œ Embedding (MaxSim) å’Œ BM25 æ£€ç´¢
    2. æ¯ç§æ–¹æ³•åˆ†åˆ«å¬å›ž top-N å€™é€‰æ–‡æ¡£
    3. ä½¿ç”¨ RRF èžåˆä¸¤ä¸ªç»“æžœé›†
    4. è¿”å›žèžåˆåŽçš„ Top-N æ–‡æ¡£
    
    ä¸ºä»€ä¹ˆä½¿ç”¨æ··åˆæ£€ç´¢ï¼š
    - Embedding: æ“…é•¿è¯­ä¹‰åŒ¹é…ï¼Œä½†å¯¹ç½•è§è¯å’Œç²¾ç¡®åŒ¹é…è¾ƒå¼±
    - BM25: æ“…é•¿ç²¾ç¡®åŒ¹é…å’Œç½•è§è¯ï¼Œä½†è¯­ä¹‰ç†è§£è¾ƒå¼±
    - RRF èžåˆ: ç»“åˆä¸¤è€…ä¼˜åŠ¿ï¼Œæå‡å¬å›žçŽ‡ 15-20%
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        emb_index: Embedding ç´¢å¼•
        bm25: BM25 ç´¢å¼•
        docs: æ–‡æ¡£åˆ—è¡¨ï¼ˆç”¨äºŽ BM25ï¼‰
        top_n: æœ€ç»ˆè¿”å›žçš„ç»“æžœæ•°é‡ï¼ˆé»˜è®¤ 40ï¼‰
        emb_candidates: Embedding æ£€ç´¢çš„å€™é€‰æ•°é‡ï¼ˆé»˜è®¤ 100ï¼‰
        bm25_candidates: BM25 æ£€ç´¢çš„å€™é€‰æ•°é‡ï¼ˆé»˜è®¤ 100ï¼‰
        rrf_k: RRF å‚æ•° kï¼ˆé»˜è®¤ 60ï¼Œç»éªŒæœ€ä¼˜å€¼ï¼‰
    
    Returns:
        èžåˆåŽçš„ Top-N ç»“æžœ [(doc, rrf_score), ...]
    
    Example:
        Query: "ä»–å–œæ¬¢åƒä»€ä¹ˆï¼Ÿ"
        
        Embedding Top-3:
        - (doc_A: "ç”¨æˆ·å–œçˆ±å·èœ", 0.92)  # è¯­ä¹‰åŒ¹é…"å–œæ¬¢"="å–œçˆ±"
        - (doc_B: "ç”¨æˆ·åå¥½æ¸…æ·¡å£å‘³", 0.78)
        - (doc_C: "æˆéƒ½æ˜¯ç¾Žé£Ÿä¹‹éƒ½", 0.65)
        
        BM25 Top-3:
        - (doc_A: "ç”¨æˆ·å–œçˆ±å·èœ", 15.3)  # ç²¾ç¡®åŒ¹é…"å–œæ¬¢"
        - (doc_D: "å–œæ¬¢åƒç«é”…", 12.7)  # ç²¾ç¡®åŒ¹é…"å–œæ¬¢åƒ"
        - (doc_E: "æœ€å–œæ¬¢çš„èœæ˜¯éº»å©†è±†è…", 10.2)
        
        RRF èžåˆ:
        - doc_A: åŒæ—¶åœ¨ä¸¤ä¸ªç»“æžœä¸­æŽ’åé å‰ â†’ æœ€é«˜åˆ† âœ…
        - doc_D: åªåœ¨ BM25 ä¸­æŽ’åé«˜
        - doc_B: åªåœ¨ Embedding ä¸­æŽ’åé«˜
        
        æœ€ç»ˆ: [(doc_A, 0.0323), (doc_D, 0.0161), (doc_B, 0.0161), ...]
    """
    # å¹¶è¡Œæ‰§è¡Œ Embedding å’Œ BM25 æ£€ç´¢ï¼ˆæé«˜æ•ˆçŽ‡ï¼‰
    emb_task = search_with_emb_index(
        query, emb_index, top_n=emb_candidates, query_embedding=query_embedding
    )
    bm25_task = asyncio.to_thread(search_with_bm25_index, query, bm25, docs, bm25_candidates)
    
    # ç­‰å¾…ä¸¤ä¸ªæ£€ç´¢ä»»åŠ¡å®Œæˆ
    emb_results, bm25_results = await asyncio.gather(emb_task, bm25_task)
    
    # å¦‚æžœå…¶ä¸­ä¸€ä¸ªæ£€ç´¢ç»“æžœä¸ºç©ºï¼Œè¿”å›žå¦ä¸€ä¸ª
    if not emb_results and not bm25_results:
        return []
    elif not emb_results:
        print(f"Warning: Embedding search returned no results for query: {query}")
        return bm25_results[:top_n]
    elif not bm25_results:
        print(f"Warning: BM25 search returned no results for query: {query}")
        return emb_results[:top_n]
    
    # ä½¿ç”¨ RRF èžåˆä¸¤ä¸ªæ£€ç´¢ç»“æžœ
    fused_results = reciprocal_rank_fusion(emb_results, bm25_results, k=rrf_k)
    
    # æ‰“å°èžåˆç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºŽè°ƒè¯•ï¼‰
    print(f"Hybrid search: Emb={len(emb_results)}, BM25={len(bm25_results)}, Fused={len(fused_results)}, Returning top-{top_n}")
    
    return fused_results[:top_n]


async def agentic_retrieval(
    query: str,
    config: ExperimentConfig,
    llm_provider: LLMProvider,  # æ”¹ç”¨ LLMProvider
    llm_config: dict,
    emb_index,
    bm25,
    docs,
) -> Tuple[List[Tuple[dict, float]], dict]:
    """
    Agentic å¤šè½®æ£€ç´¢ï¼ˆLLM å¼•å¯¼ï¼‰- æ–°æµç¨‹
    
    æµç¨‹ï¼š
    1. Round 1: æ··åˆæ£€ç´¢ â†’ Top 20 â†’ Rerank â†’ Top 5 â†’ LLM åˆ¤æ–­å……åˆ†æ€§
    2. å¦‚æžœå……åˆ†ï¼šè¿”å›žåŽŸå§‹ Top 20ï¼ˆrerank å‰çš„ï¼‰
    3. å¦‚æžœä¸å……åˆ†ï¼š
       - LLM ç”Ÿæˆæ”¹è¿›æŸ¥è¯¢
       - Round 2: æ£€ç´¢å¹¶åˆå¹¶åˆ° 40 ä¸ª
       - Rerank 40 ä¸ª â†’ è¿”å›žæœ€ç»ˆç»“æžœ
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        config: å®žéªŒé…ç½®
        llm_provider: LLM Provider (Memory Layer)
        llm_config: LLM é…ç½®å­—å…¸
        emb_index: Embedding ç´¢å¼•
        bm25: BM25 ç´¢å¼•
        docs: æ–‡æ¡£åˆ—è¡¨
    
    Returns:
        (final_results, metadata)
    """
    import time
    start_time = time.time()
    
    metadata = {
        "is_multi_round": False,
        "round1_count": 0,
        "round1_reranked_count": 0,
        "round2_count": 0,
        "is_sufficient": None,
        "reasoning": None,
        "refined_query": None,
        "final_count": 0,
        "total_latency_ms": 0.0,
    }
    
    print(f"\n{'='*60}")
    print(f"Agentic Retrieval: {query[:60]}...")
    print(f"{'='*60}")
    print(f"  [Start] Time: {time.strftime('%H:%M:%S')}")
    
    # ========== Round 1: æ··åˆæ£€ç´¢ Top 20 ==========
    print(f"  [Round 1] Hybrid search for Top 20...")
    
    round1_top20 = await hybrid_search_with_rrf(
        query=query,
        emb_index=emb_index,
        bm25=bm25,
        docs=docs,
        top_n=20,  # ðŸ”¥ åªå– Top 20
        emb_candidates=config.hybrid_emb_candidates,
        bm25_candidates=config.hybrid_bm25_candidates,
        rrf_k=config.hybrid_rrf_k,
    )
    
    metadata["round1_count"] = len(round1_top20)
    print(f"  [Round 1] Retrieved {len(round1_top20)} documents")
    
    if not round1_top20:
        print(f"  [Warning] No results from Round 1")
        metadata["total_latency_ms"] = (time.time() - start_time) * 1000
        return [], metadata
    
    # ========== Rerank Top 20 â†’ Top 5 ç”¨äºŽ Sufficiency Check ==========
    print(f"  [Rerank] Reranking Top 20 to get Top 5 for sufficiency check...")
    
    if config.use_reranker:
        reranked_top5 = await reranker_search(
            query=query,
            results=round1_top20,
            top_n=5,  # ðŸ”¥ åªå– Top 5 ç»™ LLM åˆ¤æ–­
            reranker_instruction=config.reranker_instruction,
            batch_size=config.reranker_batch_size,
            max_retries=config.reranker_max_retries,
            retry_delay=config.reranker_retry_delay,
            timeout=config.reranker_timeout,
            fallback_threshold=config.reranker_fallback_threshold,
            config=config,
        )
        metadata["round1_reranked_count"] = len(reranked_top5)
        print(f"  [Rerank] Got Top 5 for sufficiency check")
    else:
        # å¦‚æžœä¸ä½¿ç”¨ rerankerï¼Œç›´æŽ¥å–å‰ 5 ä¸ª
        reranked_top5 = round1_top20[:5]
        metadata["round1_reranked_count"] = 5
        print(f"  [No Rerank] Using original Top 5 for sufficiency check")
    
    if not reranked_top5:
        print(f"  [Warning] Reranking failed, falling back to original Top 20")
        metadata["total_latency_ms"] = (time.time() - start_time) * 1000
        return round1_top20, metadata
    
    # ========== LLM Sufficiency Check ==========
    print(f"  [LLM] Checking sufficiency on Top 5...")
    
    is_sufficient, reasoning, missing_info = await agentic_utils.check_sufficiency(
        query=query,
        results=reranked_top5,  # ðŸ”¥ ä½¿ç”¨ reranked Top 5
        llm_provider=llm_provider,  # ä½¿ç”¨ LLMProvider
        llm_config=llm_config,
        max_docs=5  # ðŸ”¥ æ˜Žç¡®åªæ£€æŸ¥ 5 ä¸ªæ–‡æ¡£
    )
    
    metadata["is_sufficient"] = is_sufficient
    metadata["reasoning"] = reasoning
    
    print(f"  [LLM] Result: {'âœ… Sufficient' if is_sufficient else 'âŒ Insufficient'}")
    print(f"  [LLM] Reasoning: {reasoning}")
    
    # ========== å¦‚æžœå……åˆ†ï¼šè¿”å›žåŽŸå§‹ Round 1 çš„ Top 20 ==========
    if is_sufficient:
        print(f"  [Decision] Sufficient! Using original Round 1 Top 20 results")
        
        final_results = round1_top20  # ðŸ”¥ è¿”å›žåŽŸå§‹çš„ Top 20ï¼ˆä¸æ˜¯ reranked çš„ï¼‰
        metadata["final_count"] = len(final_results)
        metadata["total_latency_ms"] = (time.time() - start_time) * 1000
        
        print(f"  [Complete] Latency: {metadata['total_latency_ms']:.0f}ms")
        return final_results, metadata
    
    # ========== å¦‚æžœä¸å……åˆ†ï¼šè¿›å…¥ Round 2 ==========
    metadata["is_multi_round"] = True
    metadata["missing_info"] = missing_info
    print(f"  [Decision] Insufficient, entering Round 2")
    print(f"  [Missing] {', '.join(missing_info) if missing_info else 'N/A'}")
    
    # ========== LLM ç”Ÿæˆå¤šä¸ªæ”¹è¿›æŸ¥è¯¢ï¼ˆå¤šæŸ¥è¯¢ç­–ç•¥ï¼‰==========
    use_multi_query = getattr(config, 'use_multi_query', True)  # ðŸ”¥ é»˜è®¤å¯ç”¨å¤šæŸ¥è¯¢
    
    if use_multi_query:
        print(f"  [LLM] Generating multiple refined queries...")
        
        # ðŸ”¥ ç”Ÿæˆ 2-3 ä¸ªäº’è¡¥æŸ¥è¯¢
        refined_queries, query_strategy = await agentic_utils.generate_multi_queries(
            original_query=query,
            results=reranked_top5,  # ðŸ”¥ åŸºäºŽ Top 5 ç”Ÿæˆæ”¹è¿›æŸ¥è¯¢
            missing_info=missing_info,
            llm_provider=llm_provider,  # ä½¿ç”¨ LLMProvider
            llm_config=llm_config,
            max_docs=5,
            num_queries=3  # æœŸæœ›ç”Ÿæˆ 3 ä¸ªæŸ¥è¯¢
        )
        
        metadata["refined_queries"] = refined_queries
        metadata["query_strategy"] = query_strategy
        metadata["num_queries"] = len(refined_queries)
        
        # ========== Round 2: å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæŸ¥è¯¢æ£€ç´¢ ==========
        print(f"  [Round 2] Executing {len(refined_queries)} queries in parallel...")
        
        # ðŸ”¥ å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æŸ¥è¯¢çš„æ··åˆæ£€ç´¢
        multi_query_tasks = [
            hybrid_search_with_rrf(
                query=q,
                emb_index=emb_index,
                bm25=bm25,
                docs=docs,
                top_n=50,  # ðŸ”¥ æ¯ä¸ªæŸ¥è¯¢å¬å›ž 50 ä¸ªå€™é€‰
                emb_candidates=config.hybrid_emb_candidates,
                bm25_candidates=config.hybrid_bm25_candidates,
                rrf_k=config.hybrid_rrf_k,
            )
            for q in refined_queries
        ]
        
        # ç­‰å¾…æ‰€æœ‰æŸ¥è¯¢å®Œæˆ
        multi_query_results = await asyncio.gather(*multi_query_tasks)
        
        # æ‰“å°æ¯ä¸ªæŸ¥è¯¢çš„å¬å›žæ•°
        for i, results in enumerate(multi_query_results, 1):
            print(f"    Query {i}: Retrieved {len(results)} documents")
        
        # ========== ä½¿ç”¨ RRF èžåˆå¤šä¸ªæŸ¥è¯¢çš„ç»“æžœ ==========
        print(f"  [Multi-RRF] Fusing results from {len(refined_queries)} queries...")
        
        # ðŸ”¥ ä½¿ç”¨å¤šæŸ¥è¯¢ RRF èžåˆ
        round2_results = multi_rrf_fusion(
            results_list=multi_query_results,
            k=config.hybrid_rrf_k  # ä½¿ç”¨ç›¸åŒçš„ k å‚æ•°
        )
        
        # å– Top 40 ç”¨äºŽåŽç»­åˆå¹¶
        round2_results = round2_results[:40]
        
        metadata["round2_count"] = len(round2_results)
        metadata["multi_query_total_docs"] = sum(len(r) for r in multi_query_results)
        
        print(f"  [Multi-RRF] Fused {metadata['multi_query_total_docs']} â†’ {len(round2_results)} unique documents")
    
    else:
        # ðŸ”¥ å›žé€€åˆ°å•æŸ¥è¯¢æ¨¡å¼ï¼ˆä¿æŒå‘åŽå…¼å®¹ï¼‰
        print(f"  [LLM] Generating single refined query (legacy mode)...")
        
        refined_query = await agentic_utils.generate_refined_query(
            original_query=query,
            results=reranked_top5,
            missing_info=missing_info,
            llm_provider=llm_provider,  # ä½¿ç”¨ LLMProvider
            llm_config=llm_config,
            max_docs=5
        )
        
        metadata["refined_query"] = refined_query
        print(f"  [LLM] Refined query: {refined_query}")
        
        # ========== Round 2: ä½¿ç”¨å•ä¸ªæ”¹è¿›æŸ¥è¯¢æ£€ç´¢ ==========
        print(f"  [Round 2] Hybrid search with refined query...")
        
        round2_results = await hybrid_search_with_rrf(
            query=refined_query,
            emb_index=emb_index,
            bm25=bm25,
            docs=docs,
            top_n=40,
            emb_candidates=config.hybrid_emb_candidates,
            bm25_candidates=config.hybrid_bm25_candidates,
            rrf_k=config.hybrid_rrf_k,
        )
        
        metadata["round2_count"] = len(round2_results)
        print(f"  [Round 2] Retrieved {len(round2_results)} documents")
    
    # ========== åˆå¹¶ï¼šç¡®ä¿æ€»å…± 40 ä¸ªæ–‡æ¡£ ==========
    print(f"  [Merge] Combining Round 1 and Round 2 to ensure 40 documents...")
    
    # åŽ»é‡ï¼šä½¿ç”¨æ–‡æ¡£ ID åŽ»é‡
    round1_ids = {id(doc) for doc, _ in round1_top20}
    round2_unique = [(doc, score) for doc, score in round2_results if id(doc) not in round1_ids]
    
    # åˆå¹¶ï¼šRound1 Top20 + Round2 åŽ»é‡åŽçš„æ–‡æ¡£ï¼ˆç¡®ä¿æ€»æ•°=40ï¼‰
    combined_results = round1_top20.copy()  # å…ˆåŠ å…¥ Round1 çš„ 20 ä¸ª
    needed_from_round2 = 40 - len(combined_results)  # éœ€è¦ 20 ä¸ª
    combined_results.extend(round2_unique[:needed_from_round2])
    
    actual_count = len(combined_results)
    duplicates_removed = len(round2_results) - len(round2_unique)
    round2_added = len(round2_unique[:needed_from_round2])
    
    print(f"  [Merge] Round1 Top20: 20 documents")
    print(f"  [Merge] Round2 duplicates removed: {duplicates_removed} documents")
    print(f"  [Merge] Round2 unique added: {round2_added} documents")
    print(f"  [Merge] Combined total: {actual_count} documents (target: 40)")
    
    # ========== Rerank åˆå¹¶åŽçš„ 40 ä¸ªæ–‡æ¡£ ==========
    if config.use_reranker and len(combined_results) > 0:
        print(f"  [Rerank] Reranking {len(combined_results)} documents...")
        
        final_results = await reranker_search(
            query=query,  # ðŸ”¥ ä½¿ç”¨åŽŸå§‹æŸ¥è¯¢è¿›è¡Œ rerank
            results=combined_results,
            top_n=20,  # ðŸ”¥ è¿”å›ž Top 20 ä½œä¸ºæœ€ç»ˆç»“æžœ
            reranker_instruction=config.reranker_instruction,
            batch_size=config.reranker_batch_size,
            max_retries=config.reranker_max_retries,
            retry_delay=config.reranker_retry_delay,
            timeout=config.reranker_timeout,
            fallback_threshold=config.reranker_fallback_threshold,
            config=config,
        )
        
        print(f"  [Rerank] Final Top 20 selected")
    else:
        # ä¸ä½¿ç”¨ Rerankerï¼Œç›´æŽ¥è¿”å›ž Top 20
        final_results = combined_results[:20]
        print(f"  [No Rerank] Returning Top 20 from combined results")
    
    metadata["final_count"] = len(final_results)
    metadata["total_latency_ms"] = (time.time() - start_time) * 1000
    
    print(f"  [Complete] Final: {len(final_results)} docs | Latency: {metadata['total_latency_ms']:.0f}ms")
    print(f"{'='*60}\n")
    
    return final_results, metadata


async def reranker_search(
    query: str,
    results: List[Tuple[dict, float]],
    top_n: int = 20,
    reranker_instruction: str = None,
    batch_size: int = 10,  # ðŸ”¥ æ‰¹æ¬¡å¤§å°ï¼ˆReranker API é€šå¸¸é™åˆ¶ï¼‰
    max_retries: int = 3,  # ðŸ”¥ æœ€å¤§é‡è¯•æ¬¡æ•°
    retry_delay: float = 2.0,  # ðŸ”¥ é‡è¯•åŸºç¡€å»¶è¿Ÿ
    timeout: float = 30.0,  # ðŸ”¥ å•æ‰¹æ¬¡è¶…æ—¶
    fallback_threshold: float = 0.3,  # ðŸ”¥ é™çº§é˜ˆå€¼
    config: ExperimentConfig = None,  # ðŸ”¥ æ–°å¢žï¼šå®žéªŒé…ç½®ï¼ˆç”¨äºŽèŽ·å–å¹¶å‘æ•°ï¼‰
):
    """
    ä½¿ç”¨ reranker æ¨¡åž‹å¯¹æ£€ç´¢ç»“æžœè¿›è¡Œé‡æŽ’åºï¼ˆæ”¯æŒæ‰¹é‡å¹¶å‘å¤„ç† + å¢žå¼ºç¨³å®šæ€§ï¼‰
    
    å¯¹äºŽåŒ…å« event_log çš„æ–‡æ¡£ï¼š
    - æ ¼å¼åŒ–ä¸ºå¤šè¡Œæ–‡æœ¬ï¼šæ—¶é—´ + æ¯å¥ atomic_fact å•ç‹¬ä¸€è¡Œ
    - ä¾‹å¦‚ï¼š
      2024-10-31 14:30:00
      ç”¨æˆ·å–œæ¬¢åƒå·èœ
      ç”¨æˆ·æœ€å–œæ¬¢çš„å·èœæ˜¯éº»å©†è±†è…
      ç”¨æˆ·ä¸å–œæ¬¢å¤ªè¾£çš„èœ
    
    å¯¹äºŽä¼ ç»Ÿæ–‡æ¡£ï¼š
    - å›žé€€åˆ°ä½¿ç”¨ episode å­—æ®µ
    
    ä¼˜åŒ–ç­–ç•¥ï¼ˆç¨³å®šæ€§ä¼˜å…ˆï¼‰ï¼š
    - å°†æ–‡æ¡£åˆ†æ‰¹å¤„ç†ï¼ˆæ¯æ‰¹ batch_size ä¸ªï¼‰
    - ä¸²è¡Œå¤„ç†æ‰¹æ¬¡ï¼ˆé¿å… API é™æµï¼‰
    - æ¯ä¸ªæ‰¹æ¬¡æ”¯æŒé‡è¯•å’ŒæŒ‡æ•°é€€é¿
    - æˆåŠŸçŽ‡è¿‡ä½Žæ—¶è‡ªåŠ¨é™çº§åˆ°åŽŸå§‹æŽ’åº
    - å•æ‰¹æ¬¡è¶…æ—¶ä¿æŠ¤
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        results: åˆæ­¥æ£€ç´¢ç»“æžœï¼ˆæ¥è‡ª embedding æˆ– BM25ï¼‰
        top_n: è¿”å›žçš„ç»“æžœæ•°é‡ï¼ˆé»˜è®¤ 20ï¼‰
        reranker_instruction: Reranker æŒ‡ä»¤
        batch_size: æ¯æ‰¹å¤„ç†çš„æ–‡æ¡£æ•°é‡ï¼ˆé»˜è®¤ 10ï¼‰
        max_retries: æ¯ä¸ªæ‰¹æ¬¡çš„æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤ 3ï¼‰
        retry_delay: é‡è¯•åŸºç¡€å»¶è¿Ÿç§’æ•°ï¼ˆé»˜è®¤ 2.0ï¼ŒæŒ‡æ•°é€€é¿ï¼‰
        timeout: å•æ‰¹æ¬¡è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤ 30ï¼‰
        fallback_threshold: æˆåŠŸçŽ‡ä½ŽäºŽæ­¤å€¼æ—¶é™çº§ï¼ˆé»˜è®¤ 0.3ï¼‰
    
    Returns:
        é‡æŽ’åºåŽçš„ Top-N ç»“æžœåˆ—è¡¨
    """
    if not results:
        return []

    # ç¬¬ä¸€æ­¥ï¼šæ ¼å¼åŒ–æ–‡æ¡£
    docs_with_episode = []
    doc_texts = []
    original_indices = []  # ðŸ”¥ è®°å½•åŽŸå§‹ç´¢å¼•ï¼Œç”¨äºŽè¿˜åŽŸ
    
    for idx, (doc, score) in enumerate(results):
        # ä¼˜å…ˆä½¿ç”¨ event_log æ ¼å¼åŒ–æ–‡æœ¬ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
        if doc.get("event_log") and doc["event_log"].get("atomic_fact"):
            event_log = doc["event_log"]
            time_str = event_log.get("time", "")
            atomic_facts = event_log.get("atomic_fact", [])

            if isinstance(atomic_facts, list) and atomic_facts:
                # ðŸ”¥ æ ¼å¼åŒ–ä¸ºå¤šè¡Œæ–‡æœ¬ï¼ˆæ—¶é—´ + æ¯å¥ atomic_fact å•ç‹¬ä¸€è¡Œï¼‰
                formatted_lines = []
                if time_str:
                    formatted_lines.append(time_str)
                
                # ðŸ”¥ ä¿®å¤ï¼šå…¼å®¹ä¸¤ç§æ ¼å¼ï¼ˆå­—ç¬¦ä¸² / å­—å…¸ï¼‰
                for fact in atomic_facts:
                    if isinstance(fact, dict) and "fact" in fact:
                        # æ–°æ ¼å¼ï¼š{"fact": "...", "embedding": [...]}
                        formatted_lines.append(fact["fact"])
                    elif isinstance(fact, str):
                        # æ—§æ ¼å¼ï¼šçº¯å­—ç¬¦ä¸²
                        formatted_lines.append(fact)
                
                formatted_text = "\n".join(formatted_lines)

                docs_with_episode.append(doc)
                doc_texts.append(formatted_text)
                original_indices.append(idx)
                continue

        # å›žé€€åˆ°åŽŸæœ‰çš„ episode å­—æ®µï¼ˆä¿æŒå‘åŽå…¼å®¹ï¼‰
        if episode_text := doc.get("episode"):
            docs_with_episode.append(doc)
            doc_texts.append(episode_text)
            original_indices.append(idx)

    if not doc_texts:
        return []

    reranker = rerank_service.get_rerank_service()
    print(f"Reranking query: {query}")
    print(f"Reranking {len(doc_texts)} documents in batches of {batch_size}...")
    print(f"Reranking instruction: {reranker_instruction}")
    
    # ðŸ”¥ ç¬¬äºŒæ­¥ï¼šæ‰¹é‡å¤„ç†ï¼ˆä¸²è¡Œ + é‡è¯• + é™çº§ï¼‰
    # å°†æ–‡æ¡£åˆ†æ‰¹ï¼Œæ¯æ‰¹ batch_size ä¸ª
    batches = []
    for i in range(0, len(doc_texts), batch_size):
        batch = doc_texts[i : i + batch_size]
        batches.append((i, batch))  # ä¿å­˜èµ·å§‹ç´¢å¼•å’Œæ‰¹æ¬¡æ•°æ®
    
    print(f"Split into {len(batches)} batches for serial reranking")
    
    # ðŸ”¥ å¤„ç†å•ä¸ªæ‰¹æ¬¡ï¼ˆå¸¦é‡è¯• + è¶…æ—¶ + æŒ‡æ•°é€€é¿ï¼‰
    async def process_batch_with_retry(start_idx: int, batch_texts: List[str]):
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡ï¼ˆå¸¦é‡è¯•å’Œè¶…æ—¶ï¼‰"""
        for attempt in range(max_retries):
            try:
                # ðŸ”¥ æ·»åŠ è¶…æ—¶ä¿æŠ¤
                batch_results = await asyncio.wait_for(
                    reranker._make_rerank_request(
                    query, batch_texts, instruction=reranker_instruction
                    ),
                    timeout=timeout
                )
                
                # è°ƒæ•´ç´¢å¼•ï¼šå°†æ‰¹æ¬¡å†…çš„ç´¢å¼•æ˜ å°„å›žå…¨å±€ç´¢å¼•
                for item in batch_results["results"]:
                    item["global_index"] = start_idx + item["index"]
                
                if attempt > 0:
                    print(f"  âœ“ Batch at {start_idx} succeeded on attempt {attempt + 1}")
                return batch_results["results"]
                
            except asyncio.TimeoutError:
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (2 ** attempt)  # æŒ‡æ•°é€€é¿ï¼š2s, 4s, 8s
                    print(f"  â±ï¸  Batch at {start_idx} timeout (attempt {attempt + 1}), retrying in {wait_time:.1f}s")
                    await asyncio.sleep(wait_time)
                else:
                    print(f"  âŒ Batch at {start_idx} timeout after {max_retries} attempts")
                    return []
                
            except Exception as e:
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (2 ** attempt)
                    print(f"  âš ï¸  Batch at {start_idx} failed (attempt {attempt + 1}), retrying in {wait_time:.1f}s: {e}")
                    await asyncio.sleep(wait_time)
                else:
                    print(f"  âŒ Batch at {start_idx} failed after {max_retries} attempts: {e}")
                    return []
    
    # ðŸ”¥ å¯æŽ§å¹¶å‘å¤„ç†ï¼ˆç¨³å¦¥çš„å¹¶å‘ç­–ç•¥ï¼‰
    # ä»Žé…ç½®èŽ·å–å¹¶å‘æ•°ï¼ˆé»˜è®¤ 2ï¼Œç¨³å¦¥å€¼ï¼‰
    max_concurrent = getattr(config, 'reranker_concurrent_batches', 2)
    
    batch_results_list = []
    successful_batches = 0
    
    # åˆ†ç»„å¤„ç†ï¼Œæ¯ç»„æœ€å¤š max_concurrent ä¸ªæ‰¹æ¬¡å¹¶å‘
    for group_start in range(0, len(batches), max_concurrent):
        group_batches = batches[group_start : group_start + max_concurrent]
        
        print(f"  Processing batch group {group_start//max_concurrent + 1} ({len(group_batches)} batches in parallel)...")
        
        # ðŸ”¥ å¹¶å‘å¤„ç†å½“å‰ç»„çš„æ‰€æœ‰æ‰¹æ¬¡
        tasks = [
            process_batch_with_retry(start_idx, batch) 
            for start_idx, batch in group_batches
        ]
        group_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡æˆåŠŸçš„æ‰¹æ¬¡
        for result in group_results:
            if isinstance(result, list) and result:
                batch_results_list.append(result)
                successful_batches += 1
            else:
                batch_results_list.append([])
        
        # ç»„é—´å»¶è¿Ÿï¼ˆè¿›ä¸€æ­¥é™ä½Žï¼Œæ¿€è¿›åŠ é€Ÿï¼‰
        if group_start + max_concurrent < len(batches):
            await asyncio.sleep(0.3)  # ðŸ”¥ ç»„é—´ 0.3 ç§’é—´éš”ï¼ˆä»Ž 0.8s é™ä½Žï¼‰
    
    # ðŸ”¥ ç¬¬ä¸‰æ­¥ï¼šåˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æžœ + é™çº§ç­–ç•¥
    all_rerank_results = []
    for batch_results in batch_results_list:
        all_rerank_results.extend(batch_results)
    
    # ðŸ”¥ è®¡ç®—æˆåŠŸçŽ‡
    success_rate = successful_batches / len(batches) if batches else 0.0
    print(f"Reranker success rate: {success_rate:.1%} ({successful_batches}/{len(batches)} batches)")
    
    # ðŸ”¥ é™çº§ç­–ç•¥ 1: å®Œå…¨å¤±è´¥
    if not all_rerank_results:
        print("âš ï¸ Warning: All reranker batches failed, using original ranking as fallback")
        return results[:top_n]
    
    # ðŸ”¥ é™çº§ç­–ç•¥ 2: æˆåŠŸçŽ‡è¿‡ä½Ž
    if success_rate < fallback_threshold:
        print(f"âš ï¸ Warning: Reranker success rate too low ({success_rate:.1%} < {fallback_threshold:.1%}), using original ranking")
        return results[:top_n]
    
    print(f"Reranking complete: {len(all_rerank_results)} documents scored")
    
    # ç¬¬å››æ­¥ï¼šæŒ‰ reranker åˆ†æ•°æŽ’åºå¹¶è¿”å›ž Top-N
    sorted_results = sorted(
        all_rerank_results, 
        key=lambda x: x["relevance_score"], 
        reverse=True
    )[:top_n]
    
    # æ˜ å°„å›žåŽŸå§‹æ–‡æ¡£
    final_results = [
        (results[original_indices[item["global_index"]]][0], item["relevance_score"])
        for item in sorted_results
    ]
    
    return final_results


async def main():
    """Main function to perform batch search and save results in nemori format."""
    # --- Configuration ---
    config = ExperimentConfig()
    bm25_index_dir = (
        Path(__file__).parent / "results" / config.experiment_name / "bm25_index"
    )
    emb_index_dir = (
        Path(__file__).parent / "results" / config.experiment_name / "vectors"
    )
    save_dir = Path(__file__).parent / "results" / config.experiment_name

    dataset_path = config.datase_path
    results_output_path = save_dir / "search_results.json"
    
    # ðŸ”¥ æ–­ç‚¹ç»­ä¼ ï¼šæ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
    checkpoint_path = save_dir / "search_results_checkpoint.json"

    # Ensure NLTK data is ready
    ensure_nltk_data()
    
    # ðŸ”¥ åˆå§‹åŒ– LLM Providerï¼ˆç”¨äºŽ Agentic æ£€ç´¢ï¼‰
    llm_provider = None
    llm_config = None
    if config.use_agentic_retrieval:
        if agentic_utils is None:
            print("Error: agentic_utils not found, cannot use agentic retrieval")
            print("Please check that tools/agentic_utils.py exists")
            return
        
        llm_config = config.llm_config.get(config.llm_service, config.llm_config["openai"])
        
        # ä½¿ç”¨ Memory Layer çš„ LLMProvider æ›¿ä»£ AsyncOpenAI
        llm_provider = LLMProvider(
            provider_type="openai",
            model=llm_config["model"],
            api_key=llm_config["api_key"],
            base_url=llm_config["base_url"],
            temperature=llm_config.get("temperature", 0.3),
            max_tokens=llm_config.get("max_tokens", 32768),
        )
        print(f"âœ… LLM Provider initialized for agentic retrieval")
        print(f"   Model: {llm_config['model']}")

    # Load the dataset
    print(f"Loading dataset from: {dataset_path}")
    with open(dataset_path, "r", encoding="utf-8") as f:
        dataset = json.load(f)

    # ðŸ”¥ æ–­ç‚¹ç»­ä¼ ï¼šåŠ è½½å·²æœ‰çš„æ£€æŸ¥ç‚¹
    all_search_results = {}
    processed_conversations = set()
    
    if checkpoint_path.exists():
        print(f"\nðŸ”„ Found checkpoint file: {checkpoint_path}")
        try:
            with open(checkpoint_path, "r", encoding="utf-8") as f:
                all_search_results = json.load(f)
            processed_conversations = set(all_search_results.keys())
            print(f"âœ… Loaded {len(processed_conversations)} conversations from checkpoint")
            print(f"   Already processed: {sorted(processed_conversations)}")
        except Exception as e:
            print(f"âš ï¸  Failed to load checkpoint: {e}")
            print(f"   Starting from scratch...")
            all_search_results = {}
            processed_conversations = set()
    else:
        print(f"\nðŸ†• No checkpoint found, starting from scratch")

    # Iterate through the dataset, assuming the index of the dataset list
    # corresponds to the conversation index number.
    for i, conversation_data in enumerate(dataset):
        conv_id = f"locomo_exp_user_{i}"
        
        # ðŸ”¥ æ–­ç‚¹ç»­ä¼ ï¼šè·³è¿‡å·²å¤„ç†çš„å¯¹è¯
        if conv_id in processed_conversations:
            print(f"\nâ­ï¸  Skipping Conversation ID: {conv_id} (already processed)")
            continue

        speaker_a = conversation_data["conversation"].get("speaker_a")
        speaker_b = conversation_data["conversation"].get("speaker_b")
        print(f"\n--- Processing Conversation ID: {conv_id} ({i+1}/{len(dataset)}) ---")

        if "qa" not in conversation_data:
            print(f"Warning: No 'qa' key found in conversation #{i}. Skipping.")
            continue

        # --- Load index once per conversation ---
        # ðŸ”¥ å¦‚æžœä½¿ç”¨æ··åˆæ£€ç´¢ï¼Œéœ€è¦åŒæ—¶åŠ è½½ Embedding å’Œ BM25 ç´¢å¼•
        if config.use_hybrid_search:
            # åŠ è½½ Embedding ç´¢å¼•
            emb_index_path = emb_index_dir / f"embedding_index_conv_{i}.pkl"
            if not emb_index_path.exists():
                print(
                    f"Error: Embedding index not found at {emb_index_path}. Skipping conversation."
                )
                continue
            with open(emb_index_path, "rb") as f:
                emb_index = pickle.load(f)
            
            # åŠ è½½ BM25 ç´¢å¼•
            bm25_index_path = bm25_index_dir / f"bm25_index_conv_{i}.pkl"
            if not bm25_index_path.exists():
                print(
                    f"Error: BM25 index not found at {bm25_index_path}. Skipping conversation."
                )
                continue
            with open(bm25_index_path, "rb") as f:
                index_data = pickle.load(f)
            bm25 = index_data["bm25"]
            docs = index_data["docs"]
            
            print(f"Loaded both Embedding and BM25 indexes for conversation {i} (Hybrid Search)")
        
        elif config.use_emb:
            # ä»…åŠ è½½ Embedding ç´¢å¼•
            emb_index_path = emb_index_dir / f"embedding_index_conv_{i}.pkl"
            if not emb_index_path.exists():
                print(
                    f"Error: Index file not found at {emb_index_path}. Skipping conversation."
                )
                continue
            with open(emb_index_path, "rb") as f:
                emb_index = pickle.load(f)
        else:
            # ä»…åŠ è½½ BM25 ç´¢å¼•
            bm25_index_path = bm25_index_dir / f"bm25_index_conv_{i}.pkl"
            if not bm25_index_path.exists():
                print(
                    f"Error: Index file not found at {bm25_index_path}. Skipping conversation."
                )
                continue
            with open(bm25_index_path, "rb") as f:
                index_data = pickle.load(f)
            bm25 = index_data["bm25"]
            docs = index_data["docs"]

        # Parallelize per-question retrieval with bounded concurrency
        # ðŸ”¥ å¢žåŠ å¹¶å‘æ•°ï¼šAgentic æ£€ç´¢æ—¶ä¹Ÿä½¿ç”¨æ›´é«˜å¹¶å‘ï¼ˆ10 â†’ 20ï¼‰
        max_concurrent = 20 if config.use_agentic_retrieval else 128
        sem = asyncio.Semaphore(max_concurrent)
        
        if config.use_agentic_retrieval:
            print(f"  ðŸš€ Agentic retrieval enabled with HIGH CONCURRENCY: {max_concurrent} concurrent requests")

        async def process_single_qa(qa_pair):
            """å¤„ç†å•ä¸ª QA å¯¹ï¼ˆæ”¯æŒå¤šç§æ£€ç´¢æ¨¡å¼ï¼‰"""
            question = qa_pair.get("question")
            if not question:
                return None
            if qa_pair.get("category") == 5:
                print(f"Skipping question {question} because it is category 5")
                return None
            
            # å¼€å§‹è®¡æ—¶
            qa_start_time = time.time()
            
            try:
                async with sem:
                    retrieval_metadata = {}
                    
                    # ========== æ£€ç´¢æ¨¡å¼é€‰æ‹© ==========
                    if config.retrieval_mode == "agentic":
                        # ðŸ”¥ Agentic å¤šè½®æ£€ç´¢ï¼ˆå¤æ‚ä½†è´¨é‡é«˜ï¼‰
                        top_results, retrieval_metadata = await agentic_retrieval(
                            query=question,
                            config=config,
                            llm_provider=llm_provider,  # ä½¿ç”¨ LLMProvider
                            llm_config=llm_config,
                            emb_index=emb_index,
                            bm25=bm25,
                            docs=docs,
                        )
                    
                    elif config.retrieval_mode == "lightweight":
                        # ðŸ”¥ è½»é‡çº§å¿«é€Ÿæ£€ç´¢ï¼ˆå¿«é€Ÿä½†è´¨é‡ç•¥ä½Žï¼‰
                        top_results, retrieval_metadata = await lightweight_retrieval(
                            query=question,
                            emb_index=emb_index,
                            bm25=bm25,
                            docs=docs,
                            config=config,
                        )
                    
                    else:
                        # ðŸ”¥ ä¼ ç»Ÿæ£€ç´¢åˆ†æ”¯ï¼ˆä¿æŒå‘åŽå…¼å®¹ï¼‰
                        if config.use_reranker:
                            # ç¬¬ä¸€é˜¶æ®µï¼šåˆæ­¥æ£€ç´¢ï¼Œå¬å›ž Top-N å€™é€‰
                            if config.use_hybrid_search:
                                # æ··åˆæ£€ç´¢ï¼šEmbedding (MaxSim) + BM25 + RRF èžåˆ
                                results = await hybrid_search_with_rrf(
                                    query=question,
                                    emb_index=emb_index,
                                    bm25=bm25,
                                    docs=docs,
                                    top_n=config.emb_recall_top_n,
                                    emb_candidates=config.hybrid_emb_candidates,
                                    bm25_candidates=config.hybrid_bm25_candidates,
                                    rrf_k=config.hybrid_rrf_k
                                )
                            elif config.use_emb:
                                # å•ç‹¬ä½¿ç”¨ Embedding + MaxSim æ£€ç´¢
                                results = await search_with_emb_index(
                                    query=question, 
                                    emb_index=emb_index, 
                                    top_n=config.emb_recall_top_n
                                )
                            else:
                                # å•ç‹¬ä½¿ç”¨ BM25 æ£€ç´¢
                                results = await asyncio.to_thread(
                                    search_with_bm25_index, 
                                    question, 
                                    bm25, 
                                    docs, 
                                    config.emb_recall_top_n
                                )
                            
                            # ç¬¬äºŒé˜¶æ®µï¼šReranker é‡æŽ’åº
                            top_results = await reranker_search(
                                query=question,
                                results=results,
                                top_n=config.reranker_top_n,
                                reranker_instruction=config.reranker_instruction,
                                batch_size=config.reranker_batch_size,
                                max_retries=config.reranker_max_retries,
                                retry_delay=config.reranker_retry_delay,
                                timeout=config.reranker_timeout,
                                fallback_threshold=config.reranker_fallback_threshold,
                                config=config,
                            )
                        else:
                            # å•é˜¶æ®µæ£€ç´¢ï¼ˆä¸ä½¿ç”¨ Rerankerï¼‰
                            if config.use_hybrid_search:
                                top_results = await hybrid_search_with_rrf(
                                    query=question,
                                    emb_index=emb_index,
                                    bm25=bm25,
                                    docs=docs,
                                    top_n=20,
                                    emb_candidates=config.hybrid_emb_candidates,
                                    bm25_candidates=config.hybrid_bm25_candidates,
                                    rrf_k=config.hybrid_rrf_k
                                )
                            elif config.use_emb:
                                top_results = await search_with_emb_index(
                                    query=question, emb_index=emb_index, top_n=20
                                )
                            else:
                                top_results = await asyncio.to_thread(
                                    search_with_bm25_index, question, bm25, docs, 20
                                )
                        
                        # æ·»åŠ æ£€ç´¢æ—¶é—´ç»Ÿè®¡
                        retrieval_metadata = {
                            "retrieval_mode": "traditional",
                            "use_reranker": config.use_reranker,
                            "use_hybrid_search": config.use_hybrid_search,
                        }

                    # ========== æ ¼å¼åŒ–æœ€ç»ˆ context ==========
                    context_str = ""
                    if top_results:
                        retrieved_docs_text = []
                        for doc, score in top_results:
                            subject = doc.get('subject', 'N/A')
                            episode = doc.get('episode', 'N/A')
                            doc_text = f"{subject}: {episode}\n---"
                            retrieved_docs_text.append(doc_text)
                        context_str = "\n\n".join(retrieved_docs_text)

                    # è®¡ç®—å¤„ç†æ—¶é—´
                    qa_latency_ms = (time.time() - qa_start_time) * 1000
                    
                    result = {
                        "query": question,
                        "context": TEMPLATE.format(
                            speaker_1=speaker_a,
                            speaker_2=speaker_b,
                            speaker_memories=context_str,
                        ),
                        "original_qa": qa_pair,
                        "retrieval_metadata": {
                            **retrieval_metadata,
                            "qa_latency_ms": qa_latency_ms,
                        }
                    }
                    
                    return result
                    
            except Exception as e:
                print(f"Error processing question '{question}': {e}")
                import traceback
                traceback.print_exc()
                return None

        tasks = [
            asyncio.create_task(process_single_qa(qa_pair))
            for qa_pair in conversation_data["qa"]
        ]
        results_for_conv = [
            res for res in await asyncio.gather(*tasks) if res is not None
        ]

        all_search_results[conv_id] = results_for_conv
        
        # ðŸ”¥ æ–­ç‚¹ç»­ä¼ ï¼šæ¯å¤„ç†å®Œä¸€ä¸ªå¯¹è¯å°±ä¿å­˜æ£€æŸ¥ç‚¹
        try:
            print(f"ðŸ’¾ Saving checkpoint after conversation {conv_id}...")
            with open(checkpoint_path, "w", encoding="utf-8") as f:
                json.dump(all_search_results, f, indent=2, ensure_ascii=False)
            print(f"âœ… Checkpoint saved: {len(all_search_results)} conversations")
        except Exception as e:
            print(f"âš ï¸  Failed to save checkpoint: {e}")

    # Save all results to a single JSON file in the specified format
    print(f"\n{'='*60}")
    print(f"ðŸŽ‰ All conversations processed!")
    print(f"{'='*60}")
    print(f"\nSaving final results to: {results_output_path}")
    with open(results_output_path, "w", encoding="utf-8") as f:
        json.dump(all_search_results, f, indent=2, ensure_ascii=False)

    print(f"âœ… Batch search and retrieval complete!")
    print(f"   Total conversations: {len(all_search_results)}")
    
    # ðŸ”¥ æ–­ç‚¹ç»­ä¼ ï¼šå®ŒæˆåŽåˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶
    if checkpoint_path.exists():
        try:
            checkpoint_path.unlink()
            print(f"ðŸ—‘ï¸  Checkpoint file removed (task completed)")
        except Exception as e:
            print(f"âš ï¸  Failed to remove checkpoint: {e}")

    # Clean up resources
    reranker = rerank_service.get_rerank_service()
    # Assuming the service is DeepInfraRerankService, which has a close method.
    if hasattr(reranker, 'close') and callable(getattr(reranker, 'close')):
        await reranker.close()


if __name__ == "__main__":
    asyncio.run(main())
