#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯• KeywordVocabularyMilvusRepository çš„åŠŸèƒ½

æµ‹è¯•å†…å®¹åŒ…æ‹¬:
1. åŸºç¡€CRUDæ“ä½œï¼ˆå¢åˆ æ”¹æŸ¥ï¼‰
2. å‘é‡ç›¸ä¼¼åº¦æœç´¢
3. æŒ‰ç±»å‹è¿‡æ»¤æœç´¢
4. æ‰¹é‡æ“ä½œ
5. è¾¹ç•Œæƒ…å†µæµ‹è¯•
"""

import asyncio
from typing import List
import numpy as np
from core.di import get_bean_by_type
from infra_layer.adapters.out.search.repository.keyword_vocabulary_milvus_repository import (
    KeywordVocabularyMilvusRepository,
)
from core.observation.logger import get_logger

logger = get_logger(__name__)


def generate_random_vector(dim: int = 1024) -> List[float]:
    """ç”Ÿæˆéšæœºå‘é‡ç”¨äºæµ‹è¯•"""
    return np.random.randn(dim).astype(np.float32).tolist()


def generate_similar_vector(
    base_vector: List[float], noise_level: float = 0.1
) -> List[float]:
    """ç”Ÿæˆä¸åŸºå‡†å‘é‡ç›¸ä¼¼çš„å‘é‡"""
    noise = np.random.normal(0, noise_level, len(base_vector))
    return [float(x + n) for x, n in zip(base_vector, noise)]


async def test_basic_crud_operations():
    """æµ‹è¯•åŸºç¡€CRUDæ“ä½œ"""
    logger.info("========== å¼€å§‹æµ‹è¯•åŸºç¡€CRUDæ“ä½œ ==========")

    repo = get_bean_by_type(KeywordVocabularyMilvusRepository)
    test_keyword = "æœºå™¨å­¦ä¹ "
    test_type = "technology"
    test_vector = generate_random_vector()
    test_model = "bge-m3"

    try:
        # æµ‹è¯•1: åˆ›å»ºå…³é”®è¯ï¼ˆCreateï¼‰
        logger.info("æµ‹è¯•1: åˆ›å»ºå…³é”®è¯")
        doc = await repo.create_and_save_keyword(
            keyword=test_keyword,
            keyword_type=test_type,
            vector=test_vector,
            vector_model=test_model,
        )

        assert doc is not None
        assert doc["keyword"] == test_keyword
        assert doc["type"] == test_type
        assert doc["vector_model"] == test_model
        logger.info("âœ… åˆ›å»ºå…³é”®è¯æˆåŠŸ: %s", test_keyword)

        # ç­‰å¾…æ•°æ®åˆ·æ–°
        await repo.flush()
        await asyncio.sleep(1)

        # æµ‹è¯•2: æ ¹æ®æ–‡æœ¬ç²¾ç¡®æŸ¥è¯¢ï¼ˆReadï¼‰
        logger.info("æµ‹è¯•2: æ ¹æ®æ–‡æœ¬ç²¾ç¡®æŸ¥è¯¢")
        retrieved_doc = await repo.get_keyword_by_text(
            keyword=test_keyword, keyword_type=test_type
        )

        assert retrieved_doc is not None
        assert retrieved_doc["keyword"] == test_keyword
        assert retrieved_doc["type"] == test_type
        assert retrieved_doc["vector_model"] == test_model
        logger.info("âœ… ç²¾ç¡®æŸ¥è¯¢æˆåŠŸ: %s", test_keyword)

        # æµ‹è¯•3: æ ¹æ®IDåˆ é™¤ï¼ˆDeleteï¼‰
        logger.info("æµ‹è¯•3: æ ¹æ®IDåˆ é™¤")
        keyword_id = retrieved_doc["id"]
        delete_result = await repo.delete_by_keyword_id(keyword_id)
        assert delete_result is True
        logger.info("âœ… åˆ é™¤å…³é”®è¯æˆåŠŸ: id=%s", keyword_id)

        # éªŒè¯åˆ é™¤
        await repo.flush()
        await asyncio.sleep(1)
        deleted_check = await repo.get_keyword_by_text(test_keyword, test_type)
        assert deleted_check is None, "å…³é”®è¯åº”è¯¥å·²è¢«åˆ é™¤"
        logger.info("âœ… éªŒè¯åˆ é™¤æˆåŠŸ")

    except Exception as e:
        logger.error("âŒ æµ‹è¯•åŸºç¡€CRUDæ“ä½œå¤±è´¥: %s", e)
        # æ¸…ç†å¯èƒ½æ®‹ç•™çš„æ•°æ®
        try:
            await repo.delete_by_keyword_text(test_keyword, test_type)
            await repo.flush()
        except Exception:
            pass
        raise

    logger.info("âœ… åŸºç¡€CRUDæ“ä½œæµ‹è¯•å®Œæˆ\n")


async def test_vector_similarity_search():
    """æµ‹è¯•å‘é‡ç›¸ä¼¼åº¦æœç´¢"""
    logger.info("========== å¼€å§‹æµ‹è¯•å‘é‡ç›¸ä¼¼åº¦æœç´¢ ==========")

    repo = get_bean_by_type(KeywordVocabularyMilvusRepository)
    test_type = "ai_concept"
    base_vector = generate_random_vector()
    test_model = "bge-m3"

    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_keywords = [
        {"keyword": "æ·±åº¦å­¦ä¹ ", "similarity": "high"},
        {"keyword": "ç¥ç»ç½‘ç»œ", "similarity": "high"},
        {"keyword": "å·ç§¯ç¥ç»ç½‘ç»œ", "similarity": "medium"},
        {"keyword": "å¼ºåŒ–å­¦ä¹ ", "similarity": "medium"},
        {"keyword": "è‡ªç„¶è¯­è¨€å¤„ç†", "similarity": "low"},
    ]

    try:
        # åˆ›å»ºæµ‹è¯•å…³é”®è¯
        logger.info("åˆ›å»ºæµ‹è¯•å…³é”®è¯...")
        for kw_data in test_keywords:
            # æ ¹æ®ç›¸ä¼¼åº¦çº§åˆ«ç”Ÿæˆå‘é‡
            if kw_data["similarity"] == "high":
                vector = generate_similar_vector(base_vector, noise_level=0.05)
            elif kw_data["similarity"] == "medium":
                vector = generate_similar_vector(base_vector, noise_level=0.15)
            else:
                vector = generate_random_vector()  # å®Œå…¨ä¸ç›¸ä¼¼

            await repo.create_and_save_keyword(
                keyword=kw_data["keyword"],
                keyword_type=test_type,
                vector=vector,
                vector_model=test_model,
            )

        await repo.flush()
        await repo.load()
        await asyncio.sleep(2)
        logger.info("âœ… åˆ›å»ºäº† %d ä¸ªæµ‹è¯•å…³é”®è¯", len(test_keywords))

        # æµ‹è¯•1: åŸºç¡€å‘é‡æœç´¢
        logger.info("\næµ‹è¯•1: åŸºç¡€å‘é‡æœç´¢ï¼ˆTop 3ï¼‰")
        results = await repo.search_similar_keywords(
            query_vector=base_vector, keyword_type=test_type, limit=3
        )

        assert len(results) >= 2, f"åº”è¯¥æ‰¾åˆ°è‡³å°‘2ä¸ªç›¸ä¼¼å…³é”®è¯ï¼Œå®é™…æ‰¾åˆ°{len(results)}ä¸ª"
        logger.info("æ‰¾åˆ° %d ä¸ªç›¸ä¼¼å…³é”®è¯:", len(results))
        for i, result in enumerate(results, 1):
            logger.info("  %d. %s (score: %.4f)", i, result["keyword"], result["score"])

        # éªŒè¯æœ€ç›¸ä¼¼çš„åº”è¯¥æ˜¯ high similarity çš„å…³é”®è¯
        top_keywords = [r["keyword"] for r in results[:2]]
        high_similarity_keywords = [
            kw["keyword"] for kw in test_keywords if kw["similarity"] == "high"
        ]
        assert any(
            kw in top_keywords for kw in high_similarity_keywords
        ), "Topç»“æœåº”è¯¥åŒ…å«é«˜ç›¸ä¼¼åº¦çš„å…³é”®è¯"
        logger.info("âœ… åŸºç¡€å‘é‡æœç´¢æµ‹è¯•é€šè¿‡")

        # æµ‹è¯•2: è¿”å›æ‰€æœ‰ç»“æœ
        logger.info("\næµ‹è¯•2: è¿”å›æ‰€æœ‰ç»“æœ")
        all_results = await repo.search_similar_keywords(
            query_vector=base_vector, keyword_type=test_type, limit=100
        )

        assert len(all_results) == len(
            test_keywords
        ), f"åº”è¯¥æ‰¾åˆ°å…¨éƒ¨ {len(test_keywords)} ä¸ªå…³é”®è¯ï¼Œå®é™…æ‰¾åˆ° {len(all_results)} ä¸ª"
        logger.info("âœ… æ‰¾åˆ°å…¨éƒ¨ %d ä¸ªå…³é”®è¯", len(all_results))

    except Exception as e:
        logger.error("âŒ æµ‹è¯•å‘é‡ç›¸ä¼¼åº¦æœç´¢å¤±è´¥: %s", e)
        raise
    finally:
        # æ¸…ç†æµ‹è¯•æ•°æ®
        logger.info("\næ¸…ç†æµ‹è¯•æ•°æ®...")
        try:
            delete_count = await repo.delete_by_type(test_type)
            await repo.flush()
            logger.info("âœ… æ¸…ç†äº† %d æ¡æµ‹è¯•æ•°æ®", delete_count)
        except Exception as cleanup_error:
            logger.error("æ¸…ç†æµ‹è¯•æ•°æ®æ—¶å‡ºç°é”™è¯¯: %s", cleanup_error)

    logger.info("âœ… å‘é‡ç›¸ä¼¼åº¦æœç´¢æµ‹è¯•å®Œæˆ\n")


async def test_type_filtering():
    """æµ‹è¯•æŒ‰ç±»å‹è¿‡æ»¤"""
    logger.info("========== å¼€å§‹æµ‹è¯•æŒ‰ç±»å‹è¿‡æ»¤ ==========")

    repo = get_bean_by_type(KeywordVocabularyMilvusRepository)
    base_vector = generate_random_vector()
    test_model = "bge-m3"

    # å‡†å¤‡ä¸åŒç±»å‹çš„æµ‹è¯•æ•°æ®
    test_data = [
        {"keyword": "Python", "type": "programming_language"},
        {"keyword": "JavaScript", "type": "programming_language"},
        {"keyword": "React", "type": "framework"},
        {"keyword": "Django", "type": "framework"},
        {"keyword": "MySQL", "type": "database"},
        {"keyword": "PostgreSQL", "type": "database"},
    ]

    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        logger.info("åˆ›å»ºä¸åŒç±»å‹çš„æµ‹è¯•å…³é”®è¯...")
        for data in test_data:
            vector = generate_similar_vector(base_vector, noise_level=0.1)
            await repo.create_and_save_keyword(
                keyword=data["keyword"],
                keyword_type=data["type"],
                vector=vector,
                vector_model=test_model,
            )

        await repo.flush()
        await repo.load()
        await asyncio.sleep(2)
        logger.info("âœ… åˆ›å»ºäº† %d ä¸ªä¸åŒç±»å‹çš„å…³é”®è¯", len(test_data))

        # æµ‹è¯•1: æœç´¢ç‰¹å®šç±»å‹
        logger.info("\næµ‹è¯•1: æœç´¢ programming_language ç±»å‹")
        pl_results = await repo.search_similar_keywords(
            query_vector=base_vector, keyword_type="programming_language", limit=10
        )

        assert len(pl_results) == 2, f"åº”è¯¥æ‰¾åˆ°2ä¸ªç¼–ç¨‹è¯­è¨€ï¼Œå®é™…æ‰¾åˆ°{len(pl_results)}ä¸ª"
        for result in pl_results:
            assert result["type"] == "programming_language"
            logger.info("  - %s (type: %s)", result["keyword"], result["type"])
        logger.info("âœ… ç¼–ç¨‹è¯­è¨€ç±»å‹è¿‡æ»¤æˆåŠŸ")

        # æµ‹è¯•2: æœç´¢å¦ä¸€ä¸ªç±»å‹
        logger.info("\næµ‹è¯•2: æœç´¢ framework ç±»å‹")
        fw_results = await repo.search_similar_keywords(
            query_vector=base_vector, keyword_type="framework", limit=10
        )

        assert len(fw_results) == 2, f"åº”è¯¥æ‰¾åˆ°2ä¸ªæ¡†æ¶ï¼Œå®é™…æ‰¾åˆ°{len(fw_results)}ä¸ª"
        for result in fw_results:
            assert result["type"] == "framework"
            logger.info("  - %s (type: %s)", result["keyword"], result["type"])
        logger.info("âœ… æ¡†æ¶ç±»å‹è¿‡æ»¤æˆåŠŸ")

        # æµ‹è¯•3: ä¸æŒ‡å®šç±»å‹ï¼ˆæœç´¢å…¨éƒ¨ï¼‰
        logger.info("\næµ‹è¯•3: ä¸æŒ‡å®šç±»å‹ï¼ˆæœç´¢å…¨éƒ¨ï¼‰")
        all_results = await repo.search_similar_keywords(
            query_vector=base_vector, keyword_type=None, limit=10
        )

        assert (
            len(all_results) >= 6
        ), f"åº”è¯¥æ‰¾åˆ°è‡³å°‘6ä¸ªå…³é”®è¯ï¼Œå®é™…æ‰¾åˆ°{len(all_results)}ä¸ª"
        logger.info("æ‰¾åˆ° %d ä¸ªå…³é”®è¯ï¼ˆæ‰€æœ‰ç±»å‹ï¼‰", len(all_results))

        # ç»Ÿè®¡å„ç±»å‹æ•°é‡
        type_counts = {}
        for result in all_results:
            result_type = result["type"]
            type_counts[result_type] = type_counts.get(result_type, 0) + 1

        logger.info("ç±»å‹åˆ†å¸ƒ:")
        for kw_type, count in type_counts.items():
            logger.info("  - %s: %d", kw_type, count)
        logger.info("âœ… å…¨ç±»å‹æœç´¢æˆåŠŸ")

        # æµ‹è¯•4: åˆ—å‡ºæŒ‡å®šç±»å‹çš„æ‰€æœ‰å…³é”®è¯
        logger.info("\næµ‹è¯•4: åˆ—å‡º database ç±»å‹çš„æ‰€æœ‰å…³é”®è¯")
        db_keywords = await repo.list_keywords_by_type("database")

        assert (
            len(db_keywords) == 2
        ), f"åº”è¯¥æœ‰2ä¸ªæ•°æ®åº“å…³é”®è¯ï¼Œå®é™…æœ‰{len(db_keywords)}ä¸ª"
        for kw in db_keywords:
            logger.info("  - %s", kw["keyword"])
        logger.info("âœ… åˆ—å‡ºç±»å‹å…³é”®è¯æˆåŠŸ")

    except Exception as e:
        logger.error("âŒ æµ‹è¯•æŒ‰ç±»å‹è¿‡æ»¤å¤±è´¥: %s", e)
        raise
    finally:
        # æ¸…ç†æµ‹è¯•æ•°æ®
        logger.info("\næ¸…ç†æµ‹è¯•æ•°æ®...")
        try:
            for kw_type in ["programming_language", "framework", "database"]:
                count = await repo.delete_by_type(kw_type)
                logger.info("  æ¸…ç† %s: %d æ¡", kw_type, count)
            await repo.flush()
            logger.info("âœ… æµ‹è¯•æ•°æ®æ¸…ç†å®Œæˆ")
        except Exception as cleanup_error:
            logger.error("æ¸…ç†æµ‹è¯•æ•°æ®æ—¶å‡ºç°é”™è¯¯: %s", cleanup_error)

    logger.info("âœ… æŒ‰ç±»å‹è¿‡æ»¤æµ‹è¯•å®Œæˆ\n")


async def test_batch_operations():
    """æµ‹è¯•æ‰¹é‡æ“ä½œ"""
    logger.info("========== å¼€å§‹æµ‹è¯•æ‰¹é‡æ“ä½œ ==========")

    repo = get_bean_by_type(KeywordVocabularyMilvusRepository)
    test_type = "batch_test"
    test_model = "bge-m3"

    # å‡†å¤‡æ‰¹é‡æµ‹è¯•æ•°æ®
    batch_size = 50
    keywords_data = []

    for i in range(batch_size):
        keywords_data.append(
            {
                "keyword": f"å…³é”®è¯_{i}",
                "type": test_type,
                "vector": generate_random_vector(),
                "vector_model": test_model,
            }
        )

    try:
        # æµ‹è¯•æ‰¹é‡åˆ›å»º
        logger.info("æµ‹è¯•æ‰¹é‡åˆ›å»º %d ä¸ªå…³é”®è¯...", batch_size)
        count = await repo.batch_create_keywords(keywords_data)

        assert (
            count == batch_size
        ), f"åº”è¯¥åˆ›å»º {batch_size} ä¸ªå…³é”®è¯ï¼Œå®é™…åˆ›å»º {count} ä¸ª"
        logger.info("âœ… æ‰¹é‡åˆ›å»ºæˆåŠŸ: %d ä¸ªå…³é”®è¯", count)

        # åˆ·æ–°å¹¶éªŒè¯
        await repo.flush()
        await asyncio.sleep(1)

        # éªŒè¯æ•°æ®å·²ä¿å­˜
        logger.info("éªŒè¯æ‰¹é‡æ•°æ®...")
        all_keywords = await repo.list_keywords_by_type(test_type)
        assert (
            len(all_keywords) >= batch_size
        ), f"åº”è¯¥è‡³å°‘æœ‰ {batch_size} ä¸ªå…³é”®è¯ï¼Œå®é™…æœ‰ {len(all_keywords)} ä¸ª"
        logger.info("âœ… æ•°æ®éªŒè¯æˆåŠŸ: æ‰¾åˆ° %d ä¸ªå…³é”®è¯", len(all_keywords))

        # æµ‹è¯•æ‰¹é‡åˆ é™¤
        logger.info("æµ‹è¯•æ‰¹é‡åˆ é™¤...")
        delete_count = await repo.delete_by_type(test_type)
        assert (
            delete_count >= batch_size
        ), f"åº”è¯¥åˆ é™¤è‡³å°‘ {batch_size} ä¸ªå…³é”®è¯ï¼Œå®é™…åˆ é™¤ {delete_count} ä¸ª"
        logger.info("âœ… æ‰¹é‡åˆ é™¤æˆåŠŸ: %d ä¸ªå…³é”®è¯", delete_count)

        # éªŒè¯åˆ é™¤
        await repo.flush()
        await asyncio.sleep(1)
        remaining = await repo.list_keywords_by_type(test_type)
        assert (
            len(remaining) == 0
        ), f"åˆ é™¤ååº”è¯¥æ²¡æœ‰å‰©ä½™æ•°æ®ï¼Œå®é™…å‰©ä½™ {len(remaining)} ä¸ª"
        logger.info("âœ… åˆ é™¤éªŒè¯æˆåŠŸ")

    except Exception as e:
        logger.error("âŒ æµ‹è¯•æ‰¹é‡æ“ä½œå¤±è´¥: %s", e)
        raise
    finally:
        # ç¡®ä¿æ¸…ç†
        try:
            await repo.delete_by_type(test_type)
            await repo.flush()
        except Exception:
            pass

    logger.info("âœ… æ‰¹é‡æ“ä½œæµ‹è¯•å®Œæˆ\n")


async def test_edge_cases():
    """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
    logger.info("========== å¼€å§‹æµ‹è¯•è¾¹ç•Œæƒ…å†µ ==========")

    repo = get_bean_by_type(KeywordVocabularyMilvusRepository)
    test_type = "edge_test"
    test_model = "bge-m3"

    try:
        # æµ‹è¯•1: ç©ºå…³é”®è¯
        logger.info("æµ‹è¯•1: ç©ºå…³é”®è¯å¤„ç†")
        try:
            await repo.create_and_save_keyword(
                keyword="",
                keyword_type=test_type,
                vector=generate_random_vector(),
                vector_model=test_model,
            )
            logger.info("âš ï¸  ç©ºå…³é”®è¯è¢«å…è®¸åˆ›å»º")
        except Exception as e:
            logger.info("âœ… æ­£ç¡®æ‹’ç»ç©ºå…³é”®è¯: %s", e)

        # æµ‹è¯•2: æŸ¥è¯¢ä¸å­˜åœ¨çš„å…³é”®è¯
        logger.info("\næµ‹è¯•2: æŸ¥è¯¢ä¸å­˜åœ¨çš„å…³é”®è¯")
        nonexistent = await repo.get_keyword_by_text(
            keyword="è¿™ä¸ªå…³é”®è¯ç»å¯¹ä¸å­˜åœ¨_12345", keyword_type=test_type
        )
        assert nonexistent is None, "ä¸å­˜åœ¨çš„å…³é”®è¯åº”è¯¥è¿”å› None"
        logger.info("âœ… æ­£ç¡®å¤„ç†ä¸å­˜åœ¨çš„å…³é”®è¯")

        # æµ‹è¯•3: åˆ é™¤ä¸å­˜åœ¨çš„å…³é”®è¯
        logger.info("\næµ‹è¯•3: åˆ é™¤ä¸å­˜åœ¨çš„å…³é”®è¯")
        delete_result = await repo.delete_by_keyword_id("nonexistent_id_99999")
        assert delete_result is True  # Milvus delete ä¸å­˜åœ¨çš„IDä¹Ÿè¿”å›True
        logger.info("âœ… åˆ é™¤ä¸å­˜åœ¨çš„å…³é”®è¯ä¸æŠ¥é”™")

        # æµ‹è¯•4: é”™è¯¯çš„å‘é‡ç»´åº¦
        logger.info("\næµ‹è¯•4: é”™è¯¯çš„å‘é‡ç»´åº¦")
        try:
            await repo.create_and_save_keyword(
                keyword="é”™è¯¯ç»´åº¦æµ‹è¯•",
                keyword_type=test_type,
                vector=[1.0] * 512,  # é”™è¯¯çš„ç»´åº¦
                vector_model=test_model,
            )
            assert False, "åº”è¯¥å› ä¸ºå‘é‡ç»´åº¦é”™è¯¯è€Œå¤±è´¥"
        except Exception as e:
            assert "512" in str(e) and "1024" in str(e), "é”™è¯¯ä¿¡æ¯åº”è¯¥åŒ…å«ç»´åº¦ä¿¡æ¯"
            logger.info("âœ… æ­£ç¡®æ•è·å‘é‡ç»´åº¦é”™è¯¯: %s", str(e)[:100])

        # æµ‹è¯•5: ç©ºç±»å‹æœç´¢
        logger.info("\næµ‹è¯•5: ç©ºç±»å‹ï¼ˆä¸å­˜åœ¨çš„ç±»å‹ï¼‰æœç´¢")
        empty_results = await repo.search_similar_keywords(
            query_vector=generate_random_vector(),
            keyword_type="è¿™ä¸ªç±»å‹ä¸å­˜åœ¨_99999",
            limit=10,
        )
        assert len(empty_results) == 0, "ä¸å­˜åœ¨çš„ç±»å‹åº”è¯¥è¿”å›ç©ºç»“æœ"
        logger.info("âœ… ç©ºç±»å‹æœç´¢è¿”å›ç©ºç»“æœ")

        # æµ‹è¯•6: é‡å¤å…³é”®è¯ï¼ˆç›¸åŒkeywordå’Œtypeï¼‰
        logger.info("\næµ‹è¯•6: é‡å¤å…³é”®è¯")
        duplicate_keyword = "é‡å¤æµ‹è¯•å…³é”®è¯"

        # åˆ›å»ºç¬¬ä¸€ä¸ª
        await repo.create_and_save_keyword(
            keyword=duplicate_keyword,
            keyword_type=test_type,
            vector=generate_random_vector(),
            vector_model=test_model,
        )

        # å°è¯•åˆ›å»ºé‡å¤çš„
        try:
            await repo.create_and_save_keyword(
                keyword=duplicate_keyword,
                keyword_type=test_type,
                vector=generate_random_vector(),
                vector_model=test_model,
            )
            logger.info("âš ï¸  é‡å¤å…³é”®è¯è¢«å…è®¸åˆ›å»ºï¼ˆç›¸åŒIDä¼šè¦†ç›–ï¼‰")
        except Exception as e:
            logger.info("âœ… æ­£ç¡®æ‹’ç»é‡å¤å…³é”®è¯: %s", e)

        await repo.flush()

    except Exception as e:
        logger.error("âŒ æµ‹è¯•è¾¹ç•Œæƒ…å†µå¤±è´¥: %s", e)
        raise
    finally:
        # æ¸…ç†æµ‹è¯•æ•°æ®
        try:
            await repo.delete_by_type(test_type)
            await repo.flush()
            logger.info("âœ… è¾¹ç•Œæµ‹è¯•æ•°æ®æ¸…ç†å®Œæˆ")
        except Exception:
            pass

    logger.info("âœ… è¾¹ç•Œæƒ…å†µæµ‹è¯•å®Œæˆ\n")


async def test_real_world_scenario():
    """æµ‹è¯•çœŸå®ä¸–ç•Œåœºæ™¯ï¼šæŠ€èƒ½è¯æ±‡è¡¨"""
    logger.info("========== å¼€å§‹æµ‹è¯•çœŸå®åœºæ™¯ï¼šæŠ€èƒ½è¯æ±‡è¡¨ ==========")

    repo = get_bean_by_type(KeywordVocabularyMilvusRepository)
    test_model = "bge-m3"

    # æ¨¡æ‹ŸçœŸå®çš„æŠ€èƒ½è¯æ±‡è¡¨
    skills_data = {
        "programming": [
            "Pythonç¼–ç¨‹",
            "Javaå¼€å‘",
            "C++ç¼–ç¨‹",
            "JavaScriptå¼€å‘",
            "Goè¯­è¨€",
            "Rustç¼–ç¨‹",
            "TypeScriptå¼€å‘",
        ],
        "framework": [
            "Reactæ¡†æ¶",
            "Vue.js",
            "Django",
            "Spring Boot",
            "FastAPI",
            "Flask",
            "Express.js",
        ],
        "database": [
            "MySQLæ•°æ®åº“",
            "PostgreSQL",
            "MongoDB",
            "Redisç¼“å­˜",
            "Elasticsearch",
            "Cassandra",
        ],
        "devops": [
            "Dockerå®¹å™¨",
            "Kubernetes",
            "Jenkins",
            "GitLab CI",
            "Terraform",
            "Ansible",
        ],
        "ai_ml": [
            "æœºå™¨å­¦ä¹ ",
            "æ·±åº¦å­¦ä¹ ",
            "è‡ªç„¶è¯­è¨€å¤„ç†",
            "è®¡ç®—æœºè§†è§‰",
            "PyTorch",
            "TensorFlow",
            "scikit-learn",
        ],
    }

    # ä¸ºæ¯ä¸ªæŠ€èƒ½ç±»åˆ«åˆ›å»ºä¸€ä¸ªåŸºå‡†å‘é‡ï¼ˆåŒç±»æŠ€èƒ½æ›´ç›¸ä¼¼ï¼‰
    category_base_vectors = {
        category: generate_random_vector() for category in skills_data.keys()
    }

    try:
        # åˆ›å»ºæŠ€èƒ½è¯æ±‡è¡¨
        logger.info("åˆ›å»ºæŠ€èƒ½è¯æ±‡è¡¨...")
        total_skills = 0

        for category, skills in skills_data.items():
            base_vec = category_base_vectors[category]

            for skill in skills:
                # åŒç±»æŠ€èƒ½ä½¿ç”¨ç›¸ä¼¼å‘é‡
                vector = generate_similar_vector(base_vec, noise_level=0.08)

                await repo.create_and_save_keyword(
                    keyword=skill,
                    keyword_type=category,
                    vector=vector,
                    vector_model=test_model,
                )
                total_skills += 1

        await repo.flush()
        await repo.load()
        await asyncio.sleep(2)
        logger.info(
            "âœ… åˆ›å»ºäº† %d ä¸ªæŠ€èƒ½å…³é”®è¯ï¼Œåˆ†ä¸º %d ä¸ªç±»åˆ«", total_skills, len(skills_data)
        )

        # åœºæ™¯1: æ ¹æ®å·²çŸ¥æŠ€èƒ½æŸ¥æ‰¾ç›¸ä¼¼æŠ€èƒ½
        logger.info("\nåœºæ™¯1: æŸ¥æ‰¾ä¸ 'Pythonç¼–ç¨‹' ç›¸ä¼¼çš„æŠ€èƒ½")
        python_doc = await repo.get_keyword_by_text("Pythonç¼–ç¨‹", "programming")
        assert python_doc is not None, "åº”è¯¥æ‰¾åˆ° Pythonç¼–ç¨‹"

        similar_to_python = await repo.search_similar_keywords(
            query_vector=python_doc["vector"], keyword_type=None, limit=5  # ä¸é™åˆ¶ç±»å‹
        )

        logger.info("ä¸ 'Pythonç¼–ç¨‹' æœ€ç›¸ä¼¼çš„5ä¸ªæŠ€èƒ½:")
        for i, result in enumerate(similar_to_python, 1):
            logger.info(
                "  %d. %s [%s] (score: %.4f)",
                i,
                result["keyword"],
                result["type"],
                result["score"],
            )

        # éªŒè¯ï¼šæœ€ç›¸ä¼¼çš„åº”è¯¥ä¸»è¦æ˜¯ programming ç±»åˆ«
        top_3_types = [r["type"] for r in similar_to_python[:3]]
        programming_count = sum(1 for t in top_3_types if t == "programming")
        assert programming_count >= 1, "Top 3 åº”è¯¥åŒ…å«è‡³å°‘1ä¸ªç¼–ç¨‹ç±»æŠ€èƒ½"
        logger.info("âœ… ç›¸ä¼¼æŠ€èƒ½æŸ¥æ‰¾ç¬¦åˆé¢„æœŸ")

        # åœºæ™¯2: æŒ‰ç±»åˆ«æŸ¥æ‰¾ç›¸ä¼¼æŠ€èƒ½
        logger.info("\nåœºæ™¯2: åœ¨ 'ai_ml' ç±»åˆ«ä¸­æŸ¥æ‰¾ä¸æŸä¸ªå‘é‡ç›¸ä¼¼çš„æŠ€èƒ½")
        ai_base_vec = category_base_vectors["ai_ml"]
        query_vec = generate_similar_vector(ai_base_vec, noise_level=0.05)

        ai_results = await repo.search_similar_keywords(
            query_vector=query_vec, keyword_type="ai_ml", limit=3
        )

        logger.info("AI/ML ç±»åˆ«ä¸­æœ€ç›¸ä¼¼çš„3ä¸ªæŠ€èƒ½:")
        for i, result in enumerate(ai_results, 1):
            logger.info("  %d. %s (score: %.4f)", i, result["keyword"], result["score"])

        assert len(ai_results) >= 3, "åº”è¯¥æ‰¾åˆ°è‡³å°‘3ä¸ªAI/MLæŠ€èƒ½"
        logger.info("âœ… ç±»åˆ«è¿‡æ»¤æœç´¢æˆåŠŸ")

        # åœºæ™¯3: åˆ—å‡ºæŸä¸ªç±»åˆ«çš„æ‰€æœ‰æŠ€èƒ½
        logger.info("\nåœºæ™¯3: åˆ—å‡º 'database' ç±»åˆ«çš„æ‰€æœ‰æŠ€èƒ½")
        db_skills = await repo.list_keywords_by_type("database")

        logger.info("æ•°æ®åº“ç±»æŠ€èƒ½ï¼ˆå…± %d ä¸ªï¼‰:", len(db_skills))
        for skill in db_skills:
            logger.info("  - %s", skill["keyword"])

        expected_count = len(skills_data["database"])
        assert (
            len(db_skills) == expected_count
        ), f"åº”è¯¥æœ‰ {expected_count} ä¸ªæ•°æ®åº“æŠ€èƒ½ï¼Œå®é™…æœ‰ {len(db_skills)} ä¸ª"
        logger.info("âœ… ç±»åˆ«åˆ—è¡¨æŸ¥è¯¢æˆåŠŸ")

        # åœºæ™¯4: è·¨ç±»åˆ«æœç´¢
        logger.info("\nåœºæ™¯4: è·¨ç±»åˆ«æœç´¢ï¼ˆæŸ¥æ‰¾æ‰€æœ‰ä¸å¼€å‘ç›¸å…³çš„æŠ€èƒ½ï¼‰")
        dev_query_vec = category_base_vectors["programming"]

        all_related = await repo.search_similar_keywords(
            query_vector=dev_query_vec, keyword_type=None, limit=10
        )

        logger.info("ä¸å¼€å‘æœ€ç›¸å…³çš„10ä¸ªæŠ€èƒ½ï¼ˆè·¨ç±»åˆ«ï¼‰:")
        for i, result in enumerate(all_related, 1):
            logger.info(
                "  %d. %s [%s] (score: %.4f)",
                i,
                result["keyword"],
                result["type"],
                result["score"],
            )

        assert len(all_related) == 10, "åº”è¯¥è¿”å›10ä¸ªç»“æœ"
        logger.info("âœ… è·¨ç±»åˆ«æœç´¢æˆåŠŸ")

        # åœºæ™¯5: ç»Ÿè®¡å„ç±»åˆ«æ•°é‡
        logger.info("\nåœºæ™¯5: ç»Ÿè®¡å„ç±»åˆ«æŠ€èƒ½æ•°é‡")
        category_counts = {}

        for category in skills_data.keys():
            keywords = await repo.list_keywords_by_type(category)
            category_counts[category] = len(keywords)

        logger.info("å„ç±»åˆ«æŠ€èƒ½ç»Ÿè®¡:")
        for category, count in category_counts.items():
            expected = len(skills_data[category])
            logger.info("  - %s: %d (é¢„æœŸ: %d)", category, count, expected)
            assert count == expected, f"{category} ç±»åˆ«æ•°é‡ä¸åŒ¹é…"

        logger.info("âœ… ç»Ÿè®¡éªŒè¯æˆåŠŸ")

    except Exception as e:
        logger.error("âŒ æµ‹è¯•çœŸå®åœºæ™¯å¤±è´¥: %s", e)
        raise
    finally:
        # æ¸…ç†æ‰€æœ‰æµ‹è¯•æ•°æ®
        logger.info("\næ¸…ç†æ‰€æœ‰æŠ€èƒ½æ•°æ®...")
        try:
            total_deleted = 0
            for category in skills_data.keys():
                count = await repo.delete_by_type(category)
                total_deleted += count
                logger.info("  æ¸…ç† %s: %d æ¡", category, count)
            await repo.flush()
            logger.info("âœ… æ€»å…±æ¸…ç†äº† %d æ¡æ•°æ®", total_deleted)
        except Exception as cleanup_error:
            logger.error("æ¸…ç†æµ‹è¯•æ•°æ®æ—¶å‡ºç°é”™è¯¯: %s", cleanup_error)

    logger.info("âœ… çœŸå®åœºæ™¯æµ‹è¯•å®Œæˆ\n")


async def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    logger.info("=" * 60)
    logger.info("ğŸš€ å¼€å§‹è¿è¡Œ KeywordVocabularyMilvusRepository æ‰€æœ‰æµ‹è¯•")
    logger.info("=" * 60 + "\n")

    try:
        await test_basic_crud_operations()
        await test_vector_similarity_search()
        await test_type_filtering()
        await test_batch_operations()
        await test_edge_cases()
        await test_real_world_scenario()

        logger.info("=" * 60)
        logger.info("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        logger.info("=" * 60)

    except Exception as e:
        logger.error("=" * 60)
        logger.error("âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: %s", e)
        logger.error("=" * 60)
        raise


if __name__ == "__main__":
    asyncio.run(run_all_tests())
