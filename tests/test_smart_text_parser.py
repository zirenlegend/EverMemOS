#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½æ–‡æœ¬è§£æå™¨æµ‹è¯•æ¨¡å—

å…¨é¢æµ‹è¯•SmartTextParserå’Œç›¸å…³åŠŸèƒ½çš„å„ç§åœºæ™¯
"""

import pytest
import sys
import os


from common_utils.text_utils import (
    SmartTextParser,
    TokenConfig,
    TokenType,
    Token,
    smart_truncate_text,
    clean_whitespace,
)


class TestTokenType:
    """æµ‹è¯•TokenTypeæšä¸¾"""

    def test_token_type_values(self):
        """æµ‹è¯•TokenTypeçš„å€¼"""
        assert TokenType.CJK_CHAR.value == "cjk_char"
        assert TokenType.ENGLISH_WORD.value == "english_word"
        assert TokenType.CONTINUOUS_NUMBER.value == "continuous_number"
        assert TokenType.PUNCTUATION.value == "punctuation"
        assert TokenType.WHITESPACE.value == "whitespace"
        assert TokenType.OTHER.value == "other"


class TestToken:
    """æµ‹è¯•Tokenæ•°æ®ç±»"""

    def test_token_creation(self):
        """æµ‹è¯•Tokenåˆ›å»º"""
        token = Token(
            type=TokenType.CJK_CHAR, content="ä½ ", start_pos=0, end_pos=1, score=1.0
        )
        assert token.type == TokenType.CJK_CHAR
        assert token.content == "ä½ "
        assert token.start_pos == 0
        assert token.end_pos == 1
        assert token.score == 1.0

    def test_token_default_score(self):
        """æµ‹è¯•Tokené»˜è®¤åˆ†æ•°"""
        token = Token(
            type=TokenType.ENGLISH_WORD, content="hello", start_pos=0, end_pos=5
        )
        assert token.score == 0.0


class TestTokenConfig:
    """æµ‹è¯•TokenConfigé…ç½®ç±»"""

    def test_default_config(self):
        """æµ‹è¯•é»˜è®¤é…ç½®"""
        config = TokenConfig()
        assert config.cjk_char_score == 1.0
        assert config.english_word_score == 1.0
        assert config.continuous_number_score == 0.8
        assert config.punctuation_score == 0.2
        assert config.whitespace_score == 0.1
        assert config.other_score == 0.5

    def test_custom_config(self):
        """æµ‹è¯•è‡ªå®šä¹‰é…ç½®"""
        config = TokenConfig(
            cjk_char_score=2.0, english_word_score=0.5, punctuation_score=0.0
        )
        assert config.cjk_char_score == 2.0
        assert config.english_word_score == 0.5
        assert config.punctuation_score == 0.0
        # å…¶ä»–å€¼åº”è¯¥ä¿æŒé»˜è®¤
        assert config.continuous_number_score == 0.8


class TestSmartTextParser:
    """æµ‹è¯•SmartTextParserç±»"""

    def setup_method(self):
        """æ¯ä¸ªæµ‹è¯•å‰çš„è®¾ç½®"""
        self.parser = SmartTextParser()
        self.custom_parser = SmartTextParser(
            TokenConfig(
                cjk_char_score=2.0, english_word_score=0.5, punctuation_score=0.0
            )
        )

    def test_init_default_config(self):
        """æµ‹è¯•é»˜è®¤åˆå§‹åŒ–"""
        parser = SmartTextParser()
        assert parser.config.cjk_char_score == 1.0
        assert parser.config.english_word_score == 1.0

    def test_init_custom_config(self):
        """æµ‹è¯•è‡ªå®šä¹‰é…ç½®åˆå§‹åŒ–"""
        config = TokenConfig(cjk_char_score=2.0)
        parser = SmartTextParser(config)
        assert parser.config.cjk_char_score == 2.0

    def test_is_cjk_char(self):
        """æµ‹è¯•ä¸­æ—¥éŸ©å­—ç¬¦è¯†åˆ«"""
        # ä¸­æ–‡å­—ç¬¦
        assert self.parser._is_cjk_char("ä¸­") == True
        assert self.parser._is_cjk_char("ä½ ") == True

        # æ—¥æ–‡å­—ç¬¦
        assert self.parser._is_cjk_char("ã‚") == True  # å¹³å‡å
        assert self.parser._is_cjk_char("ã‚¢") == True  # ç‰‡å‡å
        assert self.parser._is_cjk_char("æ¼¢") == True  # æ±‰å­—

        # éŸ©æ–‡å­—ç¬¦
        assert self.parser._is_cjk_char("í•œ") == True
        assert self.parser._is_cjk_char("êµ­") == True

        # éä¸­æ—¥éŸ©å­—ç¬¦
        assert self.parser._is_cjk_char("A") == False
        assert self.parser._is_cjk_char("1") == False
        assert self.parser._is_cjk_char("!") == False
        assert self.parser._is_cjk_char("") == False

    def test_is_english_char(self):
        """æµ‹è¯•è‹±æ–‡å­—ç¬¦è¯†åˆ«"""
        assert self.parser._is_english_char("A") == True
        assert self.parser._is_english_char("z") == True
        assert self.parser._is_english_char("ä¸­") == False
        assert self.parser._is_english_char("1") == False
        assert self.parser._is_english_char("!") == False

    def test_is_punctuation(self):
        """æµ‹è¯•æ ‡ç‚¹ç¬¦å·è¯†åˆ«"""
        # åŸºæœ¬æ ‡ç‚¹
        assert self.parser._is_punctuation(".") == True
        assert self.parser._is_punctuation(",") == True
        assert self.parser._is_punctuation("!") == True
        assert self.parser._is_punctuation("?") == True
        assert self.parser._is_punctuation(";") == True
        assert self.parser._is_punctuation(":") == True

        # æ‹¬å·
        assert self.parser._is_punctuation("(") == True
        assert self.parser._is_punctuation(")") == True
        assert self.parser._is_punctuation("[") == True
        assert self.parser._is_punctuation("]") == True

        # ä¸­æ–‡æ ‡ç‚¹
        assert self.parser._is_punctuation("ã€‚") == True
        assert self.parser._is_punctuation("ï¼Œ") == True
        assert self.parser._is_punctuation("ï¼") == True

        # éæ ‡ç‚¹
        assert self.parser._is_punctuation("A") == False
        assert self.parser._is_punctuation("ä¸­") == False
        assert self.parser._is_punctuation("1") == False


class TestParseTokens:
    """æµ‹è¯•parse_tokensæ–¹æ³•"""

    def setup_method(self):
        """æ¯ä¸ªæµ‹è¯•å‰çš„è®¾ç½®"""
        self.parser = SmartTextParser()

    def test_empty_text(self):
        """æµ‹è¯•ç©ºæ–‡æœ¬"""
        tokens = self.parser.parse_tokens("")
        assert tokens == []

        tokens = self.parser.parse_tokens(None)
        assert tokens == []

    def test_single_cjk_char(self):
        """æµ‹è¯•å•ä¸ªä¸­æ—¥éŸ©å­—ç¬¦"""
        tokens = self.parser.parse_tokens("ä½ ")
        assert len(tokens) == 1
        assert tokens[0].type == TokenType.CJK_CHAR
        assert tokens[0].content == "ä½ "
        assert tokens[0].start_pos == 0
        assert tokens[0].end_pos == 1
        assert tokens[0].score == 1.0

    def test_single_english_word(self):
        """æµ‹è¯•å•ä¸ªè‹±æ–‡å•è¯"""
        tokens = self.parser.parse_tokens("hello")
        assert len(tokens) == 1
        assert tokens[0].type == TokenType.ENGLISH_WORD
        assert tokens[0].content == "hello"
        assert tokens[0].start_pos == 0
        assert tokens[0].end_pos == 5
        assert tokens[0].score == 1.0

    def test_english_word_with_apostrophe(self):
        """æµ‹è¯•å¸¦æ’‡å·çš„è‹±æ–‡å•è¯"""
        tokens = self.parser.parse_tokens("don't")
        assert len(tokens) == 1
        assert tokens[0].type == TokenType.ENGLISH_WORD
        assert tokens[0].content == "don't"

    def test_continuous_number(self):
        """æµ‹è¯•è¿ç»­æ•°å­—"""
        tokens = self.parser.parse_tokens("123.45")
        assert len(tokens) == 1
        assert tokens[0].type == TokenType.CONTINUOUS_NUMBER
        assert tokens[0].content == "123.45"
        assert tokens[0].score == 0.8

        tokens = self.parser.parse_tokens("1,234")
        assert len(tokens) == 1
        assert tokens[0].type == TokenType.CONTINUOUS_NUMBER
        assert tokens[0].content == "1,234"

    def test_punctuation(self):
        """æµ‹è¯•æ ‡ç‚¹ç¬¦å·"""
        tokens = self.parser.parse_tokens("!")
        assert len(tokens) == 1
        assert tokens[0].type == TokenType.PUNCTUATION
        assert tokens[0].content == "!"
        assert tokens[0].score == 0.2

    def test_whitespace(self):
        """æµ‹è¯•ç©ºç™½å­—ç¬¦"""
        tokens = self.parser.parse_tokens("   ")
        assert len(tokens) == 1
        assert tokens[0].type == TokenType.WHITESPACE
        assert tokens[0].content == "   "
        assert tokens[0].score == 0.1

    def test_mixed_text(self):
        """æµ‹è¯•æ··åˆæ–‡æœ¬"""
        tokens = self.parser.parse_tokens("Hello ä½ å¥½!")
        assert len(tokens) == 5  # Hello, space, ä½ , å¥½, !

        assert tokens[0].type == TokenType.ENGLISH_WORD
        assert tokens[0].content == "Hello"

        assert tokens[1].type == TokenType.WHITESPACE
        assert tokens[1].content == " "

        assert tokens[2].type == TokenType.CJK_CHAR
        assert tokens[2].content == "ä½ "

        assert tokens[3].type == TokenType.CJK_CHAR
        assert tokens[3].content == "å¥½"

        assert tokens[4].type == TokenType.PUNCTUATION
        assert tokens[4].content == "!"

    def test_complex_mixed_text(self):
        """æµ‹è¯•å¤æ‚æ··åˆæ–‡æœ¬"""
        text = "Python3.9ç‰ˆæœ¬åŒ…å«123ä¸ªæ–°ç‰¹æ€§ã€‚"
        tokens = self.parser.parse_tokens(text)

        expected_types = [
            TokenType.ENGLISH_WORD,  # Python
            TokenType.CONTINUOUS_NUMBER,  # 3.9
            TokenType.CJK_CHAR,  # ç‰ˆ
            TokenType.CJK_CHAR,  # æœ¬
            TokenType.CJK_CHAR,  # åŒ…
            TokenType.CJK_CHAR,  # å«
            TokenType.CONTINUOUS_NUMBER,  # 123
            TokenType.CJK_CHAR,  # ä¸ª
            TokenType.CJK_CHAR,  # æ–°
            TokenType.CJK_CHAR,  # ç‰¹
            TokenType.CJK_CHAR,  # æ€§
            TokenType.PUNCTUATION,  # ã€‚
        ]

        assert len(tokens) == len(expected_types)
        for i, expected_type in enumerate(expected_types):
            assert tokens[i].type == expected_type

    def test_parse_tokens_with_max_score(self):
        """æµ‹è¯•å¸¦æœ€å¤§åˆ†æ•°é™åˆ¶çš„è§£æ"""
        text = "Hello World ä½ å¥½ä¸–ç•Œ"

        # ä¸é™åˆ¶åˆ†æ•°
        tokens_full = self.parser.parse_tokens(text)
        assert len(tokens_full) == 8  # Hello, space, World, space, ä½ , å¥½, ä¸–, ç•Œ

        # é™åˆ¶åˆ†æ•°ä¸º3.0
        tokens_limited = self.parser.parse_tokens(text, max_score=3.0)
        total_score = sum(token.score for token in tokens_limited)
        assert total_score <= 3.0
        assert len(tokens_limited) < len(tokens_full)

    def test_multilingual_text(self):
        """æµ‹è¯•å¤šè¯­è¨€æ–‡æœ¬"""
        text = "Englishä¸­æ–‡æ—¥æœ¬èªí•œêµ­ì–´"
        tokens = self.parser.parse_tokens(text)

        # åº”è¯¥æ­£ç¡®è¯†åˆ«æ‰€æœ‰å­—ç¬¦ç±»å‹
        assert any(token.type == TokenType.ENGLISH_WORD for token in tokens)
        assert any(token.type == TokenType.CJK_CHAR for token in tokens)


class TestCalculateTotalScore:
    """æµ‹è¯•calculate_total_scoreæ–¹æ³•"""

    def setup_method(self):
        self.parser = SmartTextParser()

    def test_empty_tokens(self):
        """æµ‹è¯•ç©ºtokenåˆ—è¡¨"""
        assert self.parser.calculate_total_score([]) == 0.0

    def test_single_token(self):
        """æµ‹è¯•å•ä¸ªtoken"""
        token = Token(TokenType.CJK_CHAR, "ä½ ", 0, 1, 1.0)
        assert self.parser.calculate_total_score([token]) == 1.0

    def test_multiple_tokens(self):
        """æµ‹è¯•å¤šä¸ªtoken"""
        tokens = [
            Token(TokenType.ENGLISH_WORD, "Hello", 0, 5, 1.0),
            Token(TokenType.WHITESPACE, " ", 5, 6, 0.1),
            Token(TokenType.CJK_CHAR, "ä½ ", 6, 7, 1.0),
        ]
        assert self.parser.calculate_total_score(tokens) == 2.1


class TestSmartTruncateByScore:
    """æµ‹è¯•smart_truncate_by_scoreæ–¹æ³•"""

    def setup_method(self):
        self.parser = SmartTextParser()

    def test_empty_text(self):
        """æµ‹è¯•ç©ºæ–‡æœ¬"""
        assert self.parser.smart_truncate_by_score("", 5.0) == ""
        assert self.parser.smart_truncate_by_score(None, 5.0) == ""

    def test_zero_max_score(self):
        """æµ‹è¯•æœ€å¤§åˆ†æ•°ä¸º0"""
        assert self.parser.smart_truncate_by_score("Hello", 0) == "Hello"
        assert self.parser.smart_truncate_by_score("Hello", -1) == "Hello"

    def test_no_truncation_needed(self):
        """æµ‹è¯•ä¸éœ€è¦æˆªæ–­"""
        text = "Hello"
        result = self.parser.smart_truncate_by_score(text, 10.0)
        assert result == text

    def test_simple_truncation(self):
        """æµ‹è¯•ç®€å•æˆªæ–­"""
        text = "Hello World"
        result = self.parser.smart_truncate_by_score(text, 1.5)  # åªå…è®¸ä¸€ä¸ªè‹±æ–‡å•è¯
        # ç”±äºå•è¯è¾¹ç•Œä¿æŠ¤ï¼Œå¯èƒ½ä¿ç•™å®Œæ•´çš„ç¬¬äºŒä¸ªå•è¯
        assert result == "Hello..." or result == "Hello World"

    def test_cjk_truncation(self):
        """æµ‹è¯•ä¸­æ—¥éŸ©å­—ç¬¦æˆªæ–­"""
        text = "ä½ å¥½ä¸–ç•Œ"
        result = self.parser.smart_truncate_by_score(text, 2.0)
        # ç”±äºå•è¯è¾¹ç•Œä¿æŠ¤ï¼Œå¯èƒ½ä¿ç•™æ›´å¤šå†…å®¹
        assert result == "ä½ å¥½..." or result == "ä½ å¥½ä¸–ç•Œ"

    def test_mixed_text_truncation(self):
        """æµ‹è¯•æ··åˆæ–‡æœ¬æˆªæ–­"""
        text = "Hello ä½ å¥½ä¸–ç•Œ"
        # Hello(1.0) + space(0.1) + ä½ (1.0) + å¥½(1.0) = 3.1
        result = self.parser.smart_truncate_by_score(text, 3.0)
        # ç”±äºå•è¯è¾¹ç•Œä¿æŠ¤ï¼Œå¯èƒ½ä¿ç•™æ›´å¤šå†…å®¹
        assert result == "Hello ä½ ..." or result == "Hello ä½ å¥½ä¸–ç•Œ"

    def test_custom_suffix(self):
        """æµ‹è¯•è‡ªå®šä¹‰åç¼€"""
        text = "Hello World"
        result = self.parser.smart_truncate_by_score(text, 1.5, suffix="[...]")
        # ç”±äºå•è¯è¾¹ç•Œä¿æŠ¤ï¼Œå¯èƒ½ä¸éœ€è¦æˆªæ–­
        assert result == "Hello[...]" or result == "Hello World"

    def test_punctuation_handling(self):
        """æµ‹è¯•æ ‡ç‚¹ç¬¦å·å¤„ç†"""
        config = TokenConfig(punctuation_score=0.5)
        parser = SmartTextParser(config)

        text = "Hello, World!"
        result = parser.smart_truncate_by_score(text, 2.0)
        # Hello(1.0) + ,(0.5) + space(0.1) + World(1.0) = 2.6 > 2.0
        # ç”±äºå•è¯è¾¹ç•Œä¿æŠ¤ï¼Œå¯èƒ½ä¿ç•™å®Œæ•´å†…å®¹
        assert "Hello," in result

    def test_word_boundary_protection(self):
        """æµ‹è¯•å•è¯è¾¹ç•Œä¿æŠ¤"""
        text = "Hello World"
        result = self.parser.smart_truncate_by_score(text, 1.8)  # åˆšå¥½è¶…è¿‡ä¸€ä¸ªå•è¯
        # åº”è¯¥å®Œæ•´ä¿ç•™ç¬¬äºŒä¸ªå•è¯ï¼Œä¸åœ¨ä¸­é—´æˆªæ–­
        assert result == "Hello World" or result == "Hello..."

    def test_fallback_mode_enabled(self):
        """æµ‹è¯•å¯ç”¨fallbackæ¨¡å¼"""
        # æ¨¡æ‹Ÿè§£æå¼‚å¸¸çš„æƒ…å†µ
        text = "Normal text"
        result = self.parser.smart_truncate_by_score(text, 5.0, enable_fallback=True)
        assert isinstance(result, str)

    def test_fallback_mode_disabled(self):
        """æµ‹è¯•ç¦ç”¨fallbackæ¨¡å¼"""
        text = "Normal text"
        # æ­£å¸¸æƒ…å†µä¸‹ä¸ä¼šæŠ›å‡ºå¼‚å¸¸
        result = self.parser.smart_truncate_by_score(text, 5.0, enable_fallback=False)
        assert isinstance(result, str)


class TestGetTextAnalysis:
    """æµ‹è¯•get_text_analysisæ–¹æ³•"""

    def setup_method(self):
        self.parser = SmartTextParser()

    def test_empty_text(self):
        """æµ‹è¯•ç©ºæ–‡æœ¬åˆ†æ"""
        analysis = self.parser.get_text_analysis("")
        assert analysis["total_tokens"] == 0
        assert analysis["total_score"] == 0.0
        assert all(count == 0 for count in analysis["type_counts"].values())

    def test_simple_text_analysis(self):
        """æµ‹è¯•ç®€å•æ–‡æœ¬åˆ†æ"""
        text = "Hello ä½ å¥½"
        analysis = self.parser.get_text_analysis(text)

        assert analysis["total_tokens"] == 4  # Hello, space, ä½ , å¥½
        assert analysis["total_score"] == 3.1  # 1.0 + 0.1 + 1.0 + 1.0

        assert analysis["type_counts"]["english_word"] == 1
        assert analysis["type_counts"]["cjk_char"] == 2
        assert analysis["type_counts"]["whitespace"] == 1

        assert analysis["type_scores"]["english_word"] == 1.0
        assert analysis["type_scores"]["cjk_char"] == 2.0
        assert analysis["type_scores"]["whitespace"] == 0.1

    def test_complex_text_analysis(self):
        """æµ‹è¯•å¤æ‚æ–‡æœ¬åˆ†æ"""
        text = "Python3.9ç‰ˆæœ¬åŒ…å«123ä¸ªæ–°ç‰¹æ€§ï¼"
        analysis = self.parser.get_text_analysis(text)

        # éªŒè¯tokenæ•°é‡å’Œç±»å‹
        assert analysis["total_tokens"] > 0
        assert analysis["type_counts"]["english_word"] >= 1  # Python
        assert analysis["type_counts"]["continuous_number"] >= 2  # 3.9, 123
        assert analysis["type_counts"]["cjk_char"] >= 6  # ç‰ˆæœ¬åŒ…å«ä¸ªæ–°ç‰¹æ€§
        assert analysis["type_counts"]["punctuation"] >= 1  # ï¼


class TestSmartTruncateText:
    """æµ‹è¯•å‘åå…¼å®¹çš„smart_truncate_textå‡½æ•°"""

    def test_backward_compatibility(self):
        """æµ‹è¯•å‘åå…¼å®¹æ€§"""
        text = "Hello World ä½ å¥½ä¸–ç•Œ"

        # åŸºæœ¬è°ƒç”¨
        result = smart_truncate_text(text, 4)
        # åº”è¯¥è¢«æˆªæ–­ï¼ˆå› ä¸ºæ€»åˆ†æ•°è¶…è¿‡4ï¼‰
        assert "..." in result or result == text

        # å¸¦æƒé‡è°ƒç”¨
        result_weighted = smart_truncate_text(text, 4, chinese_weight=0.5)
        # ä¸­æ–‡æƒé‡é™ä½ï¼Œå¯èƒ½ä¸éœ€è¦æˆªæ–­ï¼Œæ‰€ä»¥ç»“æœå¯èƒ½æ›´çŸ­ï¼ˆæ²¡æœ‰"..."åç¼€ï¼‰
        assert "..." not in result_weighted or len(result_weighted) >= len(result)

    def test_empty_and_edge_cases(self):
        """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
        assert smart_truncate_text("", 5) == ""
        assert smart_truncate_text(None, 5) == ""
        assert smart_truncate_text("Hello", 0) == "Hello"
        assert smart_truncate_text("Hello", -1) == "Hello"

    def test_custom_weights(self):
        """æµ‹è¯•è‡ªå®šä¹‰æƒé‡"""
        text = "Hello World ä½ å¥½ä¸–ç•Œæµ‹è¯•é•¿æ–‡æœ¬"  # ä½¿ç”¨æ›´é•¿çš„æ–‡æœ¬

        # ä½¿ç”¨è¾ƒå°çš„é™åˆ¶ç¡®ä¿æˆªæ–­
        # é»˜è®¤æƒé‡
        result1 = smart_truncate_text(text, 4)

        # é™ä½ä¸­æ–‡æƒé‡
        result2 = smart_truncate_text(text, 4, chinese_weight=0.2)

        # é™ä½è‹±æ–‡æƒé‡
        result3 = smart_truncate_text(text, 4, english_word_weight=0.2)

        # ç”±äºä¼˜åŒ–åçš„å•è¯è¾¹ç•Œä¿æŠ¤ï¼Œå¯èƒ½ç»“æœç›¸åŒï¼Œè¿™æ˜¯æ­£å¸¸çš„
        # è‡³å°‘ç¡®ä¿åŠŸèƒ½æ­£å¸¸å·¥ä½œ
        assert isinstance(result1, str)
        assert isinstance(result2, str)
        assert isinstance(result3, str)


class TestPerformance:
    """æ€§èƒ½æµ‹è¯•"""

    def setup_method(self):
        self.parser = SmartTextParser()

    def test_large_text_parsing(self):
        """æµ‹è¯•å¤§æ–‡æœ¬è§£ææ€§èƒ½"""
        import time

        # ç”Ÿæˆå¤§æ–‡æœ¬
        large_text = "Hello World ä½ å¥½ä¸–ç•Œ! " * 100

        start_time = time.time()
        tokens = self.parser.parse_tokens(large_text)
        end_time = time.time()

        assert len(tokens) > 0
        assert (end_time - start_time) < 1.0  # åº”è¯¥åœ¨1ç§’å†…å®Œæˆ

    def test_early_truncation_performance(self):
        """æµ‹è¯•æå‰æˆªæ–­çš„æ€§èƒ½ä¼˜åŠ¿"""
        import time

        # ç”Ÿæˆå¤§æ–‡æœ¬
        large_text = "Hello World ä½ å¥½ä¸–ç•Œ! " * 1000

        # ä¸é™åˆ¶åˆ†æ•°çš„è§£æ
        start_time = time.time()
        tokens_full = self.parser.parse_tokens(large_text)
        time_full = time.time() - start_time

        # é™åˆ¶åˆ†æ•°çš„è§£æ
        start_time = time.time()
        tokens_limited = self.parser.parse_tokens(large_text, max_score=10.0)
        time_limited = time.time() - start_time

        # é™åˆ¶åˆ†æ•°çš„è§£æåº”è¯¥æ›´å¿«
        assert len(tokens_limited) < len(tokens_full)
        assert time_limited <= time_full  # é€šå¸¸åº”è¯¥æ›´å¿«ï¼Œä½†è‡³å°‘ä¸ä¼šæ›´æ…¢


class TestEdgeCases:
    """è¾¹ç•Œæƒ…å†µæµ‹è¯•"""

    def setup_method(self):
        self.parser = SmartTextParser()

    def test_special_characters(self):
        """æµ‹è¯•ç‰¹æ®Šå­—ç¬¦"""
        special_chars = "Â°Â©Â®â„¢â‚¬Â£Â¥Â§Â¶â€ â€¡â€¢â€¦â€°â€¹â€º" "''â€“â€”"
        tokens = self.parser.parse_tokens(special_chars)
        assert len(tokens) > 0
        # å¤§éƒ¨åˆ†åº”è¯¥è¢«è¯†åˆ«ä¸ºOTHERç±»å‹
        assert any(token.type == TokenType.OTHER for token in tokens)

    def test_emoji_handling(self):
        """æµ‹è¯•emojiå¤„ç†"""
        text = "Hello ğŸ˜Š ä½ å¥½ ğŸŒŸ"
        tokens = self.parser.parse_tokens(text)
        assert len(tokens) > 0
        # emojiåº”è¯¥è¢«è¯†åˆ«ä¸ºOTHERç±»å‹
        emoji_tokens = [token for token in tokens if token.type == TokenType.OTHER]
        assert len(emoji_tokens) >= 2  # è‡³å°‘æœ‰ä¸¤ä¸ªemoji

    def test_mixed_numbers_and_letters(self):
        """æµ‹è¯•æ•°å­—å­—æ¯æ··åˆ"""
        text = "ABC123DEF456"
        tokens = self.parser.parse_tokens(text)

        # åº”è¯¥è¢«åˆ†åˆ«è¯†åˆ«ä¸ºè‹±æ–‡å•è¯å’Œæ•°å­—
        assert len(tokens) == 4
        assert tokens[0].type == TokenType.ENGLISH_WORD
        assert tokens[0].content == "ABC"
        assert tokens[1].type == TokenType.CONTINUOUS_NUMBER
        assert tokens[1].content == "123"
        assert tokens[2].type == TokenType.ENGLISH_WORD
        assert tokens[2].content == "DEF"
        assert tokens[3].type == TokenType.CONTINUOUS_NUMBER
        assert tokens[3].content == "456"

    def test_url_like_text(self):
        """æµ‹è¯•URLç±»ä¼¼æ–‡æœ¬"""
        text = "https://example.com/path?param=value"
        tokens = self.parser.parse_tokens(text)

        # åº”è¯¥è¢«æ­£ç¡®åˆ†å‰²
        assert len(tokens) > 1
        # åŒ…å«è‹±æ–‡å•è¯ã€æ ‡ç‚¹ã€æ•°å­—ç­‰
        token_types = {token.type for token in tokens}
        assert TokenType.ENGLISH_WORD in token_types
        assert TokenType.PUNCTUATION in token_types

    def test_very_long_word(self):
        """æµ‹è¯•æé•¿å•è¯"""
        long_word = "a" * 1000
        tokens = self.parser.parse_tokens(long_word)
        assert len(tokens) == 1
        assert tokens[0].type == TokenType.ENGLISH_WORD
        assert tokens[0].content == long_word

    def test_unicode_edge_cases(self):
        """æµ‹è¯•Unicodeè¾¹ç•Œæƒ…å†µ"""
        # æµ‹è¯•å„ç§UnicodeèŒƒå›´çš„å­—ç¬¦
        text = "ğŸ€€ğŸ€ğŸ€‚"  # Mahjong tiles
        tokens = self.parser.parse_tokens(text)
        assert len(tokens) == 3
        assert all(token.type == TokenType.OTHER for token in tokens)


class TestCleanWhitespace:
    """æµ‹è¯•clean_whitespaceå‡½æ•°"""

    def test_empty_text(self):
        """æµ‹è¯•ç©ºæ–‡æœ¬"""
        assert clean_whitespace("") == ""
        assert clean_whitespace(None) == None

    def test_no_whitespace(self):
        """æµ‹è¯•æ²¡æœ‰ç©ºç™½å­—ç¬¦çš„æ–‡æœ¬"""
        text = "HelloWorld"
        assert clean_whitespace(text) == text

    def test_single_spaces(self):
        """æµ‹è¯•å•ä¸ªç©ºæ ¼"""
        text = "Hello World"
        assert clean_whitespace(text) == "Hello World"

    def test_multiple_spaces(self):
        """æµ‹è¯•å¤šä¸ªè¿ç»­ç©ºæ ¼"""
        text = "Hello    World"
        assert clean_whitespace(text) == "Hello World"

    def test_mixed_whitespace(self):
        """æµ‹è¯•æ··åˆç©ºç™½å­—ç¬¦"""
        text = "Hello\t\n  \r World"
        assert clean_whitespace(text) == "Hello World"

    def test_leading_trailing_whitespace(self):
        """æµ‹è¯•é¦–å°¾ç©ºç™½"""
        text = "  Hello World  "
        assert clean_whitespace(text) == "Hello World"

    def test_complex_mixed_text(self):
        """æµ‹è¯•å¤æ‚æ··åˆæ–‡æœ¬"""
        text = "  Hello   World!  \t\n  ä½ å¥½    ä¸–ç•Œã€‚  "
        result = clean_whitespace(text)
        assert result == "Hello World! ä½ å¥½ ä¸–ç•Œã€‚"
        # ç¡®ä¿ä¸­æ–‡å­—ç¬¦å’Œæ ‡ç‚¹ç¬¦å·ä¿æŒå®Œæ•´
        assert "ä½ å¥½" in result
        assert "ä¸–ç•Œ" in result
        assert "!" in result
        assert "ã€‚" in result

    def test_preserve_non_whitespace_tokens(self):
        """æµ‹è¯•ä¿æŒéç©ºç™½tokençš„å®Œæ•´æ€§"""
        text = "Python3.9   ç‰ˆæœ¬  åŒ…å«   123ä¸ª   æ–°ç‰¹æ€§ï¼"
        result = clean_whitespace(text)
        expected = "Python3.9 ç‰ˆæœ¬ åŒ…å« 123ä¸ª æ–°ç‰¹æ€§ï¼"
        assert result == expected
        # ç¡®ä¿æ•°å­—ã€è‹±æ–‡å•è¯ã€ä¸­æ–‡å­—ç¬¦éƒ½ä¿æŒå®Œæ•´
        assert "Python3.9" in result
        assert "123" in result
        assert "æ–°ç‰¹æ€§" in result

    def test_only_whitespace(self):
        """æµ‹è¯•çº¯ç©ºç™½å­—ç¬¦"""
        text = "   \t\n\r   "
        assert clean_whitespace(text) == ""

    def test_whitespace_between_cjk_chars(self):
        """æµ‹è¯•ä¸­æ—¥éŸ©å­—ç¬¦é—´çš„ç©ºç™½"""
        text = "ä½   å¥½  ä¸–  ç•Œ"
        assert clean_whitespace(text) == "ä½  å¥½ ä¸– ç•Œ"

    def test_whitespace_around_punctuation(self):
        """æµ‹è¯•æ ‡ç‚¹ç¬¦å·å‘¨å›´çš„ç©ºç™½"""
        text = "Hello  ,   World  !  "
        result = clean_whitespace(text)
        assert result == "Hello , World !"
        # ç¡®ä¿æ ‡ç‚¹ç¬¦å·ä¿æŒåŸæ ·
        assert "," in result
        assert "!" in result


if __name__ == "__main__":
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    pytest.main([__file__, "-v"])
