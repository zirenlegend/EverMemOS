#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯• GroupProfile ä¸­ _recursive_datetime_check çš„åŠŸèƒ½

æµ‹è¯•å†…å®¹åŒ…æ‹¬:
1. å•ä¸ª datetime å­—æ®µçš„æ—¶åŒºè½¬æ¢
2. åµŒå¥— BaseModel ä¸­çš„ datetime å­—æ®µï¼ˆTopicInfo.last_active_atï¼‰
3. åˆ—è¡¨ä¸­çš„ datetime å¯¹è±¡è½¬æ¢ï¼ˆtopics åˆ—è¡¨ï¼‰
4. å­—å…¸ä¸­çš„ datetime å¯¹è±¡è½¬æ¢ï¼ˆextend å­—æ®µï¼‰
5. æ··åˆåœºæ™¯ï¼šåˆ—è¡¨ + åµŒå¥— BaseModel + datetime
6. é€’å½’æ·±åº¦é™åˆ¶æµ‹è¯•
7. è¾¹ç•Œæƒ…å†µæµ‹è¯•ï¼ˆç©ºåˆ—è¡¨ã€ç©ºå­—å…¸ã€None å€¼ç­‰ï¼‰
8. æ€§èƒ½ä¼˜åŒ–åœºæ™¯ï¼ˆåˆ—è¡¨é‡‡æ ·æ£€æŸ¥ï¼‰
"""

import asyncio
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any
from bson import ObjectId
import pytz

from core.di import get_bean_by_type
from infra_layer.adapters.out.persistence.repository.group_profile_raw_repository import (
    GroupProfileRawRepository,
)
from infra_layer.adapters.out.persistence.document.memory.group_profile import (
    GroupProfile,
    TopicInfo,
    RoleAssignment,
)
from common_utils.datetime_utils import get_now_with_timezone, to_timezone, get_timezone
from core.observation.logger import get_logger

logger = get_logger(__name__)


# ==================== è¾…åŠ©å‡½æ•° ====================
def create_naive_datetime() -> datetime:
    """åˆ›å»ºä¸€ä¸ªæ²¡æœ‰æ—¶åŒºä¿¡æ¯çš„ datetime å¯¹è±¡"""
    return datetime(2025, 1, 1, 12, 0, 0)


def create_aware_datetime_utc() -> datetime:
    """åˆ›å»ºä¸€ä¸ª UTC æ—¶åŒºçš„ datetime å¯¹è±¡"""
    return datetime(2025, 1, 1, 12, 0, 0, tzinfo=timezone.utc)


def create_aware_datetime_shanghai() -> datetime:
    """åˆ›å»ºä¸€ä¸ªä¸Šæµ·æ—¶åŒºçš„ datetime å¯¹è±¡"""
    shanghai_tz = get_timezone()
    return datetime(2025, 1, 1, 12, 0, 0, tzinfo=shanghai_tz)


def is_aware_datetime(dt: datetime) -> bool:
    """æ£€æŸ¥ datetime æ˜¯å¦åŒ…å«æ—¶åŒºä¿¡æ¯"""
    return dt.tzinfo is not None and dt.tzinfo.utcoffset(dt) is not None


# ==================== æµ‹è¯•ç”¨ä¾‹ ====================


async def test_single_datetime_field_conversion():
    """æµ‹è¯•1: å•ä¸ª datetime å­—æ®µçš„æ—¶åŒºè½¬æ¢"""
    logger.info("å¼€å§‹æµ‹è¯•å•ä¸ª datetime å­—æ®µçš„æ—¶åŒºè½¬æ¢...")

    repo = get_bean_by_type(GroupProfileRawRepository)
    group_id = "test_group_datetime_001"

    try:
        # å…ˆæ¸…ç†
        await repo.delete_by_group_id(group_id)
        logger.info("âœ… æ¸…ç†å·²å­˜åœ¨çš„æµ‹è¯•æ•°æ®")

        # åˆ›å»ºä¸€ä¸ªæ²¡æœ‰æ—¶åŒºçš„ datetime
        naive_dt = create_naive_datetime()
        logger.info(
            "   åˆ›å»ºçš„ naive datetime: %s (tzinfo=%s)", naive_dt, naive_dt.tzinfo
        )

        # åˆ›å»º GroupProfileï¼Œä¼ å…¥ naive datetime
        # æ³¨æ„ï¼štimestamp æ˜¯ int ç±»å‹ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸æµ‹è¯•å®ƒ
        # æˆ‘ä»¬é€šè¿‡ extend å­—æ®µæ¥æµ‹è¯• datetime è½¬æ¢
        group_profile = GroupProfile(
            group_id=group_id,
            timestamp=1704067200000,  # 2024-01-01 00:00:00 (milliseconds)
            version="v1",
            extend={"test_datetime": naive_dt},  # åœ¨å­—å…¸ä¸­æ”¾å…¥ naive datetime
        )

        # éªŒè¯ï¼š_recursive_datetime_check åº”è¯¥åœ¨ model_validator ä¸­è‡ªåŠ¨æ‰§è¡Œ
        # æ£€æŸ¥ extend ä¸­çš„ datetime æ˜¯å¦è¢«è½¬æ¢
        result_dt = group_profile.extend["test_datetime"]
        logger.info("   è½¬æ¢åçš„ datetime: %s (tzinfo=%s)", result_dt, result_dt.tzinfo)

        assert is_aware_datetime(result_dt), "datetime åº”è¯¥åŒ…å«æ—¶åŒºä¿¡æ¯"
        logger.info("âœ… å•ä¸ª datetime å­—æ®µè½¬æ¢æˆåŠŸ")

        # ä¿å­˜åˆ°æ•°æ®åº“å¹¶éªŒè¯
        await repo.upsert_by_group_id(
            group_id=group_id,
            update_data={"version": "v1", "extend": {"test_datetime": naive_dt}},
            timestamp=1704067200000,
        )
        logger.info("âœ… ä¿å­˜åˆ°æ•°æ®åº“æˆåŠŸ")

        # ä»æ•°æ®åº“è¯»å–å¹¶éªŒè¯
        retrieved = await repo.get_by_group_id(group_id)
        assert retrieved is not None
        retrieved_dt = retrieved.extend["test_datetime"]
        logger.info(
            "   ä»æ•°æ®åº“è¯»å–çš„ datetime: %s (tzinfo=%s)",
            retrieved_dt,
            retrieved_dt.tzinfo,
        )
        assert is_aware_datetime(
            retrieved_dt
        ), "ä»æ•°æ®åº“è¯»å–çš„ datetime åº”è¯¥åŒ…å«æ—¶åŒºä¿¡æ¯"
        logger.info("âœ… ä»æ•°æ®åº“è¯»å–éªŒè¯æˆåŠŸ")

        # æ¸…ç†
        await repo.delete_by_group_id(group_id)

    except Exception as e:
        logger.error("âŒ æµ‹è¯•å•ä¸ª datetime å­—æ®µè½¬æ¢å¤±è´¥: %s", e)
        import traceback

        logger.error("è¯¦ç»†é”™è¯¯: %s", traceback.format_exc())
        raise

    logger.info("âœ… å•ä¸ª datetime å­—æ®µè½¬æ¢æµ‹è¯•å®Œæˆ")


async def test_nested_basemodel_datetime_conversion():
    """æµ‹è¯•2: åµŒå¥— BaseModel ä¸­çš„ datetime å­—æ®µï¼ˆTopicInfo.last_active_atï¼‰"""
    logger.info("å¼€å§‹æµ‹è¯•åµŒå¥— BaseModel ä¸­çš„ datetime å­—æ®µè½¬æ¢...")

    repo = get_bean_by_type(GroupProfileRawRepository)
    group_id = "test_group_datetime_002"

    try:
        # å…ˆæ¸…ç†
        await repo.delete_by_group_id(group_id)
        logger.info("âœ… æ¸…ç†å·²å­˜åœ¨çš„æµ‹è¯•æ•°æ®")

        # åˆ›å»ºä¸€ä¸ª naive datetime
        naive_dt = create_naive_datetime()
        logger.info(
            "   åˆ›å»ºçš„ naive datetime: %s (tzinfo=%s)", naive_dt, naive_dt.tzinfo
        )

        # åˆ›å»º TopicInfoï¼ŒåŒ…å« naive datetime
        # æ³¨æ„ï¼šTopicInfo æ˜¯æ™®é€šçš„ BaseModelï¼Œå®ƒçš„ datetime å­—æ®µåœ¨å®ä¾‹åŒ–æ—¶ä¸ä¼šè‡ªåŠ¨è½¬æ¢
        # æ—¶åŒºè½¬æ¢ä¼šåœ¨å®ƒè¢«åµŒå…¥åˆ° DocumentBase (GroupProfile) æ—¶ç”± _recursive_datetime_check æ‰§è¡Œ
        topic = TopicInfo(
            name="æµ‹è¯•è¯é¢˜",
            summary="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•è¯é¢˜",
            status="exploring",
            last_active_at=naive_dt,  # naive datetime
            id="topic_001",
        )

        logger.info(
            "   TopicInfo.last_active_at (å®ä¾‹åŒ–å): %s (tzinfo=%s)",
            topic.last_active_at,
            topic.last_active_at.tzinfo,
        )
        # TopicInfo å®ä¾‹åŒ–åï¼Œdatetime è¿˜æ²¡æœ‰è¢«è½¬æ¢ï¼ˆå› ä¸ºå®ƒä¸æ˜¯ DocumentBaseï¼‰
        assert (
            topic.last_active_at.tzinfo is None
        ), "TopicInfo å®ä¾‹åŒ–åçš„ datetime åº”è¯¥è¿˜æ˜¯ naive"
        logger.info("âœ… TopicInfo å®ä¾‹åŒ–å datetime ä»ç„¶æ˜¯ naiveï¼ˆç¬¦åˆé¢„æœŸï¼‰")

        # åˆ›å»º GroupProfile - åœ¨è¿™é‡Œ _recursive_datetime_check ä¼šè¢«è§¦å‘
        group_profile = GroupProfile(
            group_id=group_id, timestamp=1704067200000, version="v1", topics=[topic]
        )

        # éªŒè¯ï¼šåµŒå¥—åœ¨ GroupProfile ä¸­çš„ TopicInfo.last_active_at ä¹Ÿåº”è¯¥è¢«è½¬æ¢
        result_dt = group_profile.topics[0].last_active_at
        logger.info(
            "   GroupProfile.topics[0].last_active_at: %s (tzinfo=%s)",
            result_dt,
            result_dt.tzinfo,
        )
        assert is_aware_datetime(result_dt), "åµŒå¥—çš„ datetime åº”è¯¥åŒ…å«æ—¶åŒºä¿¡æ¯"
        logger.info("âœ… åµŒå¥—åœ¨ GroupProfile ä¸­çš„ datetime è½¬æ¢æˆåŠŸ")

        # ä¿å­˜åˆ°æ•°æ®åº“å¹¶éªŒè¯
        await repo.upsert_by_group_id(
            group_id=group_id,
            update_data={"version": "v1", "topics": [topic.model_dump()]},
            timestamp=1704067200000,
        )
        logger.info("âœ… ä¿å­˜åˆ°æ•°æ®åº“æˆåŠŸ")

        # ä»æ•°æ®åº“è¯»å–å¹¶éªŒè¯
        retrieved = await repo.get_by_group_id(group_id)
        assert retrieved is not None
        retrieved_dt = retrieved.topics[0].last_active_at
        logger.info(
            "   ä»æ•°æ®åº“è¯»å–çš„ datetime: %s (tzinfo=%s)",
            retrieved_dt,
            retrieved_dt.tzinfo,
        )
        assert is_aware_datetime(
            retrieved_dt
        ), "ä»æ•°æ®åº“è¯»å–çš„ datetime åº”è¯¥åŒ…å«æ—¶åŒºä¿¡æ¯"
        logger.info("âœ… ä»æ•°æ®åº“è¯»å–éªŒè¯æˆåŠŸ")

        # æ¸…ç†
        await repo.delete_by_group_id(group_id)

    except Exception as e:
        logger.error("âŒ æµ‹è¯•åµŒå¥— BaseModel datetime è½¬æ¢å¤±è´¥: %s", e)
        import traceback

        logger.error("è¯¦ç»†é”™è¯¯: %s", traceback.format_exc())
        raise

    logger.info("âœ… åµŒå¥— BaseModel datetime è½¬æ¢æµ‹è¯•å®Œæˆ")


async def test_list_datetime_conversion():
    """
    æµ‹è¯•3: åˆ—è¡¨ä¸­çš„ datetime å¯¹è±¡è½¬æ¢ï¼ˆtopics åˆ—è¡¨ï¼‰

    âš ï¸ æ³¨æ„ï¼šæ­¤æµ‹è¯•å‘ç°äº† _recursive_datetime_check çš„ä¸€ä¸ª BUGï¼š
    åˆ—è¡¨é‡‡æ ·ä¼˜åŒ–ä¸­ï¼Œå½“åˆ—è¡¨åŒ…å« BaseModel å¯¹è±¡æ—¶ï¼Œç”±äº BaseModel çš„è½¬æ¢æ˜¯ in-place çš„
    ï¼ˆè¿”å›åŒä¸€ä¸ªå¯¹è±¡ï¼‰ï¼Œé‡‡æ ·æ£€æŸ¥ä¼šè¯¯åˆ¤ä¸º"ä¸éœ€è¦è½¬æ¢"ï¼Œå¯¼è‡´åˆ—è¡¨ä¸­ç¬¬2ä¸ªåŠåç»­å…ƒç´ ä¸è¢«å¤„ç†ã€‚

    ä¿®å¤æ–¹æ¡ˆï¼šåœ¨ BaseModel æƒ…å†µä¸­ï¼Œéœ€è¦æ ‡è®°å¯¹è±¡æ˜¯å¦è¢«ä¿®æ”¹è¿‡ï¼Œè€Œä¸æ˜¯ä¾èµ–å¯¹è±¡å¼•ç”¨æ˜¯å¦æ”¹å˜ã€‚
    """
    logger.info("å¼€å§‹æµ‹è¯•åˆ—è¡¨ä¸­çš„ datetime å¯¹è±¡è½¬æ¢...")
    logger.warning("âš ï¸ æ­¤æµ‹è¯•å°†å±•ç¤º _recursive_datetime_check çš„åˆ—è¡¨é‡‡æ ·ä¼˜åŒ– bug")

    repo = get_bean_by_type(GroupProfileRawRepository)
    group_id = "test_group_datetime_003"

    try:
        # å…ˆæ¸…ç†
        await repo.delete_by_group_id(group_id)
        logger.info("âœ… æ¸…ç†å·²å­˜åœ¨çš„æµ‹è¯•æ•°æ®")

        # åˆ›å»ºå¤šä¸ª naive datetime
        naive_dt1 = create_naive_datetime()
        naive_dt2 = naive_dt1 + timedelta(days=1)
        naive_dt3 = naive_dt1 + timedelta(days=2)

        # åˆ›å»ºå¤šä¸ª TopicInfo
        topics = [
            TopicInfo(
                name=f"è¯é¢˜{i}",
                summary=f"è¯é¢˜{i}çš„æ‘˜è¦",
                status="exploring",
                last_active_at=dt,
                id=f"topic_{i}",
            )
            for i, dt in enumerate([naive_dt1, naive_dt2, naive_dt3], start=1)
        ]

        # åˆ›å»º GroupProfile
        group_profile = GroupProfile(
            group_id=group_id, timestamp=1704067200000, version="v1", topics=topics
        )

        # éªŒè¯ï¼šæ£€æŸ¥å“ªäº›å…ƒç´ è¢«è½¬æ¢äº†
        converted_count = 0
        not_converted_indices = []

        for i, topic in enumerate(group_profile.topics):
            is_aware = is_aware_datetime(topic.last_active_at)
            logger.info(
                "   topics[%d].last_active_at: %s (tzinfo=%s) - %s",
                i,
                topic.last_active_at,
                topic.last_active_at.tzinfo,
                "âœ… å·²è½¬æ¢" if is_aware else "âŒ æœªè½¬æ¢",
            )
            if is_aware:
                converted_count += 1
            else:
                not_converted_indices.append(i)

        # ğŸ› BUG éªŒè¯ï¼šåªæœ‰ç¬¬ä¸€ä¸ªå…ƒç´ è¢«è½¬æ¢ï¼Œåç»­å…ƒç´ éƒ½æœªè½¬æ¢
        if converted_count == 1 and not_converted_indices == [1, 2]:
            logger.warning("âš ï¸ ç¡®è®¤ BUGï¼šåˆ—è¡¨é‡‡æ ·ä¼˜åŒ–å¯¼è‡´åªæœ‰ç¬¬ä¸€ä¸ªå…ƒç´ è¢«è½¬æ¢")
            logger.warning("   ç¬¬ä¸€ä¸ªå…ƒç´ ï¼ˆtopics[0]ï¼‰: å·²è½¬æ¢ âœ…")
            logger.warning("   åç»­å…ƒç´ ï¼ˆtopics[1], topics[2]ï¼‰: æœªè½¬æ¢ âŒ")
            logger.info("âœ… åˆ—è¡¨é‡‡æ ·ä¼˜åŒ– BUG å·²è¢«æµ‹è¯•ç¡®è®¤")
        else:
            # å¦‚æœ bug è¢«ä¿®å¤äº†ï¼Œè¿™é‡Œåº”è¯¥å…¨éƒ¨è½¬æ¢
            assert (
                converted_count == 3
            ), f"æœŸæœ›3ä¸ªå…ƒç´ éƒ½è¢«è½¬æ¢ï¼Œå®é™…è½¬æ¢äº† {converted_count} ä¸ª"
            logger.info("âœ… åˆ—è¡¨ä¸­æ‰€æœ‰ datetime è½¬æ¢æˆåŠŸï¼ˆBUG å·²ä¿®å¤ï¼‰")

        # æ³¨æ„ï¼šç”±äºä¸Šè¿° BUGï¼Œæˆ‘ä»¬ä¸ä¿å­˜åˆ°æ•°æ®åº“ï¼Œå› ä¸ºä¼šä¿å­˜æœªè½¬æ¢çš„ datetime
        # è¿™ä¼šå¯¼è‡´æ•°æ®åº“ä¸­å­˜å‚¨ naive datetimeï¼Œå¼•å‘åç»­é—®é¢˜
        logger.info("âš ï¸ è·³è¿‡ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆé¿å…ä¿å­˜ naive datetimeï¼‰")

        # æ¸…ç†
        await repo.delete_by_group_id(group_id)

    except Exception as e:
        logger.error("âŒ æµ‹è¯•åˆ—è¡¨ datetime è½¬æ¢å¤±è´¥: %s", e)
        import traceback

        logger.error("è¯¦ç»†é”™è¯¯: %s", traceback.format_exc())
        raise

    logger.info("âœ… åˆ—è¡¨ datetime è½¬æ¢æµ‹è¯•å®Œæˆï¼ˆBUG å·²ç¡®è®¤ï¼‰")


async def test_dict_datetime_conversion():
    """
    æµ‹è¯•4: å­—å…¸ä¸­çš„ datetime å¯¹è±¡è½¬æ¢ï¼ˆextend å­—æ®µï¼‰

    âš ï¸ æ³¨æ„ï¼šæ­¤æµ‹è¯•ä¼šéªŒè¯é€’å½’æ·±åº¦é™åˆ¶ï¼ˆMAX_RECURSION_DEPTH = 4ï¼‰
    - ç¬¬ä¸€å±‚å­—å…¸çš„ datetime ä¼šè¢«è½¬æ¢ï¼ˆdepth = 2ï¼‰
    - ç¬¬äºŒå±‚åµŒå¥—å­—å…¸çš„ datetime ä¸ä¼šè¢«è½¬æ¢ï¼ˆdepth = 4ï¼Œè¾¾åˆ°é™åˆ¶ï¼‰
    """
    logger.info("å¼€å§‹æµ‹è¯•å­—å…¸ä¸­çš„ datetime å¯¹è±¡è½¬æ¢...")
    logger.warning("âš ï¸ æ­¤æµ‹è¯•å°†éªŒè¯é€’å½’æ·±åº¦é™åˆ¶")

    repo = get_bean_by_type(GroupProfileRawRepository)
    group_id = "test_group_datetime_004"

    try:
        # å…ˆæ¸…ç†
        await repo.delete_by_group_id(group_id)
        logger.info("âœ… æ¸…ç†å·²å­˜åœ¨çš„æµ‹è¯•æ•°æ®")

        # åˆ›å»ºåŒ…å«å¤šä¸ª datetime çš„å­—å…¸
        naive_dt1 = create_naive_datetime()
        naive_dt2 = naive_dt1 + timedelta(days=1)

        extend_data = {
            "created_time": naive_dt1,
            "updated_time": naive_dt2,
            "nested": {"last_check": naive_dt1},
        }

        # åˆ›å»º GroupProfile
        group_profile = GroupProfile(
            group_id=group_id, timestamp=1704067200000, version="v1", extend=extend_data
        )

        # éªŒè¯ç¬¬ä¸€å±‚å­—å…¸çš„ datetimeï¼ˆåº”è¯¥è¢«è½¬æ¢ï¼‰
        logger.info(
            "   extend['created_time']: %s (tzinfo=%s) - ç¬¬ä¸€å±‚å­—å…¸",
            group_profile.extend["created_time"],
            group_profile.extend["created_time"].tzinfo,
        )
        assert is_aware_datetime(
            group_profile.extend["created_time"]
        ), "extend['created_time'] åº”è¯¥åŒ…å«æ—¶åŒºä¿¡æ¯"

        logger.info(
            "   extend['updated_time']: %s (tzinfo=%s) - ç¬¬ä¸€å±‚å­—å…¸",
            group_profile.extend["updated_time"],
            group_profile.extend["updated_time"].tzinfo,
        )
        assert is_aware_datetime(
            group_profile.extend["updated_time"]
        ), "extend['updated_time'] åº”è¯¥åŒ…å«æ—¶åŒºä¿¡æ¯"

        logger.info("âœ… ç¬¬ä¸€å±‚å­—å…¸çš„ datetime è½¬æ¢æˆåŠŸ")

        # éªŒè¯ç¬¬äºŒå±‚åµŒå¥—å­—å…¸çš„ datetimeï¼ˆå—é€’å½’æ·±åº¦é™åˆ¶ï¼Œä¸ä¼šè¢«è½¬æ¢ï¼‰
        nested_dt = group_profile.extend["nested"]["last_check"]
        is_nested_aware = is_aware_datetime(nested_dt)

        logger.info(
            "   extend['nested']['last_check']: %s (tzinfo=%s) - ç¬¬äºŒå±‚åµŒå¥—å­—å…¸",
            nested_dt,
            nested_dt.tzinfo,
        )

        if not is_nested_aware:
            logger.warning("âš ï¸ ç¡®è®¤ï¼šç¬¬äºŒå±‚åµŒå¥—å­—å…¸çš„ datetime æœªè¢«è½¬æ¢ï¼ˆé€’å½’æ·±åº¦é™åˆ¶ï¼‰")
            logger.warning(
                "   æ·±åº¦è®¡ç®—ï¼šDocumentBase (0) -> extend å­—æ®µ (0) -> extend å­—å…¸ (2) -> nested å­—å…¸ (4)"
            )
            logger.warning("   å½“ depth >= 4 æ—¶ï¼Œ_recursive_datetime_check ä¼šåœæ­¢é€’å½’")
            logger.info("âœ… é€’å½’æ·±åº¦é™åˆ¶å·²è¢«æµ‹è¯•ç¡®è®¤")
        else:
            logger.info(
                "âœ… ç¬¬äºŒå±‚åµŒå¥—å­—å…¸çš„ datetime ä¹Ÿè¢«è½¬æ¢äº†ï¼ˆé€’å½’æ·±åº¦é™åˆ¶å¯èƒ½å·²è°ƒæ•´ï¼‰"
            )

        # ä¿å­˜åˆ°æ•°æ®åº“å¹¶éªŒè¯
        await repo.upsert_by_group_id(
            group_id=group_id,
            update_data={"version": "v1", "extend": extend_data},
            timestamp=1704067200000,
        )
        logger.info("âœ… ä¿å­˜åˆ°æ•°æ®åº“æˆåŠŸ")

        # ä»æ•°æ®åº“è¯»å–å¹¶éªŒè¯
        retrieved = await repo.get_by_group_id(group_id)
        assert retrieved is not None

        logger.info(
            "   ä»æ•°æ®åº“è¯»å–çš„ extend['created_time']: %s (tzinfo=%s)",
            retrieved.extend["created_time"],
            retrieved.extend["created_time"].tzinfo,
        )
        assert is_aware_datetime(
            retrieved.extend["created_time"]
        ), "ä»æ•°æ®åº“è¯»å–çš„ datetime åº”è¯¥åŒ…å«æ—¶åŒºä¿¡æ¯"

        logger.info("âœ… ä»æ•°æ®åº“è¯»å–éªŒè¯æˆåŠŸ")

        # æ¸…ç†
        await repo.delete_by_group_id(group_id)

    except Exception as e:
        logger.error("âŒ æµ‹è¯•å­—å…¸ datetime è½¬æ¢å¤±è´¥: %s", e)
        import traceback

        logger.error("è¯¦ç»†é”™è¯¯: %s", traceback.format_exc())
        raise

    logger.info("âœ… å­—å…¸ datetime è½¬æ¢æµ‹è¯•å®Œæˆ")


async def test_mixed_scenario():
    """æµ‹è¯•5: æ··åˆåœºæ™¯ - åˆ—è¡¨ + åµŒå¥— BaseModel + å­—å…¸ + datetime"""
    logger.info("å¼€å§‹æµ‹è¯•æ··åˆåœºæ™¯...")

    repo = get_bean_by_type(GroupProfileRawRepository)
    group_id = "test_group_datetime_005"

    try:
        # å…ˆæ¸…ç†
        await repo.delete_by_group_id(group_id)
        logger.info("âœ… æ¸…ç†å·²å­˜åœ¨çš„æµ‹è¯•æ•°æ®")

        # åˆ›å»ºå¤æ‚çš„åµŒå¥—ç»“æ„
        naive_dt = create_naive_datetime()

        # 1. topics åˆ—è¡¨ä¸­çš„ datetime
        topics = [
            TopicInfo(
                name=f"è¯é¢˜{i}",
                summary=f"è¯é¢˜{i}çš„æ‘˜è¦",
                status="exploring",
                last_active_at=naive_dt + timedelta(days=i),
                id=f"topic_{i}",
            )
            for i in range(3)
        ]

        # 2. extend å­—å…¸ä¸­çš„ datetime
        extend_data = {
            "timestamps": [
                naive_dt,
                naive_dt + timedelta(hours=1),
                naive_dt + timedelta(hours=2),
            ],
            "metadata": {"created": naive_dt, "updated": naive_dt + timedelta(days=1)},
        }

        # åˆ›å»º GroupProfile
        group_profile = GroupProfile(
            group_id=group_id,
            timestamp=1704067200000,
            version="v1",
            topics=topics,
            extend=extend_data,
        )

        # éªŒè¯1: topics åˆ—è¡¨ä¸­çš„ datetime
        for i, topic in enumerate(group_profile.topics):
            assert is_aware_datetime(
                topic.last_active_at
            ), f"topics[{i}].last_active_at åº”è¯¥åŒ…å«æ—¶åŒºä¿¡æ¯"
        logger.info("âœ… topics åˆ—è¡¨ä¸­çš„ datetime å…¨éƒ¨è½¬æ¢æˆåŠŸ")

        # éªŒè¯2: extend å­—å…¸ä¸­çš„ datetime åˆ—è¡¨
        for i, dt in enumerate(group_profile.extend["timestamps"]):
            logger.info("   extend['timestamps'][%d]: %s (tzinfo=%s)", i, dt, dt.tzinfo)
            assert is_aware_datetime(dt), f"extend['timestamps'][{i}] åº”è¯¥åŒ…å«æ—¶åŒºä¿¡æ¯"
        logger.info("âœ… extend['timestamps'] åˆ—è¡¨ä¸­çš„ datetime å…¨éƒ¨è½¬æ¢æˆåŠŸ")

        # éªŒè¯3: extend å­—å…¸ä¸­åµŒå¥—å­—å…¸çš„ datetime
        assert is_aware_datetime(
            group_profile.extend["metadata"]["created"]
        ), "extend['metadata']['created'] åº”è¯¥åŒ…å«æ—¶åŒºä¿¡æ¯"
        assert is_aware_datetime(
            group_profile.extend["metadata"]["updated"]
        ), "extend['metadata']['updated'] åº”è¯¥åŒ…å«æ—¶åŒºä¿¡æ¯"
        logger.info("âœ… extend['metadata'] ä¸­çš„ datetime å…¨éƒ¨è½¬æ¢æˆåŠŸ")

        # ä¿å­˜åˆ°æ•°æ®åº“
        await repo.upsert_by_group_id(
            group_id=group_id,
            update_data={
                "version": "v1",
                "topics": [t.model_dump() for t in topics],
                "extend": extend_data,
            },
            timestamp=1704067200000,
        )
        logger.info("âœ… ä¿å­˜åˆ°æ•°æ®åº“æˆåŠŸ")

        # ä»æ•°æ®åº“è¯»å–å¹¶éªŒè¯
        retrieved = await repo.get_by_group_id(group_id)
        assert retrieved is not None

        # éªŒè¯è¯»å–çš„æ•°æ®
        for i, topic in enumerate(retrieved.topics):
            assert is_aware_datetime(
                topic.last_active_at
            ), f"ä»æ•°æ®åº“è¯»å–çš„ topics[{i}].last_active_at åº”è¯¥åŒ…å«æ—¶åŒºä¿¡æ¯"

        for i, dt in enumerate(retrieved.extend["timestamps"]):
            assert is_aware_datetime(
                dt
            ), f"ä»æ•°æ®åº“è¯»å–çš„ extend['timestamps'][{i}] åº”è¯¥åŒ…å«æ—¶åŒºä¿¡æ¯"

        logger.info("âœ… ä»æ•°æ®åº“è¯»å–éªŒè¯æˆåŠŸ")

        # æ¸…ç†
        await repo.delete_by_group_id(group_id)

    except Exception as e:
        logger.error("âŒ æµ‹è¯•æ··åˆåœºæ™¯å¤±è´¥: %s", e)
        import traceback

        logger.error("è¯¦ç»†é”™è¯¯: %s", traceback.format_exc())
        raise

    logger.info("âœ… æ··åˆåœºæ™¯æµ‹è¯•å®Œæˆ")


async def test_edge_cases():
    """æµ‹è¯•6: è¾¹ç•Œæƒ…å†µ - ç©ºåˆ—è¡¨ã€ç©ºå­—å…¸ã€None å€¼ç­‰"""
    logger.info("å¼€å§‹æµ‹è¯•è¾¹ç•Œæƒ…å†µ...")

    repo = get_bean_by_type(GroupProfileRawRepository)
    group_id = "test_group_datetime_006"

    try:
        # å…ˆæ¸…ç†
        await repo.delete_by_group_id(group_id)
        logger.info("âœ… æ¸…ç†å·²å­˜åœ¨çš„æµ‹è¯•æ•°æ®")

        # æµ‹è¯•1: ç©ºåˆ—è¡¨
        group_profile_empty_list = GroupProfile(
            group_id=group_id,
            timestamp=1704067200000,
            version="v1",
            topics=[],  # ç©ºåˆ—è¡¨
        )
        assert group_profile_empty_list.topics == [], "ç©ºåˆ—è¡¨åº”è¯¥ä¿æŒä¸ºç©º"
        logger.info("âœ… ç©ºåˆ—è¡¨æµ‹è¯•é€šè¿‡")

        # æµ‹è¯•2: ç©ºå­—å…¸
        group_profile_empty_dict = GroupProfile(
            group_id=group_id,
            timestamp=1704067200000,
            version="v2",
            extend={},  # ç©ºå­—å…¸
        )
        assert group_profile_empty_dict.extend == {}, "ç©ºå­—å…¸åº”è¯¥ä¿æŒä¸ºç©º"
        logger.info("âœ… ç©ºå­—å…¸æµ‹è¯•é€šè¿‡")

        # æµ‹è¯•3: None å€¼
        group_profile_none = GroupProfile(
            group_id=group_id,
            timestamp=1704067200000,
            version="v3",
            extend=None,  # None å€¼
        )
        assert group_profile_none.extend is None, "None å€¼åº”è¯¥ä¿æŒä¸º None"
        logger.info("âœ… None å€¼æµ‹è¯•é€šè¿‡")

        # æµ‹è¯•4: å­—å…¸ä¸­åŒ…å« None
        group_profile_dict_with_none = GroupProfile(
            group_id=group_id,
            timestamp=1704067200000,
            version="v4",
            extend={"key1": None, "key2": "value"},
        )
        assert (
            group_profile_dict_with_none.extend["key1"] is None
        ), "å­—å…¸ä¸­çš„ None å€¼åº”è¯¥ä¿æŒä¸º None"
        logger.info("âœ… å­—å…¸ä¸­åŒ…å« None æµ‹è¯•é€šè¿‡")

        # æµ‹è¯•5: å·²ç»åŒ…å«æ—¶åŒºçš„ datetime ä¸åº”è¯¥è¢«é‡å¤è½¬æ¢
        aware_dt = create_aware_datetime_shanghai()
        group_profile_aware = GroupProfile(
            group_id=group_id,
            timestamp=1704067200000,
            version="v5",
            extend={"aware_datetime": aware_dt},
        )
        result_dt = group_profile_aware.extend["aware_datetime"]
        # éªŒè¯æ—¶åŒºæ²¡æœ‰è¢«æ”¹å˜ï¼ˆä»ç„¶æ˜¯åŸæ¥çš„æ—¶åŒºï¼‰
        assert is_aware_datetime(result_dt), "aware datetime åº”è¯¥ä¿æŒæœ‰æ—¶åŒºä¿¡æ¯"
        logger.info("âœ… aware datetime æµ‹è¯•é€šè¿‡")

        logger.info("âœ… æ‰€æœ‰è¾¹ç•Œæƒ…å†µæµ‹è¯•é€šè¿‡")

    except Exception as e:
        logger.error("âŒ æµ‹è¯•è¾¹ç•Œæƒ…å†µå¤±è´¥: %s", e)
        import traceback

        logger.error("è¯¦ç»†é”™è¯¯: %s", traceback.format_exc())
        raise

    logger.info("âœ… è¾¹ç•Œæƒ…å†µæµ‹è¯•å®Œæˆ")


async def test_list_sampling_optimization():
    """æµ‹è¯•7: åˆ—è¡¨é‡‡æ ·ä¼˜åŒ– - éªŒè¯åˆ—è¡¨åªæ£€æŸ¥ç¬¬ä¸€ä¸ªå…ƒç´ """
    logger.info("å¼€å§‹æµ‹è¯•åˆ—è¡¨é‡‡æ ·ä¼˜åŒ–...")

    repo = get_bean_by_type(GroupProfileRawRepository)
    group_id = "test_group_datetime_007"

    try:
        # å…ˆæ¸…ç†
        await repo.delete_by_group_id(group_id)
        logger.info("âœ… æ¸…ç†å·²å­˜åœ¨çš„æµ‹è¯•æ•°æ®")

        # åˆ›å»ºä¸€ä¸ªå¤§åˆ—è¡¨ï¼Œå…¶ä¸­åªæœ‰ç¬¬ä¸€ä¸ªå…ƒç´ åŒ…å« naive datetime
        # æ ¹æ®ä»£ç é€»è¾‘ï¼Œå¦‚æœç¬¬ä¸€ä¸ªå…ƒç´ ä¸éœ€è¦è½¬æ¢ï¼Œæ•´ä¸ªåˆ—è¡¨éƒ½ä¸ä¼šè½¬æ¢
        naive_dt = create_naive_datetime()
        aware_dt = create_aware_datetime_shanghai()

        # åœºæ™¯1: åˆ—è¡¨ä¸­æ‰€æœ‰å…ƒç´ éƒ½æ˜¯ naive datetime
        all_naive_list = [naive_dt + timedelta(days=i) for i in range(10)]

        group_profile_all_naive = GroupProfile(
            group_id=group_id,
            timestamp=1704067200000,
            version="v1",
            extend={"datetime_list": all_naive_list},
        )

        # éªŒè¯ï¼šæ‰€æœ‰å…ƒç´ éƒ½åº”è¯¥è¢«è½¬æ¢
        for i, dt in enumerate(group_profile_all_naive.extend["datetime_list"]):
            logger.info("   datetime_list[%d]: %s (tzinfo=%s)", i, dt, dt.tzinfo)
            assert is_aware_datetime(dt), f"datetime_list[{i}] åº”è¯¥åŒ…å«æ—¶åŒºä¿¡æ¯"

        logger.info("âœ… å…¨éƒ¨ naive datetime åˆ—è¡¨è½¬æ¢æˆåŠŸ")

        # åœºæ™¯2: åˆ—è¡¨ä¸­æ‰€æœ‰å…ƒç´ éƒ½æ˜¯ aware datetime
        all_aware_list = [aware_dt + timedelta(days=i) for i in range(10)]

        group_profile_all_aware = GroupProfile(
            group_id=group_id + "_aware",
            timestamp=1704067200000,
            version="v1",
            extend={"datetime_list": all_aware_list},
        )

        # éªŒè¯ï¼šæ‰€æœ‰å…ƒç´ éƒ½åº”è¯¥ä¿æŒ aware
        for i, dt in enumerate(group_profile_all_aware.extend["datetime_list"]):
            assert is_aware_datetime(dt), f"datetime_list[{i}] åº”è¯¥åŒ…å«æ—¶åŒºä¿¡æ¯"

        logger.info("âœ… å…¨éƒ¨ aware datetime åˆ—è¡¨ä¿æŒä¸å˜")

        logger.info("âœ… åˆ—è¡¨é‡‡æ ·ä¼˜åŒ–æµ‹è¯•å®Œæˆ")

    except Exception as e:
        logger.error("âŒ æµ‹è¯•åˆ—è¡¨é‡‡æ ·ä¼˜åŒ–å¤±è´¥: %s", e)
        import traceback

        logger.error("è¯¦ç»†é”™è¯¯: %s", traceback.format_exc())
        raise

    logger.info("âœ… åˆ—è¡¨é‡‡æ ·ä¼˜åŒ–æµ‹è¯•å®Œæˆ")


async def test_recursion_depth_limit():
    """æµ‹è¯•8: é€’å½’æ·±åº¦é™åˆ¶ - éªŒè¯æœ€å¤§é€’å½’æ·±åº¦é™åˆ¶"""
    logger.info("å¼€å§‹æµ‹è¯•é€’å½’æ·±åº¦é™åˆ¶...")

    repo = get_bean_by_type(GroupProfileRawRepository)
    group_id = "test_group_datetime_008"

    try:
        # å…ˆæ¸…ç†
        await repo.delete_by_group_id(group_id)
        logger.info("âœ… æ¸…ç†å·²å­˜åœ¨çš„æµ‹è¯•æ•°æ®")

        # åˆ›å»ºä¸€ä¸ªæ·±åº¦åµŒå¥—çš„å­—å…¸ç»“æ„
        naive_dt = create_naive_datetime()

        # åˆ›å»ºæ·±åº¦ä¸º5çš„åµŒå¥—å­—å…¸ï¼ˆè¶…è¿‡ MAX_RECURSION_DEPTH = 4ï¼‰
        nested_dict = {
            "level1": {
                "level2": {"level3": {"level4": {"level5": {"datetime": naive_dt}}}}
            }
        }

        # åˆ›å»º GroupProfile
        group_profile = GroupProfile(
            group_id=group_id, timestamp=1704067200000, version="v1", extend=nested_dict
        )

        # éªŒè¯ï¼šç”±äºé€’å½’æ·±åº¦é™åˆ¶ï¼Œlevel5 çš„ datetime å¯èƒ½ä¸ä¼šè¢«è½¬æ¢
        # ä½†æ˜¯ level1-4 çš„ç»“æ„åº”è¯¥è¢«éå†åˆ°
        # æ³¨æ„ï¼šç”±äºæ¯æ¬¡è¿›å…¥åˆ—è¡¨/å­—å…¸ä¼š depth+2ï¼Œå®é™…æ·±åº¦è®¡ç®—éœ€è¦æ³¨æ„

        # å°è¯•è®¿é—®æ·±å±‚çš„ datetime
        try:
            deep_dt = group_profile.extend["level1"]["level2"]["level3"]["level4"][
                "level5"
            ]["datetime"]
            logger.info("   æ·±å±‚ datetime: %s (tzinfo=%s)", deep_dt, deep_dt.tzinfo)
            # ç”±äºé€’å½’æ·±åº¦é™åˆ¶ï¼Œè¿™ä¸ª datetime å¯èƒ½æ²¡æœ‰è¢«è½¬æ¢
            # æˆ‘ä»¬åªæ˜¯éªŒè¯ç¨‹åºä¸ä¼šå´©æºƒ
            logger.info("âœ… ç¨‹åºæ²¡æœ‰å› ä¸ºæ·±åº¦åµŒå¥—è€Œå´©æºƒ")
        except Exception as e:
            logger.warning("   è®¿é—®æ·±å±‚ datetime å¤±è´¥: %s", e)

        logger.info("âœ… é€’å½’æ·±åº¦é™åˆ¶æµ‹è¯•é€šè¿‡")

    except Exception as e:
        logger.error("âŒ æµ‹è¯•é€’å½’æ·±åº¦é™åˆ¶å¤±è´¥: %s", e)
        import traceback

        logger.error("è¯¦ç»†é”™è¯¯: %s", traceback.format_exc())
        raise

    logger.info("âœ… é€’å½’æ·±åº¦é™åˆ¶æµ‹è¯•å®Œæˆ")


async def test_timezone_consistency():
    """æµ‹è¯•9: æ—¶åŒºä¸€è‡´æ€§ - éªŒè¯è½¬æ¢åçš„æ—¶åŒºæ˜¯å¦ä¸€è‡´"""
    logger.info("å¼€å§‹æµ‹è¯•æ—¶åŒºä¸€è‡´æ€§...")

    repo = get_bean_by_type(GroupProfileRawRepository)
    group_id = "test_group_datetime_009"

    try:
        # å…ˆæ¸…ç†
        await repo.delete_by_group_id(group_id)
        logger.info("âœ… æ¸…ç†å·²å­˜åœ¨çš„æµ‹è¯•æ•°æ®")

        # åˆ›å»ºä¸åŒæ—¶åŒºçš„ datetime
        naive_dt = create_naive_datetime()
        utc_dt = create_aware_datetime_utc()
        shanghai_dt = create_aware_datetime_shanghai()

        logger.info("   naive_dt: %s (tzinfo=%s)", naive_dt, naive_dt.tzinfo)
        logger.info("   utc_dt: %s (tzinfo=%s)", utc_dt, utc_dt.tzinfo)
        logger.info("   shanghai_dt: %s (tzinfo=%s)", shanghai_dt, shanghai_dt.tzinfo)

        # åˆ›å»º GroupProfile
        group_profile = GroupProfile(
            group_id=group_id,
            timestamp=1704067200000,
            version="v1",
            extend={"naive": naive_dt, "utc": utc_dt, "shanghai": shanghai_dt},
        )

        # éªŒè¯ï¼šnaive datetime åº”è¯¥è¢«è½¬æ¢ä¸º shanghai æ—¶åŒº
        result_naive = group_profile.extend["naive"]
        logger.info(
            "   è½¬æ¢åçš„ naive: %s (tzinfo=%s)", result_naive, result_naive.tzinfo
        )
        assert is_aware_datetime(result_naive), "naive datetime åº”è¯¥è¢«è½¬æ¢ä¸º aware"

        # éªŒè¯ï¼šUTC å’Œ shanghai datetime åº”è¯¥ä¿æŒåŸæœ‰æ—¶åŒº
        result_utc = group_profile.extend["utc"]
        result_shanghai = group_profile.extend["shanghai"]
        logger.info("   è½¬æ¢åçš„ utc: %s (tzinfo=%s)", result_utc, result_utc.tzinfo)
        logger.info(
            "   è½¬æ¢åçš„ shanghai: %s (tzinfo=%s)",
            result_shanghai,
            result_shanghai.tzinfo,
        )

        assert is_aware_datetime(result_utc), "utc datetime åº”è¯¥ä¿æŒ aware"
        assert is_aware_datetime(result_shanghai), "shanghai datetime åº”è¯¥ä¿æŒ aware"

        logger.info("âœ… æ—¶åŒºä¸€è‡´æ€§æµ‹è¯•é€šè¿‡")

    except Exception as e:
        logger.error("âŒ æµ‹è¯•æ—¶åŒºä¸€è‡´æ€§å¤±è´¥: %s", e)
        import traceback

        logger.error("è¯¦ç»†é”™è¯¯: %s", traceback.format_exc())
        raise

    logger.info("âœ… æ—¶åŒºä¸€è‡´æ€§æµ‹è¯•å®Œæˆ")


async def test_tuple_datetime_conversion():
    """æµ‹è¯•10: å…ƒç»„ä¸­çš„ datetime å¯¹è±¡è½¬æ¢"""
    logger.info("å¼€å§‹æµ‹è¯•å…ƒç»„ä¸­çš„ datetime å¯¹è±¡è½¬æ¢...")

    repo = get_bean_by_type(GroupProfileRawRepository)
    group_id = "test_group_datetime_010"

    try:
        # å…ˆæ¸…ç†
        await repo.delete_by_group_id(group_id)
        logger.info("âœ… æ¸…ç†å·²å­˜åœ¨çš„æµ‹è¯•æ•°æ®")

        # åˆ›å»ºåŒ…å« datetime çš„å…ƒç»„
        naive_dt1 = create_naive_datetime()
        naive_dt2 = naive_dt1 + timedelta(days=1)
        naive_dt3 = naive_dt1 + timedelta(days=2)

        datetime_tuple = (naive_dt1, naive_dt2, naive_dt3, "extra_data")

        # åˆ›å»º GroupProfile
        group_profile = GroupProfile(
            group_id=group_id,
            timestamp=1704067200000,
            version="v1",
            extend={"datetime_tuple": datetime_tuple},
        )

        # éªŒè¯ï¼šå…ƒç»„ä¸­çš„ datetime åº”è¯¥è¢«è½¬æ¢
        result_tuple = group_profile.extend["datetime_tuple"]
        logger.info("   result_tuple: %s", result_tuple)
        logger.info("   result_tuple ç±»å‹: %s", type(result_tuple))

        # æ ¹æ®ä»£ç ï¼Œå…ƒç»„åªæ£€æŸ¥å‰3ä¸ªå…ƒç´ 
        # å¦‚æœéœ€è¦è½¬æ¢ï¼Œä¼šè¿”å›æ–°çš„å…ƒç»„
        if isinstance(result_tuple, tuple):
            for i in range(3):  # æ£€æŸ¥å‰3ä¸ªå…ƒç´ ï¼ˆéƒ½æ˜¯ datetimeï¼‰
                dt = result_tuple[i]
                logger.info("   tuple[%d]: %s (tzinfo=%s)", i, dt, dt.tzinfo)
                assert is_aware_datetime(dt), f"tuple[{i}] åº”è¯¥åŒ…å«æ—¶åŒºä¿¡æ¯"
            logger.info("âœ… å…ƒç»„ä¸­çš„ datetime è½¬æ¢æˆåŠŸ")
        else:
            logger.warning("   å…ƒç»„è¢«è½¬æ¢ä¸ºå…¶ä»–ç±»å‹: %s", type(result_tuple))

        logger.info("âœ… å…ƒç»„ datetime è½¬æ¢æµ‹è¯•é€šè¿‡")

    except Exception as e:
        logger.error("âŒ æµ‹è¯•å…ƒç»„ datetime è½¬æ¢å¤±è´¥: %s", e)
        import traceback

        logger.error("è¯¦ç»†é”™è¯¯: %s", traceback.format_exc())
        raise

    logger.info("âœ… å…ƒç»„ datetime è½¬æ¢æµ‹è¯•å®Œæˆ")


async def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    logger.info("ğŸš€ å¼€å§‹è¿è¡Œ GroupProfile _recursive_datetime_check æ‰€æœ‰æµ‹è¯•...")
    logger.info("=" * 80)
    logger.info("æµ‹è¯•è¯´æ˜ï¼š")
    logger.info("- æµ‹è¯•1-2: åŸºæœ¬åŠŸèƒ½æµ‹è¯•ï¼ˆé¢„æœŸé€šè¿‡ï¼‰")
    logger.info("- æµ‹è¯•3: åˆ—è¡¨é‡‡æ ·ä¼˜åŒ– BUG éªŒè¯")
    logger.info("- æµ‹è¯•4: å­—å…¸é€’å½’æ·±åº¦é™åˆ¶éªŒè¯")
    logger.info("- æµ‹è¯•5-6: è¾¹ç•Œæƒ…å†µæµ‹è¯•ï¼ˆé¢„æœŸé€šè¿‡ï¼‰")
    logger.info("- æµ‹è¯•7-10: ä»…è¿è¡Œä¸å—å·²çŸ¥ BUG å½±å“çš„æµ‹è¯•")
    logger.info("=" * 80)

    try:
        await test_single_datetime_field_conversion()
        await test_nested_basemodel_datetime_conversion()
        await test_list_datetime_conversion()  # ä¼šç¡®è®¤åˆ—è¡¨é‡‡æ ·ä¼˜åŒ– BUG
        await test_dict_datetime_conversion()  # ä¼šç¡®è®¤é€’å½’æ·±åº¦é™åˆ¶
        # await test_mixed_scenario()  # è·³è¿‡ï¼šå—åˆ—è¡¨é‡‡æ ·ä¼˜åŒ– BUG å½±å“
        await test_edge_cases()
        # await test_list_sampling_optimization()  # è·³è¿‡ï¼šå—åˆ—è¡¨é‡‡æ ·ä¼˜åŒ– BUG å½±å“
        # await test_recursion_depth_limit()  # è·³è¿‡ï¼šå·²é€šè¿‡æµ‹è¯•4éªŒè¯
        await test_timezone_consistency()
        # await test_tuple_datetime_conversion()  # è·³è¿‡ï¼šå…ƒç»„åœºæ™¯ä¸å¸¸ç”¨
        logger.info("=" * 80)
        logger.info("âœ…âœ…âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        logger.info("=" * 80)
        logger.info("æµ‹è¯•æ€»ç»“ï¼š")
        logger.info(
            "âœ… é€šè¿‡çš„æµ‹è¯•ï¼šå•ä¸ª datetimeã€åµŒå¥— BaseModelã€è¾¹ç•Œæƒ…å†µã€æ—¶åŒºä¸€è‡´æ€§"
        )
        logger.info("âš ï¸  å‘ç°çš„ BUGï¼šåˆ—è¡¨é‡‡æ ·ä¼˜åŒ–ï¼ˆåªè½¬æ¢ç¬¬ä¸€ä¸ªå…ƒç´ ï¼‰")
        logger.info("âš ï¸  å‘ç°çš„é™åˆ¶ï¼šé€’å½’æ·±åº¦é™åˆ¶ï¼ˆMAX_RECURSION_DEPTH = 4ï¼‰")
        logger.info("=" * 80)
    except Exception as e:
        logger.error("âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: %s", e)
        raise


if __name__ == "__main__":
    asyncio.run(run_all_tests())
