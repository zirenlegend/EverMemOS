# Memsys ç³»ç»Ÿç¨³å®šæ€§åˆ†æä¸æµ‹è¯•ç”¨ä¾‹è®¾è®¡

## ğŸ“Š ç³»ç»Ÿæ¶æ„ç¨³å®šæ€§åˆ†æ

### ğŸ—ï¸ ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ

Memsys æ˜¯ä¸€ä¸ªåŸºäºå…­è¾¹å½¢æ¶æ„çš„æ™ºèƒ½è®°å¿†ç³»ç»Ÿï¼ŒåŒ…å«ä»¥ä¸‹æ ¸å¿ƒå±‚æ¬¡ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API Layer (FastAPI)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 Core Infrastructure                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   DI Container â”‚ â”‚  Middleware â”‚ â”‚  Lifespan   â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                Business Logic Layer                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Decision   â”‚ â”‚  Retrieval  â”‚ â”‚   Storage   â”‚          â”‚
â”‚  â”‚   Layer     â”‚ â”‚   Layer     â”‚ â”‚   Layer     â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              Infrastructure Layer                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ PostgreSQL  â”‚ â”‚  MongoDB    â”‚ â”‚   Redis     â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš¨ å…³é”®ç¨³å®šæ€§é£é™©ç‚¹åˆ†æ

### 1. æ•°æ®åº“è¿æ¥ä¸è¿æ¥æ± ç®¡ç†

#### ğŸ”´ é«˜é£é™©ç‚¹
- **è¿æ¥æ± è€—å°½**: é»˜è®¤é…ç½® `pool_size=40, max_overflow=25`ï¼Œåœ¨é«˜å¹¶å‘ä¸‹å¯èƒ½ä¸è¶³
- **è¿æ¥æ³„æ¼**: å¼‚æ­¥æ“ä½œä¸­æœªæ­£ç¡®é‡Šæ”¾æ•°æ®åº“è¿æ¥
- **è¿æ¥è¶…æ—¶**: é•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡å¯èƒ½å¯¼è‡´è¿æ¥è¶…æ—¶
- **æ•°æ®åº“æ•…éšœ**: PostgreSQL/MongoDB æœåŠ¡ä¸å¯ç”¨æ—¶çš„é™çº§å¤„ç†

#### ğŸ“‹ é£é™©åœºæ™¯
```python
# é£é™©åœºæ™¯1: è¿æ¥æ± è€—å°½
async def concurrent_database_operations():
    tasks = []
    for i in range(100):  # è¶…è¿‡è¿æ¥æ± å¤§å°
        task = asyncio.create_task(heavy_db_operation())
        tasks.append(task)
    await asyncio.gather(*tasks)  # å¯èƒ½å¯¼è‡´è¿æ¥æ± è€—å°½

# é£é™©åœºæ™¯2: è¿æ¥æ³„æ¼
async def risky_db_operation():
    session = get_session()
    try:
        result = await session.execute(complex_query)
        # å¦‚æœè¿™é‡ŒæŠ›å‡ºå¼‚å¸¸ï¼Œè¿æ¥å¯èƒ½ä¸ä¼šè¢«æ­£ç¡®é‡Šæ”¾
        return result
    except Exception as e:
        # è¿æ¥æ³„æ¼é£é™©
        raise e
```

### 2. å¼‚æ­¥å¹¶å‘æ§åˆ¶

#### ğŸ”´ é«˜é£é™©ç‚¹
- **å¹¶å‘é™åˆ¶ä¸è¶³**: å¤§é‡å¹¶å‘è¯·æ±‚å¯èƒ½å¯¼è‡´ç³»ç»Ÿèµ„æºè€—å°½
- **æ­»é”é£é™©**: å¼‚æ­¥æ“ä½œä¸­çš„èµ„æºç«äº‰
- **å†…å­˜æ³„æ¼**: é•¿æ—¶é—´è¿è¡Œçš„å¼‚æ­¥ä»»åŠ¡ç§¯ç´¯å†…å­˜
- **ä»»åŠ¡å–æ¶ˆå¤„ç†**: å¼‚æ­¥ä»»åŠ¡è¢«å–æ¶ˆæ—¶çš„æ¸…ç†å·¥ä½œ

#### ğŸ“‹ é£é™©åœºæ™¯
```python
# é£é™©åœºæ™¯1: æ— é™åˆ¶å¹¶å‘
async def unlimited_concurrent_requests():
    tasks = []
    for i in range(1000):  # æ— é™åˆ¶å¹¶å‘
        task = asyncio.create_task(process_request())
        tasks.append(task)
    await asyncio.gather(*tasks)  # å¯èƒ½å¯¼è‡´ç³»ç»Ÿå´©æºƒ

# é£é™©åœºæ™¯2: å¼‚æ­¥ä»»åŠ¡å–æ¶ˆå¤„ç†ä¸å½“
async def risky_async_task():
    try:
        await long_running_operation()
    except asyncio.CancelledError:
        # æ²¡æœ‰æ¸…ç†èµ„æº
        raise
```

### 3. å¤–éƒ¨æœåŠ¡ä¾èµ–

#### ğŸ”´ é«˜é£é™©ç‚¹
- **LLM API é™æµ**: OpenAI/Gemini API è°ƒç”¨é¢‘ç‡é™åˆ¶
- **ç½‘ç»œè¶…æ—¶**: å¤–éƒ¨æœåŠ¡å“åº”è¶…æ—¶
- **æœåŠ¡é™çº§**: å¤–éƒ¨æœåŠ¡ä¸å¯ç”¨æ—¶çš„é™çº§ç­–ç•¥
- **API å¯†é’¥ç®¡ç†**: å¯†é’¥è¿‡æœŸæˆ–æ— æ•ˆçš„å¤„ç†

#### ğŸ“‹ é£é™©åœºæ™¯
```python
# é£é™©åœºæ™¯1: LLM API é™æµ
async def llm_api_risk():
    for i in range(100):
        try:
            response = await llm_provider.generate(prompt)
        except RateLimitError:
            # æ²¡æœ‰é‡è¯•æœºåˆ¶æˆ–é™çº§ç­–ç•¥
            raise

# é£é™©åœºæ™¯2: ç½‘ç»œè¶…æ—¶
async def network_timeout_risk():
    try:
        response = await external_api_call()
    except asyncio.TimeoutError:
        # æ²¡æœ‰è¶…æ—¶é‡è¯•æœºåˆ¶
        raise
```

### 4. å†…å­˜ç®¡ç†

#### ğŸ”´ é«˜é£é™©ç‚¹
- **å¤§æ–‡ä»¶å¤„ç†**: å¤„ç†å¤§å‹æ–‡æ¡£æ—¶çš„å†…å­˜å ç”¨
- **å‘é‡ç´¢å¼•**: FAISS ç´¢å¼•çš„å†…å­˜ç®¡ç†
- **ç¼“å­˜ç®¡ç†**: Redis ç¼“å­˜çš„å†…å­˜ä½¿ç”¨
- **åƒåœ¾å›æ”¶**: Python åƒåœ¾å›æ”¶ä¸åŠæ—¶

#### ğŸ“‹ é£é™©åœºæ™¯
```python
# é£é™©åœºæ™¯1: å¤§æ–‡ä»¶å†…å­˜å ç”¨
def process_large_file():
    with open('huge_file.json', 'r') as f:
        data = json.load(f)  # å¯èƒ½å ç”¨å¤§é‡å†…å­˜
        process_data(data)

# é£é™©åœºæ™¯2: å‘é‡ç´¢å¼•å†…å­˜æ³„æ¼
class VectorIndex:
    def __init__(self):
        self.index = faiss.IndexFlatL2(768)
        # æ²¡æœ‰å†…å­˜é™åˆ¶å’Œæ¸…ç†æœºåˆ¶
```

### 5. é…ç½®ç®¡ç†

#### ğŸ”´ é«˜é£é™©ç‚¹
- **ç¯å¢ƒå˜é‡ç¼ºå¤±**: å…³é”®é…ç½®é¡¹æœªè®¾ç½®
- **é…ç½®éªŒè¯**: é…ç½®é¡¹æ ¼å¼æˆ–å€¼ä¸æ­£ç¡®
- **åŠ¨æ€é…ç½®**: è¿è¡Œæ—¶é…ç½®å˜æ›´çš„å½±å“
- **æ•æ„Ÿä¿¡æ¯**: API å¯†é’¥ç­‰æ•æ„Ÿä¿¡æ¯çš„å®‰å…¨å­˜å‚¨

## ğŸ§ª ç¨³å®šæ€§æµ‹è¯•ç”¨ä¾‹è®¾è®¡

### 1. æ•°æ®åº“è¿æ¥æ± æµ‹è¯•

#### æµ‹è¯•ç”¨ä¾‹ 1.1: è¿æ¥æ± è€—å°½æµ‹è¯•
```python
import pytest
import asyncio
from unittest.mock import patch
from component.database_session_provider import DatabaseSessionProvider

class TestDatabaseConnectionPool:
    
    @pytest.mark.asyncio
    async def test_connection_pool_exhaustion(self):
        """æµ‹è¯•è¿æ¥æ± è€—å°½åœºæ™¯"""
        provider = DatabaseSessionProvider()
        
        # åˆ›å»ºè¶…è¿‡è¿æ¥æ± å¤§å°çš„å¹¶å‘ä»»åŠ¡
        async def db_operation():
            async with provider.get_async_session() as session:
                await session.execute("SELECT 1")
                await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿé•¿æ—¶é—´æ“ä½œ
        
        # åˆ›å»ºå¤§é‡å¹¶å‘ä»»åŠ¡
        tasks = [asyncio.create_task(db_operation()) for _ in range(100)]
        
        # éªŒè¯ç³»ç»Ÿè¡Œä¸º
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¿æ¥æ± ç›¸å…³çš„å¼‚å¸¸
        connection_errors = [r for r in results if isinstance(r, Exception)]
        assert len(connection_errors) == 0, f"è¿æ¥æ± è€—å°½å¯¼è‡´å¼‚å¸¸: {connection_errors}"
    
    @pytest.mark.asyncio
    async def test_connection_leak_detection(self):
        """æµ‹è¯•è¿æ¥æ³„æ¼æ£€æµ‹"""
        provider = DatabaseSessionProvider()
        initial_connections = provider.async_engine.pool.size()
        
        # æ¨¡æ‹Ÿè¿æ¥æ³„æ¼åœºæ™¯
        async def leaky_operation():
            session = provider.create_session()
            # æ•…æ„ä¸å…³é—­session
            await session.execute("SELECT 1")
            # ä¸è°ƒç”¨ session.close()
        
        await leaky_operation()
        
        # æ£€æŸ¥è¿æ¥æ± çŠ¶æ€
        current_connections = provider.async_engine.pool.size()
        assert current_connections <= initial_connections + 5, "æ£€æµ‹åˆ°è¿æ¥æ³„æ¼"
```

#### æµ‹è¯•ç”¨ä¾‹ 1.2: æ•°æ®åº“æ•…éšœæ¢å¤æµ‹è¯•
```python
@pytest.mark.asyncio
async def test_database_failure_recovery(self):
    """æµ‹è¯•æ•°æ®åº“æ•…éšœæ¢å¤"""
    provider = DatabaseSessionProvider()
    
    # æ¨¡æ‹Ÿæ•°æ®åº“è¿æ¥å¤±è´¥
    with patch.object(provider.async_engine, 'execute') as mock_execute:
        mock_execute.side_effect = Exception("Database connection failed")
        
        # æµ‹è¯•é‡è¯•æœºåˆ¶
        retry_count = 0
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                async with provider.get_async_session() as session:
                    await session.execute("SELECT 1")
                break
            except Exception as e:
                retry_count += 1
                if retry_count >= max_retries:
                    # éªŒè¯é™çº§ç­–ç•¥
                    assert True, "åº”è¯¥è§¦å‘é™çº§ç­–ç•¥"
```

### 2. å¼‚æ­¥å¹¶å‘æ§åˆ¶æµ‹è¯•

#### æµ‹è¯•ç”¨ä¾‹ 2.1: å¹¶å‘é™åˆ¶æµ‹è¯•
```python
import asyncio
from unittest.mock import patch

class TestConcurrencyControl:
    
    @pytest.mark.asyncio
    async def test_semaphore_limitation(self):
        """æµ‹è¯•ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°"""
        max_concurrent = 5
        semaphore = asyncio.Semaphore(max_concurrent)
        concurrent_count = 0
        max_observed = 0
        
        async def limited_operation():
            nonlocal concurrent_count, max_observed
            async with semaphore:
                concurrent_count += 1
                max_observed = max(max_observed, concurrent_count)
                await asyncio.sleep(0.1)
                concurrent_count -= 1
        
        # åˆ›å»ºå¤§é‡ä»»åŠ¡
        tasks = [asyncio.create_task(limited_operation()) for _ in range(50)]
        await asyncio.gather(*tasks)
        
        # éªŒè¯å¹¶å‘æ•°ä¸è¶…è¿‡é™åˆ¶
        assert max_observed <= max_concurrent, f"å¹¶å‘æ•°è¶…è¿‡é™åˆ¶: {max_observed}"
    
    @pytest.mark.asyncio
    async def test_task_cancellation_cleanup(self):
        """æµ‹è¯•ä»»åŠ¡å–æ¶ˆæ—¶çš„èµ„æºæ¸…ç†"""
        cleanup_called = False
        
        async def cancellable_task():
            nonlocal cleanup_called
            try:
                await asyncio.sleep(10)  # é•¿æ—¶é—´è¿è¡Œ
            except asyncio.CancelledError:
                # æ¸…ç†èµ„æº
                cleanup_called = True
                raise
        
        task = asyncio.create_task(cancellable_task())
        await asyncio.sleep(0.1)
        task.cancel()
        
        try:
            await task
        except asyncio.CancelledError:
            pass
        
        assert cleanup_called, "ä»»åŠ¡å–æ¶ˆæ—¶æœªæ‰§è¡Œæ¸…ç†æ“ä½œ"
```

### 3. å¤–éƒ¨æœåŠ¡ä¾èµ–æµ‹è¯•

#### æµ‹è¯•ç”¨ä¾‹ 3.1: LLM API é™æµå¤„ç†æµ‹è¯•
```python
from unittest.mock import AsyncMock, patch
from openai import RateLimitError

class TestLLMServiceResilience:
    
    @pytest.mark.asyncio
    async def test_rate_limit_handling(self):
        """æµ‹è¯•APIé™æµå¤„ç†"""
        mock_llm = AsyncMock()
        mock_llm.generate.side_effect = RateLimitError("Rate limit exceeded")
        
        retry_count = 0
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                response = await mock_llm.generate("test prompt")
                break
            except RateLimitError:
                retry_count += 1
                if retry_count < max_retries:
                    await asyncio.sleep(2 ** retry_count)  # æŒ‡æ•°é€€é¿
                else:
                    # éªŒè¯é™çº§ç­–ç•¥
                    assert True, "åº”è¯¥è§¦å‘é™çº§ç­–ç•¥"
    
    @pytest.mark.asyncio
    async def test_network_timeout_handling(self):
        """æµ‹è¯•ç½‘ç»œè¶…æ—¶å¤„ç†"""
        mock_llm = AsyncMock()
        mock_llm.generate.side_effect = asyncio.TimeoutError("Request timeout")
        
        try:
            response = await asyncio.wait_for(
                mock_llm.generate("test prompt"),
                timeout=5.0
            )
        except asyncio.TimeoutError:
            # éªŒè¯è¶…æ—¶å¤„ç†ç­–ç•¥
            assert True, "åº”è¯¥æ­£ç¡®å¤„ç†è¶…æ—¶"
```

### 4. å†…å­˜ç®¡ç†æµ‹è¯•

#### æµ‹è¯•ç”¨ä¾‹ 4.1: å¤§æ–‡ä»¶å¤„ç†æµ‹è¯•
```python
import psutil
import os

class TestMemoryManagement:
    
    def test_large_file_processing(self):
        """æµ‹è¯•å¤§æ–‡ä»¶å¤„ç†çš„å†…å­˜ä½¿ç”¨"""
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss
        
        # æ¨¡æ‹Ÿå¤§æ–‡ä»¶å¤„ç†
        large_data = []
        for i in range(10000):
            large_data.append({"id": i, "content": "x" * 1000})
        
        peak_memory = process.memory_info().rss
        memory_increase = peak_memory - initial_memory
        
        # æ¸…ç†æ•°æ®
        del large_data
        
        # éªŒè¯å†…å­˜ä½¿ç”¨åˆç†
        assert memory_increase < 100 * 1024 * 1024, f"å†…å­˜ä½¿ç”¨è¿‡å¤š: {memory_increase / 1024 / 1024}MB"
    
    def test_vector_index_memory_usage(self):
        """æµ‹è¯•å‘é‡ç´¢å¼•å†…å­˜ä½¿ç”¨"""
        import faiss
        import numpy as np
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss
        
        # åˆ›å»ºå¤§å‹å‘é‡ç´¢å¼•
        dimension = 768
        index = faiss.IndexFlatL2(dimension)
        
        # æ·»åŠ å¤§é‡å‘é‡
        vectors = np.random.random((10000, dimension)).astype('float32')
        index.add(vectors)
        
        peak_memory = process.memory_info().rss
        memory_increase = peak_memory - initial_memory
        
        # éªŒè¯å†…å­˜ä½¿ç”¨åˆç†
        assert memory_increase < 200 * 1024 * 1024, f"å‘é‡ç´¢å¼•å†…å­˜ä½¿ç”¨è¿‡å¤š: {memory_increase / 1024 / 1024}MB"
```

### 5. é…ç½®ç®¡ç†æµ‹è¯•

#### æµ‹è¯•ç”¨ä¾‹ 5.1: é…ç½®éªŒè¯æµ‹è¯•
```python
import os
from unittest.mock import patch

class TestConfigurationManagement:
    
    def test_missing_environment_variables(self):
        """æµ‹è¯•ç¼ºå¤±ç¯å¢ƒå˜é‡çš„å¤„ç†"""
        required_vars = [
            'DATABASE_URL',
            'GEMINI_API_KEY',
            'REDIS_URL'
        ]
        
        for var in required_vars:
            with patch.dict(os.environ, {var: None}, clear=True):
                try:
                    from component.database_session_provider import DatabaseSessionProvider
                    provider = DatabaseSessionProvider()
                    assert False, f"åº”è¯¥æ£€æµ‹åˆ°ç¼ºå¤±çš„ç¯å¢ƒå˜é‡: {var}"
                except ValueError as e:
                    assert var in str(e), f"é”™è¯¯ä¿¡æ¯åº”è¯¥åŒ…å«å˜é‡å: {var}"
    
    def test_invalid_database_url(self):
        """æµ‹è¯•æ— æ•ˆæ•°æ®åº“URLçš„å¤„ç†"""
        invalid_urls = [
            "invalid://url",
            "postgresql://",
            "postgresql://user@host",
            ""
        ]
        
        for url in invalid_urls:
            with patch.dict(os.environ, {'DATABASE_URL': url}):
                try:
                    from component.database_session_provider import DatabaseSessionProvider
                    provider = DatabaseSessionProvider()
                    # è¿™é‡Œåº”è¯¥éªŒè¯URLæ ¼å¼
                    assert True, "åº”è¯¥éªŒè¯æ•°æ®åº“URLæ ¼å¼"
                except Exception as e:
                    assert "database" in str(e).lower(), f"åº”è¯¥æ£€æµ‹åˆ°æ•°æ®åº“é…ç½®é”™è¯¯: {e}"
```

### 6. ç³»ç»Ÿé›†æˆç¨³å®šæ€§æµ‹è¯•

#### æµ‹è¯•ç”¨ä¾‹ 6.1: ç«¯åˆ°ç«¯å‹åŠ›æµ‹è¯•
```python
import asyncio
import time
from concurrent.futures import ThreadPoolExecutor

class TestSystemStability:
    
    @pytest.mark.asyncio
    async def test_high_concurrency_stress(self):
        """é«˜å¹¶å‘å‹åŠ›æµ‹è¯•"""
        async def api_request():
            # æ¨¡æ‹ŸAPIè¯·æ±‚
            await asyncio.sleep(0.01)
            return {"status": "success"}
        
        # åˆ›å»ºå¤§é‡å¹¶å‘è¯·æ±‚
        start_time = time.time()
        tasks = [asyncio.create_task(api_request()) for _ in range(1000)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        end_time = time.time()
        
        # éªŒè¯ç»“æœ
        success_count = sum(1 for r in results if not isinstance(r, Exception))
        error_count = len(results) - success_count
        
        assert success_count >= 950, f"æˆåŠŸç‡è¿‡ä½: {success_count}/1000"
        assert end_time - start_time < 30, f"å“åº”æ—¶é—´è¿‡é•¿: {end_time - start_time}ç§’"
    
    @pytest.mark.asyncio
    async def test_memory_leak_detection(self):
        """å†…å­˜æ³„æ¼æ£€æµ‹æµ‹è¯•"""
        import gc
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss
        
        # æ‰§è¡Œå¤šè½®æ“ä½œ
        for round_num in range(10):
            # æ¨¡æ‹Ÿä¸€è½®å®Œæ•´çš„ä¸šåŠ¡æ“ä½œ
            await self.simulate_business_operation()
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨
            current_memory = process.memory_info().rss
            memory_increase = current_memory - initial_memory
            
            # æ¯è½®å†…å­˜å¢é•¿ä¸åº”è¶…è¿‡10MB
            assert memory_increase < 10 * 1024 * 1024, f"ç¬¬{round_num}è½®å†…å­˜æ³„æ¼: {memory_increase / 1024 / 1024}MB"
    
    async def simulate_business_operation(self):
        """æ¨¡æ‹Ÿä¸šåŠ¡æ“ä½œ"""
        # è¿™é‡Œæ¨¡æ‹Ÿå®Œæ•´çš„ä¸šåŠ¡æ“ä½œæµç¨‹
        await asyncio.sleep(0.1)
```

## ğŸ”§ ç¨³å®šæ€§æ”¹è¿›å»ºè®®

### 1. æ•°æ®åº“è¿æ¥æ± ä¼˜åŒ–

```python
# å»ºè®®çš„è¿æ¥æ± é…ç½®
DATABASE_CONFIG = {
    "pool_size": 50,           # å¢åŠ åŸºç¡€è¿æ¥æ± å¤§å°
    "max_overflow": 30,        # å¢åŠ æœ€å¤§æº¢å‡ºè¿æ¥
    "pool_recycle": 300,       # 5åˆ†é’Ÿå›æ”¶è¿æ¥
    "pool_pre_ping": True,     # è¿æ¥å‰pingæ£€æŸ¥
    "pool_timeout": 30,        # è·å–è¿æ¥è¶…æ—¶æ—¶é—´
    "max_retries": 3,          # æœ€å¤§é‡è¯•æ¬¡æ•°
    "retry_delay": 1.0,        # é‡è¯•å»¶è¿Ÿ
}
```

### 2. å¼‚æ­¥å¹¶å‘æ§åˆ¶ä¼˜åŒ–

```python
# å»ºè®®çš„å¹¶å‘æ§åˆ¶é…ç½®
CONCURRENCY_CONFIG = {
    "max_concurrent_requests": 100,    # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
    "max_concurrent_db_ops": 50,       # æœ€å¤§å¹¶å‘æ•°æ®åº“æ“ä½œ
    "max_concurrent_llm_calls": 10,    # æœ€å¤§å¹¶å‘LLMè°ƒç”¨
    "request_timeout": 30,             # è¯·æ±‚è¶…æ—¶æ—¶é—´
    "task_cleanup_timeout": 5,         # ä»»åŠ¡æ¸…ç†è¶…æ—¶æ—¶é—´
}
```

### 3. é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

```python
# å»ºè®®çš„é‡è¯•é…ç½®
RETRY_CONFIG = {
    "max_retries": 3,
    "base_delay": 1.0,
    "max_delay": 60.0,
    "exponential_base": 2,
    "jitter": True,
    "retryable_errors": [
        "ConnectionError",
        "TimeoutError",
        "RateLimitError"
    ]
}
```

### 4. ç›‘æ§å’Œå‘Šè­¦

```python
# å»ºè®®çš„ç›‘æ§æŒ‡æ ‡
MONITORING_METRICS = {
    "database_connections": "è¿æ¥æ± ä½¿ç”¨ç‡",
    "memory_usage": "å†…å­˜ä½¿ç”¨ç‡",
    "response_time": "å“åº”æ—¶é—´",
    "error_rate": "é”™è¯¯ç‡",
    "throughput": "ååé‡",
    "queue_length": "é˜Ÿåˆ—é•¿åº¦"
}
```

## ğŸ“Š æµ‹è¯•æ‰§è¡Œè®¡åˆ’

### é˜¶æ®µ1: å•å…ƒæµ‹è¯• (1-2å‘¨)
- æ•°æ®åº“è¿æ¥æ± æµ‹è¯•
- å¼‚æ­¥å¹¶å‘æ§åˆ¶æµ‹è¯•
- é…ç½®ç®¡ç†æµ‹è¯•

### é˜¶æ®µ2: é›†æˆæµ‹è¯• (2-3å‘¨)
- å¤–éƒ¨æœåŠ¡ä¾èµ–æµ‹è¯•
- å†…å­˜ç®¡ç†æµ‹è¯•
- é”™è¯¯å¤„ç†æµ‹è¯•

### é˜¶æ®µ3: ç³»ç»Ÿæµ‹è¯• (3-4å‘¨)
- ç«¯åˆ°ç«¯å‹åŠ›æµ‹è¯•
- é•¿æ—¶é—´è¿è¡Œæµ‹è¯•
- æ•…éšœæ¢å¤æµ‹è¯•

### é˜¶æ®µ4: ç”Ÿäº§ç¯å¢ƒæµ‹è¯• (1-2å‘¨)
- ç°åº¦å‘å¸ƒæµ‹è¯•
- ç›‘æ§å‘Šè­¦æµ‹è¯•
- æ€§èƒ½åŸºå‡†æµ‹è¯•

## ğŸ¯ æµ‹è¯•è¦†ç›–ç‡ç›®æ ‡

- **ä»£ç è¦†ç›–ç‡**: â‰¥ 80%
- **åˆ†æ”¯è¦†ç›–ç‡**: â‰¥ 75%
- **å…³é”®è·¯å¾„è¦†ç›–ç‡**: 100%
- **å¼‚å¸¸å¤„ç†è¦†ç›–ç‡**: â‰¥ 90%

## ğŸ“ˆ æ€§èƒ½åŸºå‡†

- **å“åº”æ—¶é—´**: P95 < 2ç§’
- **ååé‡**: â‰¥ 1000 QPS
- **å†…å­˜ä½¿ç”¨**: < 2GB
- **CPUä½¿ç”¨ç‡**: < 70%
- **é”™è¯¯ç‡**: < 0.1%

---

*æœ¬æ–‡æ¡£å°†æŒç»­æ›´æ–°ï¼Œåæ˜ ç³»ç»Ÿç¨³å®šæ€§çš„æ”¹è¿›å’Œæ–°çš„æµ‹è¯•å‘ç°ã€‚*
