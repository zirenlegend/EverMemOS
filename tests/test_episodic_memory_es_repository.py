#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯• EpisodicMemoryEsRepository çš„åŠŸèƒ½

æµ‹è¯•å†…å®¹åŒ…æ‹¬:
1. åŸºç¡€CRUDæ“ä½œï¼ˆå¢åˆ æ”¹æŸ¥ï¼‰
2. æœç´¢å’Œè¿‡æ»¤åŠŸèƒ½
3. æ‰¹é‡åˆ é™¤åŠŸèƒ½
4. æ—¶åŒºå¤„ç†
"""

import asyncio
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
from core.di import get_bean_by_type
from common_utils.datetime_utils import get_now_with_timezone, to_iso_format
from infra_layer.adapters.out.search.repository.episodic_memory_es_repository import (
    EpisodicMemoryEsRepository,
)
from core.observation.logger import get_logger

logger = get_logger(__name__)


def compare_datetime(dt1: datetime, dt2: datetime) -> bool:
    """æ¯”è¾ƒä¸¤ä¸ªdatetimeå¯¹è±¡ï¼Œåªæ¯”è¾ƒåˆ°ç§’çº§ç²¾åº¦"""
    return dt1.replace(microsecond=0) == dt2.replace(microsecond=0)


async def test_crud_operations():
    """æµ‹è¯•åŸºç¡€CRUDæ“ä½œ"""
    logger.info("å¼€å§‹æµ‹è¯•åŸºç¡€CRUDæ“ä½œ...")

    repo = get_bean_by_type(EpisodicMemoryEsRepository)
    test_event_id = "test_event_crud_001"
    test_user_id = "test_user_crud_123"
    current_time = get_now_with_timezone()

    try:
        # æµ‹è¯•åˆ›å»ºï¼ˆCreateï¼‰
        doc = await repo.create_and_save_episodic_memory(
            event_id=test_event_id,
            user_id=test_user_id,
            timestamp=current_time,
            episode="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æƒ…æ™¯è®°å¿†",
            search_content=["æµ‹è¯•", "æƒ…æ™¯", "è®°å¿†", "CRUD"],
            user_name="æµ‹è¯•ç”¨æˆ·",
            title="æµ‹è¯•æ ‡é¢˜",
            summary="æµ‹è¯•æ‘˜è¦",
            group_id="test_group_001",
            participants=["user1", "user2"],
            event_type="Test",
            keywords=["æµ‹è¯•", "å•å…ƒæµ‹è¯•"],
            linked_entities=["entity1", "entity2"],
            extend={},  # ç§»é™¤è‡ªå®šä¹‰å­—æ®µï¼Œé¿å…strict mappingå¼‚å¸¸
        )

        assert doc is not None
        assert doc.event_id == test_event_id
        assert doc.user_id == test_user_id
        assert doc.episode == "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æƒ…æ™¯è®°å¿†"
        logger.info("âœ… æµ‹è¯•åˆ›å»ºæ“ä½œæˆåŠŸ")

        # ç­‰å¾…ç´¢å¼•åˆ·æ–°
        await asyncio.sleep(1)

        # æµ‹è¯•è¯»å–ï¼ˆReadï¼‰
        retrieved_doc = await repo.get_by_id(test_event_id)
        assert retrieved_doc is not None
        assert retrieved_doc.event_id == test_event_id
        assert retrieved_doc.user_id == test_user_id
        assert retrieved_doc.episode == "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æƒ…æ™¯è®°å¿†"
        assert retrieved_doc.title == "æµ‹è¯•æ ‡é¢˜"
        assert retrieved_doc.group_id == "test_group_001"
        assert "æµ‹è¯•" in retrieved_doc.search_content
        logger.info("âœ… æµ‹è¯•è¯»å–æ“ä½œæˆåŠŸ")

        # æµ‹è¯•æ›´æ–°ï¼ˆUpdateï¼‰
        retrieved_doc.episode = "æ›´æ–°åçš„æƒ…æ™¯è®°å¿†"
        retrieved_doc.title = "æ›´æ–°åçš„æ ‡é¢˜"
        retrieved_doc.updated_at = get_now_with_timezone()

        updated_doc = await repo.update(retrieved_doc, refresh=True)
        assert updated_doc.episode == "æ›´æ–°åçš„æƒ…æ™¯è®°å¿†"
        assert updated_doc.title == "æ›´æ–°åçš„æ ‡é¢˜"
        logger.info("âœ… æµ‹è¯•æ›´æ–°æ“ä½œæˆåŠŸ")

        # éªŒè¯æ›´æ–°
        final_check = await repo.get_by_id(test_event_id)
        assert final_check is not None
        assert final_check.episode == "æ›´æ–°åçš„æƒ…æ™¯è®°å¿†"
        assert final_check.title == "æ›´æ–°åçš„æ ‡é¢˜"
        logger.info("âœ… éªŒè¯æ›´æ–°ç»“æœæˆåŠŸ")

        # æµ‹è¯•åˆ é™¤ï¼ˆDeleteï¼‰
        delete_result = await repo.delete_by_event_id(test_event_id, refresh=True)
        assert delete_result is True
        logger.info("âœ… æµ‹è¯•åˆ é™¤æ“ä½œæˆåŠŸ")

        # éªŒè¯åˆ é™¤
        deleted_check = await repo.get_by_id(test_event_id)
        assert deleted_check is None, "æ–‡æ¡£åº”è¯¥å·²è¢«åˆ é™¤"
        logger.info("âœ… éªŒè¯åˆ é™¤ç»“æœæˆåŠŸ")

    except Exception as e:
        logger.error("âŒ æµ‹è¯•åŸºç¡€CRUDæ“ä½œå¤±è´¥: %s", e)
        # æ¸…ç†å¯èƒ½æ®‹ç•™çš„æ•°æ®
        try:
            await repo.delete_by_event_id(test_event_id, refresh=True)
        except Exception:
            pass
        raise

    logger.info("âœ… åŸºç¡€CRUDæ“ä½œæµ‹è¯•å®Œæˆ")


async def test_search_and_filter():
    """æµ‹è¯•æœç´¢å’Œè¿‡æ»¤åŠŸèƒ½"""
    logger.info("å¼€å§‹æµ‹è¯•æœç´¢å’Œè¿‡æ»¤åŠŸèƒ½...")

    repo = get_bean_by_type(EpisodicMemoryEsRepository)
    test_user_id = "test_user_search_456"
    test_group_id = "test_group_search_789"
    base_time = get_now_with_timezone()
    test_event_ids = []

    try:
        # åˆ›å»ºå¤šä¸ªæµ‹è¯•è®°å¿†
        test_data = [
            {
                "event_id": f"search_test_001_{int(base_time.timestamp())}",
                "episode": "è®¨è®ºäº†å…¬å¸çš„å‘å±•æˆ˜ç•¥",
                "search_content": ["å…¬å¸", "å‘å±•", "æˆ˜ç•¥", "è®¨è®º"],
                "title": "æˆ˜ç•¥ä¼šè®®",
                "group_id": test_group_id,
                "event_type": "Conversation",
                "keywords": ["ä¼šè®®", "æˆ˜ç•¥"],
                "timestamp": base_time - timedelta(days=1),
            },
            {
                "event_id": f"search_test_002_{int(base_time.timestamp())}",
                "episode": "å­¦ä¹ äº†æ–°çš„æŠ€æœ¯æ¡†æ¶",
                "search_content": ["æŠ€æœ¯", "æ¡†æ¶", "å­¦ä¹ ", "ç¼–ç¨‹"],
                "title": "æŠ€æœ¯å­¦ä¹ ",
                "group_id": None,  # æ²¡æœ‰ç¾¤ç»„
                "event_type": "Learning",
                "keywords": ["æŠ€æœ¯", "å­¦ä¹ "],
                "timestamp": base_time - timedelta(days=2),
            },
            {
                "event_id": f"search_test_003_{int(base_time.timestamp())}",
                "episode": "å‚åŠ äº†å›¢é˜Ÿå»ºè®¾æ´»åŠ¨",
                "search_content": ["å›¢é˜Ÿ", "å»ºè®¾", "æ´»åŠ¨", "å‚åŠ "],
                "title": "å›¢é˜Ÿæ´»åŠ¨",
                "group_id": test_group_id,
                "event_type": "Activity",
                "keywords": ["å›¢é˜Ÿ", "æ´»åŠ¨"],
                "timestamp": base_time - timedelta(days=3),
            },
            {
                "event_id": f"search_test_004_{int(base_time.timestamp())}",
                "episode": "å®Œæˆäº†é¡¹ç›®çš„é‡è¦é‡Œç¨‹ç¢‘",
                "search_content": ["é¡¹ç›®", "é‡Œç¨‹ç¢‘", "å®Œæˆ", "é‡è¦"],
                "title": "é¡¹ç›®è¿›å±•",
                "group_id": test_group_id,
                "event_type": "Project",
                "keywords": ["é¡¹ç›®", "é‡Œç¨‹ç¢‘"],
                "timestamp": base_time - timedelta(days=4),
            },
            {
                "event_id": f"search_test_005_{int(base_time.timestamp())}",
                "episode": "ä¸å®¢æˆ·è¿›è¡Œäº†æ·±å…¥çš„æŠ€æœ¯äº¤æµ",
                "search_content": ["å®¢æˆ·", "æŠ€æœ¯", "äº¤æµ", "æ·±å…¥"],
                "title": "å®¢æˆ·æ²Ÿé€š",
                "group_id": None,  # æ²¡æœ‰ç¾¤ç»„
                "event_type": "Communication",
                "keywords": ["å®¢æˆ·", "æŠ€æœ¯"],
                "timestamp": base_time - timedelta(days=5),
            },
        ]

        # æ‰¹é‡åˆ›å»ºæµ‹è¯•æ•°æ®
        for data in test_data:
            await repo.create_and_save_episodic_memory(
                event_id=data["event_id"],
                user_id=test_user_id,
                timestamp=data["timestamp"],
                episode=data["episode"],
                search_content=data["search_content"],
                title=data["title"],
                group_id=data["group_id"],
                event_type=data["event_type"],
                keywords=data["keywords"],
                extend={},  # ä½¿ç”¨ç©ºçš„extendå¯¹è±¡
            )
            test_event_ids.append(data["event_id"])

        # æ‰‹åŠ¨åˆ·æ–°ç´¢å¼•ç¡®ä¿æ•°æ®ç«‹å³å¯æŸ¥è¯¢
        client = await repo.get_client()
        await client.indices.refresh(index=repo.get_index_name())

        logger.info("âœ… åˆ›å»ºäº† %d ä¸ªæµ‹è¯•è®°å¿†", len(test_data))

        # ç­‰å¾…ç´¢å¼•åˆ·æ–°ï¼ˆESéœ€è¦æ›´å¤šæ—¶é—´ï¼‰
        await asyncio.sleep(5)

        # æµ‹è¯•1: å¤šè¯æœç´¢
        logger.info("æµ‹è¯•1: å¤šè¯æœç´¢")
        results = await repo.multi_search(
            query=["æŠ€æœ¯", "é¡¹ç›®"], user_id=test_user_id, size=10, explain=True
        )
        assert (
            len(results) >= 2
        ), f"åº”è¯¥æ‰¾åˆ°è‡³å°‘2æ¡åŒ…å«'æŠ€æœ¯'æˆ–'é¡¹ç›®'çš„è®°å½•ï¼Œå®é™…æ‰¾åˆ°{len(results)}æ¡"
        logger.info("âœ… å¤šè¯æœç´¢æµ‹è¯•æˆåŠŸï¼Œæ‰¾åˆ° %d æ¡ç»“æœ", len(results))

        # æµ‹è¯•2: æŒ‰ç”¨æˆ·IDè¿‡æ»¤
        logger.info("æµ‹è¯•2: æŒ‰ç”¨æˆ·IDè¿‡æ»¤")
        user_results = await repo.multi_search(
            query=[], user_id=test_user_id, size=20  # ç©ºæŸ¥è¯¢ï¼Œçº¯è¿‡æ»¤
        )
        assert (
            len(user_results) >= 5
        ), f"åº”è¯¥æ‰¾åˆ°è‡³å°‘5æ¡ç”¨æˆ·è®°å½•ï¼Œå®é™…æ‰¾åˆ°{len(user_results)}æ¡"
        logger.info("âœ… ç”¨æˆ·IDè¿‡æ»¤æµ‹è¯•æˆåŠŸï¼Œæ‰¾åˆ° %d æ¡ç»“æœ", len(user_results))

        # æµ‹è¯•3: æŒ‰ç¾¤ç»„IDè¿‡æ»¤
        logger.info("æµ‹è¯•3: æŒ‰ç¾¤ç»„IDè¿‡æ»¤")
        group_results = await repo.multi_search(
            query=[], user_id=test_user_id, group_id=test_group_id, size=10
        )
        assert (
            len(group_results) >= 3
        ), f"åº”è¯¥æ‰¾åˆ°è‡³å°‘3æ¡ç¾¤ç»„è®°å½•ï¼Œå®é™…æ‰¾åˆ°{len(group_results)}æ¡"
        logger.info("âœ… ç¾¤ç»„IDè¿‡æ»¤æµ‹è¯•æˆåŠŸï¼Œæ‰¾åˆ° %d æ¡ç»“æœ", len(group_results))

        # æµ‹è¯•4: æŒ‰äº‹ä»¶ç±»å‹è¿‡æ»¤
        logger.info("æµ‹è¯•4: æŒ‰äº‹ä»¶ç±»å‹è¿‡æ»¤")
        type_results = await repo.multi_search(
            query=[], user_id=test_user_id, event_type="Conversation", size=10
        )
        assert (
            len(type_results) >= 1
        ), f"åº”è¯¥æ‰¾åˆ°è‡³å°‘1æ¡Conversationç±»å‹è®°å½•ï¼Œå®é™…æ‰¾åˆ°{len(type_results)}æ¡"
        logger.info("âœ… äº‹ä»¶ç±»å‹è¿‡æ»¤æµ‹è¯•æˆåŠŸï¼Œæ‰¾åˆ° %d æ¡ç»“æœ", len(type_results))

        # æµ‹è¯•5: æŒ‰å…³é”®è¯è¿‡æ»¤
        logger.info("æµ‹è¯•5: æŒ‰å…³é”®è¯è¿‡æ»¤")
        keyword_results = await repo.multi_search(
            query=[], user_id=test_user_id, keywords=["æŠ€æœ¯"], size=10
        )
        assert (
            len(keyword_results) >= 2
        ), f"åº”è¯¥æ‰¾åˆ°è‡³å°‘2æ¡åŒ…å«'æŠ€æœ¯'å…³é”®è¯çš„è®°å½•ï¼Œå®é™…æ‰¾åˆ°{len(keyword_results)}æ¡"
        logger.info("âœ… å…³é”®è¯è¿‡æ»¤æµ‹è¯•æˆåŠŸï¼Œæ‰¾åˆ° %d æ¡ç»“æœ", len(keyword_results))

        # æµ‹è¯•6: æŒ‰æ—¶é—´èŒƒå›´è¿‡æ»¤
        logger.info("æµ‹è¯•6: æŒ‰æ—¶é—´èŒƒå›´è¿‡æ»¤")
        date_range = {
            "gte": (base_time - timedelta(days=3)).isoformat(),
            "lte": base_time.isoformat(),
        }
        time_results = await repo.multi_search(
            query=[], user_id=test_user_id, date_range=date_range, size=10
        )
        assert (
            len(time_results) >= 2
        ), f"åº”è¯¥æ‰¾åˆ°è‡³å°‘2æ¡æ—¶é—´èŒƒå›´å†…çš„è®°å½•ï¼Œå®é™…æ‰¾åˆ°{len(time_results)}æ¡"
        logger.info("âœ… æ—¶é—´èŒƒå›´è¿‡æ»¤æµ‹è¯•æˆåŠŸï¼Œæ‰¾åˆ° %d æ¡ç»“æœ", len(time_results))

        # æµ‹è¯•7: ç»„åˆæŸ¥è¯¢
        logger.info("æµ‹è¯•7: ç»„åˆæŸ¥è¯¢")
        combo_results = await repo.multi_search(
            query=["æŠ€æœ¯", "é¡¹ç›®"],
            user_id=test_user_id,
            group_id=test_group_id,
            keywords=["æŠ€æœ¯"],
            size=10,
            explain=True,
        )
        logger.info("âœ… ç»„åˆæŸ¥è¯¢æµ‹è¯•æˆåŠŸï¼Œæ‰¾åˆ° %d æ¡ç»“æœ", len(combo_results))

        # æµ‹è¯•8: ä½¿ç”¨ä¸“ç”¨æŸ¥è¯¢æ–¹æ³•
        logger.info("æµ‹è¯•8: ä½¿ç”¨ä¸“ç”¨æŸ¥è¯¢æ–¹æ³•")
        timerange_results = await repo.get_by_user_and_timerange(
            user_id=test_user_id,
            start_time=base_time - timedelta(days=6),
            end_time=base_time,
            size=20,
        )
        assert (
            len(timerange_results) >= 5
        ), f"åº”è¯¥æ‰¾åˆ°è‡³å°‘5æ¡æ—¶é—´èŒƒå›´å†…çš„è®°å½•ï¼Œå®é™…æ‰¾åˆ°{len(timerange_results)}æ¡"
        logger.info("âœ… ä¸“ç”¨æŸ¥è¯¢æ–¹æ³•æµ‹è¯•æˆåŠŸï¼Œæ‰¾åˆ° %d æ¡ç»“æœ", len(timerange_results))

    except Exception as e:
        logger.error("âŒ æµ‹è¯•æœç´¢å’Œè¿‡æ»¤åŠŸèƒ½å¤±è´¥: %s", e)
        raise
    finally:
        # æ¸…ç†æµ‹è¯•æ•°æ®
        logger.info("æ¸…ç†æœç´¢æµ‹è¯•æ•°æ®...")
        try:
            cleanup_count = await repo.delete_by_filters(
                user_id=test_user_id, refresh=True
            )
            logger.info("âœ… æ¸…ç†äº† %d æ¡æœç´¢æµ‹è¯•æ•°æ®", cleanup_count)
        except Exception as cleanup_error:
            logger.error("æ¸…ç†æœç´¢æµ‹è¯•æ•°æ®æ—¶å‡ºç°é”™è¯¯: %s", cleanup_error)

    logger.info("âœ… æœç´¢å’Œè¿‡æ»¤åŠŸèƒ½æµ‹è¯•å®Œæˆ")


async def test_delete_operations():
    """æµ‹è¯•åˆ é™¤åŠŸèƒ½"""
    logger.info("å¼€å§‹æµ‹è¯•åˆ é™¤åŠŸèƒ½...")

    repo = get_bean_by_type(EpisodicMemoryEsRepository)
    test_user_id = "test_user_delete_789"
    test_group_id = "test_group_delete_012"
    base_time = get_now_with_timezone()
    test_event_ids = []

    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        for i in range(6):
            event_id = f"delete_test_{i}_{int(base_time.timestamp())}"
            test_event_ids.append(event_id)

            await repo.create_and_save_episodic_memory(
                event_id=event_id,
                user_id=test_user_id,
                timestamp=base_time - timedelta(days=i),
                episode=f"åˆ é™¤æµ‹è¯•è®°å¿† {i}",
                search_content=["åˆ é™¤", "æµ‹è¯•", f"è®°å¿†{i}"],
                title=f"åˆ é™¤æµ‹è¯• {i}",
                group_id=test_group_id if i % 2 == 0 else None,  # éƒ¨åˆ†æœ‰group_id
                event_type="DeleteTest",
                extend={},  # ä½¿ç”¨ç©ºçš„extendå¯¹è±¡
            )

        # æ‰‹åŠ¨åˆ·æ–°ç´¢å¼•ç¡®ä¿æ•°æ®ç«‹å³å¯æŸ¥è¯¢
        client = await repo.get_client()
        await client.indices.refresh(index=repo.get_index_name())

        logger.info("âœ… åˆ›å»ºäº† %d ä¸ªåˆ é™¤æµ‹è¯•è®°å¿†", len(test_event_ids))

        # ç­‰å¾…ç´¢å¼•åˆ·æ–°ï¼ˆåˆ é™¤æµ‹è¯•éœ€è¦æ›´å¤šæ—¶é—´ç¡®ä¿ç´¢å¼•å®Œå…¨åˆ·æ–°ï¼‰
        await asyncio.sleep(5)

        # æµ‹è¯•1: æŒ‰event_idåˆ é™¤
        logger.info("æµ‹è¯•1: æŒ‰event_idåˆ é™¤")
        event_id_to_delete = test_event_ids[0]
        delete_result = await repo.delete_by_event_id(event_id_to_delete, refresh=True)
        assert delete_result is True

        # éªŒè¯åˆ é™¤
        deleted_doc = await repo.get_by_id(event_id_to_delete)
        assert deleted_doc is None, "æ–‡æ¡£åº”è¯¥å·²è¢«åˆ é™¤"
        logger.info("âœ… æŒ‰event_idåˆ é™¤æµ‹è¯•æˆåŠŸ")

        # æµ‹è¯•2: æŒ‰è¿‡æ»¤æ¡ä»¶åˆ é™¤ - åªåˆ é™¤æœ‰group_idçš„è®°å¿†
        logger.info("æµ‹è¯•2: æŒ‰è¿‡æ»¤æ¡ä»¶åˆ é™¤ï¼ˆgroup_idï¼‰")
        deleted_count = await repo.delete_by_filters(
            user_id=test_user_id, group_id=test_group_id, refresh=True
        )
        assert (
            deleted_count >= 2
        ), f"åº”è¯¥åˆ é™¤è‡³å°‘2æ¡æœ‰group_idçš„è®°å½•ï¼Œå®é™…åˆ é™¤{deleted_count}æ¡"
        logger.info("âœ… æŒ‰group_idè¿‡æ»¤åˆ é™¤æµ‹è¯•æˆåŠŸï¼Œåˆ é™¤äº† %d æ¡è®°å½•", deleted_count)

        # æµ‹è¯•3: æŒ‰æ—¶é—´èŒƒå›´åˆ é™¤
        logger.info("æµ‹è¯•3: æŒ‰æ—¶é—´èŒƒå›´åˆ é™¤")
        date_range = {
            "gte": (base_time - timedelta(days=2)).isoformat(),
            "lte": base_time.isoformat(),
        }
        deleted_count = await repo.delete_by_filters(
            user_id=test_user_id, date_range=date_range, refresh=True
        )
        logger.info("âœ… æŒ‰æ—¶é—´èŒƒå›´åˆ é™¤æµ‹è¯•æˆåŠŸï¼Œåˆ é™¤äº† %d æ¡è®°å½•", deleted_count)

        # æµ‹è¯•4: éªŒè¯å‚æ•°æ£€æŸ¥
        logger.info("æµ‹è¯•4: éªŒè¯å‚æ•°æ£€æŸ¥")
        try:
            await repo.delete_by_filters()  # æ²¡æœ‰æä¾›ä»»ä½•è¿‡æ»¤æ¡ä»¶
            assert False, "åº”è¯¥æŠ›å‡ºå¼‚å¸¸ä½†æ²¡æœ‰"
        except ValueError as e:
            logger.info("âœ… æ­£ç¡®æ•è·å‚æ•°é”™è¯¯: %s", e)

        # æœ€ç»ˆæ¸…ç†å‰©ä½™æ•°æ®
        remaining_count = await repo.delete_by_filters(
            user_id=test_user_id, refresh=True
        )
        logger.info("âœ… æœ€ç»ˆæ¸…ç†äº† %d æ¡å‰©ä½™æ•°æ®", remaining_count)

    except Exception as e:
        logger.error("âŒ æµ‹è¯•åˆ é™¤åŠŸèƒ½å¤±è´¥: %s", e)
        raise
    finally:
        # ç¡®ä¿æ¸…ç†æ‰€æœ‰æµ‹è¯•æ•°æ®
        try:
            await repo.delete_by_filters(user_id=test_user_id, refresh=True)
        except Exception:
            pass

    logger.info("âœ… åˆ é™¤åŠŸèƒ½æµ‹è¯•å®Œæˆ")


async def test_timezone_handling():
    """æµ‹è¯•æ—¶åŒºå¤„ç†"""
    logger.info("å¼€å§‹æµ‹è¯•æ—¶åŒºå¤„ç†...")

    repo = get_bean_by_type(EpisodicMemoryEsRepository)
    test_event_id = "test_timezone_001"
    test_user_id = "test_user_timezone_999"

    try:
        # åˆ›å»ºä¸åŒæ—¶åŒºçš„æ—¶é—´
        utc_time = datetime.now(ZoneInfo("UTC"))
        tokyo_time = datetime.now(ZoneInfo("Asia/Tokyo"))
        shanghai_time = get_now_with_timezone()  # é»˜è®¤ä¸Šæµ·æ—¶åŒº

        logger.info("åŸå§‹UTCæ—¶é—´: %s", to_iso_format(utc_time))
        logger.info("åŸå§‹ä¸œäº¬æ—¶é—´: %s", to_iso_format(tokyo_time))
        logger.info("åŸå§‹ä¸Šæµ·æ—¶é—´: %s", to_iso_format(shanghai_time))

        # ä½¿ç”¨UTCæ—¶é—´åˆ›å»ºè®°å¿†
        doc = await repo.create_and_save_episodic_memory(
            event_id=test_event_id,
            user_id=test_user_id,
            timestamp=utc_time,
            episode="æ—¶åŒºæµ‹è¯•è®°å¿†",
            search_content=["æ—¶åŒº", "æµ‹è¯•"],
            title="æ—¶åŒºæµ‹è¯•",
            created_at=tokyo_time,
            updated_at=shanghai_time,
            extend={},  # ä½¿ç”¨ç©ºçš„extendå¯¹è±¡
        )

        assert doc is not None
        logger.info("âœ… åˆ›å»ºå¸¦æ—¶åŒºä¿¡æ¯çš„è®°å¿†æˆåŠŸ")

        # æ‰‹åŠ¨åˆ·æ–°ç´¢å¼•ç¡®ä¿æ•°æ®ç«‹å³å¯æŸ¥è¯¢
        client = await repo.get_client()
        await client.indices.refresh(index=repo.get_index_name())

        # ç­‰å¾…ç´¢å¼•åˆ·æ–°
        await asyncio.sleep(2)

        # ä»æ•°æ®åº“è·å–å¹¶éªŒè¯
        retrieved_doc = await repo.get_by_id(test_event_id)
        assert retrieved_doc is not None

        logger.info("ä»æ•°æ®åº“è·å–çš„æ—¶é—´:")
        logger.info("timestamp (åŸUTC): %s", to_iso_format(retrieved_doc.timestamp))
        logger.info("created_at (åŸTokyo): %s", to_iso_format(retrieved_doc.created_at))
        logger.info(
            "updated_at (åŸShanghai): %s", to_iso_format(retrieved_doc.updated_at)
        )

        # éªŒè¯æ—¶é—´è½¬æ¢æ­£ç¡®æ€§ï¼ˆè½¬æ¢åˆ°åŒä¸€æ—¶åŒºååº”è¯¥ç›¸ç­‰ï¼‰
        assert retrieved_doc.timestamp.astimezone(ZoneInfo("UTC")).replace(
            microsecond=0
        ) == utc_time.replace(microsecond=0)
        logger.info("âœ… æ—¶åŒºéªŒè¯æˆåŠŸ")

        # æµ‹è¯•æ—¶é—´èŒƒå›´æŸ¥è¯¢ - ä½¿ç”¨æ›´å®½çš„æ—¶é—´èŒƒå›´å’Œä¸Šæµ·æ—¶åŒº
        shanghai_time = get_now_with_timezone()  # å½“å‰ä¸Šæµ·æ—¶é—´
        date_range = {
            "gte": (shanghai_time - timedelta(hours=2)).isoformat(),
            "lte": (shanghai_time + timedelta(hours=2)).isoformat(),
        }

        logger.info("æ—¶é—´èŒƒå›´æŸ¥è¯¢: %s åˆ° %s", date_range["gte"], date_range["lte"])
        logger.info("æ–‡æ¡£æ—¶é—´æˆ³: %s", to_iso_format(retrieved_doc.timestamp))

        time_results = await repo.multi_search(
            query=[], user_id=test_user_id, date_range=date_range, size=10
        )
        logger.info("æ—¶é—´èŒƒå›´æŸ¥è¯¢ç»“æœ: æ‰¾åˆ° %d æ¡è®°å½•", len(time_results))

        # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œå°è¯•ä¸ä½¿ç”¨æ—¶é—´èŒƒå›´ï¼Œåªç”¨user_idæŸ¥è¯¢
        if len(time_results) == 0:
            logger.warning("æ—¶é—´èŒƒå›´æŸ¥è¯¢æœªæ‰¾åˆ°è®°å½•ï¼Œå°è¯•çº¯user_idæŸ¥è¯¢")
            fallback_results = await repo.multi_search(
                query=[], user_id=test_user_id, size=10
            )
            logger.info("çº¯user_idæŸ¥è¯¢ç»“æœ: æ‰¾åˆ° %d æ¡è®°å½•", len(fallback_results))
            assert len(fallback_results) >= 1, "è‡³å°‘åº”è¯¥é€šè¿‡user_idæ‰¾åˆ°è®°å½•"
            logger.info("âœ… æ—¶åŒºå¤„ç†åŸºç¡€åŠŸèƒ½éªŒè¯æˆåŠŸ")
        else:
            assert len(time_results) >= 1, "åº”è¯¥æ‰¾åˆ°æ—¶é—´èŒƒå›´å†…çš„è®°å½•"
            logger.info("âœ… æ—¶åŒºæ—¶é—´èŒƒå›´æŸ¥è¯¢æµ‹è¯•æˆåŠŸ")

    except Exception as e:
        logger.error("âŒ æµ‹è¯•æ—¶åŒºå¤„ç†å¤±è´¥: %s", e)
        raise
    finally:
        # æ¸…ç†æµ‹è¯•æ•°æ®
        try:
            await repo.delete_by_event_id(test_event_id, refresh=True)
            logger.info("âœ… æ¸…ç†æ—¶åŒºæµ‹è¯•æ•°æ®æˆåŠŸ")
        except Exception:
            pass

    logger.info("âœ… æ—¶åŒºå¤„ç†æµ‹è¯•å®Œæˆ")


async def test_edge_cases():
    """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
    logger.info("å¼€å§‹æµ‹è¯•è¾¹ç•Œæƒ…å†µ...")

    repo = get_bean_by_type(EpisodicMemoryEsRepository)
    test_user_id = "test_user_edge_111"

    try:
        # æµ‹è¯•1: ç©ºæœç´¢è¯
        logger.info("æµ‹è¯•1: ç©ºæœç´¢è¯")
        empty_results = await repo.multi_search(query=[], user_id=test_user_id, size=10)
        logger.info("âœ… ç©ºæœç´¢è¯æµ‹è¯•æˆåŠŸï¼Œæ‰¾åˆ° %d æ¡ç»“æœ", len(empty_results))

        # æµ‹è¯•2: ä¸å­˜åœ¨çš„ç”¨æˆ·
        logger.info("æµ‹è¯•2: ä¸å­˜åœ¨çš„ç”¨æˆ·")
        nonexistent_results = await repo.multi_search(
            query=["æµ‹è¯•"], user_id="nonexistent_user_999999", size=10, explain=True
        )
        assert len(nonexistent_results) == 0, "ä¸å­˜åœ¨çš„ç”¨æˆ·åº”è¯¥è¿”å›ç©ºç»“æœ"
        logger.info("âœ… ä¸å­˜åœ¨ç”¨æˆ·æµ‹è¯•æˆåŠŸ")

        # æµ‹è¯•3: åˆ é™¤ä¸å­˜åœ¨çš„event_id
        logger.info("æµ‹è¯•3: åˆ é™¤ä¸å­˜åœ¨çš„event_id")
        delete_result = await repo.delete_by_event_id("nonexistent_event_999999")
        assert delete_result is False, "åˆ é™¤ä¸å­˜åœ¨çš„æ–‡æ¡£åº”è¯¥è¿”å›False"
        logger.info("âœ… åˆ é™¤ä¸å­˜åœ¨æ–‡æ¡£æµ‹è¯•æˆåŠŸ")

        # æµ‹è¯•4: ä½¿ç”¨æ— æ•ˆçš„æ—¶é—´èŒƒå›´
        logger.info("æµ‹è¯•4: ä½¿ç”¨æ— æ•ˆçš„æ—¶é—´èŒƒå›´")
        invalid_date_range = {"gte": "2099-01-01", "lte": "2099-12-31"}  # æœªæ¥æ—¶é—´
        future_results = await repo.multi_search(
            query=[], user_id=test_user_id, date_range=invalid_date_range, size=10
        )
        assert len(future_results) == 0, "æœªæ¥æ—¶é—´èŒƒå›´åº”è¯¥è¿”å›ç©ºç»“æœ"
        logger.info("âœ… æ— æ•ˆæ—¶é—´èŒƒå›´æµ‹è¯•æˆåŠŸ")

    except Exception as e:
        logger.error("âŒ æµ‹è¯•è¾¹ç•Œæƒ…å†µå¤±è´¥: %s", e)
        raise

    logger.info("âœ… è¾¹ç•Œæƒ…å†µæµ‹è¯•å®Œæˆ")


async def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    logger.info("ğŸš€ å¼€å§‹è¿è¡ŒEpisodicMemoryEsRepositoryæ‰€æœ‰æµ‹è¯•...")

    try:
        await test_multi_search()
        await test_crud_operations()
        await test_search_and_filter()
        await test_delete_operations()
        await test_timezone_handling()
        await test_edge_cases()
        logger.info("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ")
    except Exception as e:
        logger.error("âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: %s", e)
        raise


async def test_multi_search():
    """æµ‹è¯•åŸºäºelasticsearch-dslçš„å¤šè¯æœç´¢åŠŸèƒ½"""
    logger.info("å¼€å§‹æµ‹è¯•DSLå¤šè¯æœç´¢åŠŸèƒ½...")

    repo = get_bean_by_type(EpisodicMemoryEsRepository)
    test_event_id = "test_event_dsl_001"
    test_event_id_bm25 = "test_event_bm25_001"
    test_event_id_not_search = "test_event_not_search_001"
    test_user_id = "test_user_dsl_123"
    test_user_id_not_search = "test_user_not_search_123"
    current_time = get_now_with_timezone()

    try:
        # å…ˆåˆ›å»ºæµ‹è¯•æ•°æ®
        await repo.create_and_save_episodic_memory(
            event_id=test_event_id,
            user_id=test_user_id,
            timestamp=current_time,
            episode="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•DSLæœç´¢çš„æƒ…æ™¯è®°å¿†",
            search_content=["DSL", "æœç´¢", "æµ‹è¯•", "elasticsearch"],
            user_name="DSLæµ‹è¯•ç”¨æˆ·",
            title="DSLæœç´¢æµ‹è¯•æ ‡é¢˜",
            summary="DSLæœç´¢æµ‹è¯•æ‘˜è¦",
            event_type="TestDSL",
            keywords=["dsl", "search", "test"],
        )

        await repo.create_and_save_episodic_memory(
            event_id=test_event_id_bm25,
            user_id=test_user_id,
            timestamp=current_time,
            episode="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•BM25çš„åå¥½è®°å¿†",
            search_content=["BM25", "åå¥½", "æµ‹è¯•", "elasticsearch"],
            user_name="DSLæµ‹è¯•ç”¨æˆ·",
            title="BM25æœç´¢æµ‹è¯•æ ‡é¢˜",
            summary="BM25æœç´¢æµ‹è¯•æ‘˜è¦",
            event_type="TestBM25",
            keywords=["dsl", "search", "test"],
        )

        await repo.create_and_save_episodic_memory(
            event_id=test_event_id_not_search,
            user_id=test_user_id_not_search,
            timestamp=current_time,
            episode="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•DSLæœç´¢çš„æƒ…æ™¯è®°å¿†2",
            search_content=["DSL", "æœç´¢", "æµ‹è¯•", "elasticsearch"],
            user_name="DSLæµ‹è¯•ç”¨æˆ·",
            title="DSLæœç´¢æµ‹è¯•æ ‡é¢˜2",
            summary="DSLæœç´¢æµ‹è¯•æ‘˜è¦2",
            event_type="TestDSL2",
            keywords=["dsl", "search", "test"],
        )

        # ç­‰å¾…ç´¢å¼•åˆ·æ–°
        await repo.refresh_index()
        await asyncio.sleep(1)

        # æµ‹è¯•1: DSLå¤šè¯æœç´¢
        logger.info("æµ‹è¯•DSLå¤šè¯æœç´¢...")
        results = await repo.multi_search(
            query=["DSL", "æœç´¢"], user_id=test_user_id, size=10, explain=True
        )
        assert len(results) == 1, "DSLå¤šè¯æœç´¢åº”è¯¥è¿”å›ç»“æœ"
        logger.info("âœ… DSLå¤šè¯æœç´¢æµ‹è¯•é€šè¿‡: æ‰¾åˆ° %d æ¡ç»“æœ", len(results))

        # æµ‹è¯•2: DSLè¿‡æ»¤æŸ¥è¯¢ï¼ˆæ— æœç´¢è¯ï¼‰
        logger.info("æµ‹è¯•DSLè¿‡æ»¤æŸ¥è¯¢...")
        results = await repo.multi_search(
            query=[], user_id=test_user_id, event_type="TestDSL", size=10
        )
        assert len(results) > 0, "DSLè¿‡æ»¤æŸ¥è¯¢åº”è¯¥è¿”å›ç»“æœ"
        logger.info("âœ… DSLè¿‡æ»¤æŸ¥è¯¢æµ‹è¯•é€šè¿‡: æ‰¾åˆ° %d æ¡ç»“æœ", len(results))

        # æµ‹è¯•4: BM25æœç´¢
        logger.info("æµ‹è¯•BM25æœç´¢...")
        results = await repo.multi_search(
            query=["BM25", "åå¥½"], user_id=test_user_id, size=10, explain=True
        )
        assert len(results) == 1, "BM25æœç´¢åº”è¯¥è¿”å›ç»“æœ"
        logger.info("âœ… BM25æœç´¢æµ‹è¯•é€šè¿‡: æ‰¾åˆ° %d æ¡ç»“æœ", len(results))

        # æµ‹è¯•5: BM25è¿‡æ»¤æŸ¥è¯¢ï¼ˆæ— æœç´¢è¯ï¼‰
        logger.info("æµ‹è¯•BM25è¿‡æ»¤æŸ¥è¯¢...")
        results = await repo.multi_search(
            query=[], user_id=test_user_id, event_type="TestBM25", size=10
        )
        assert len(results) == 1, "BM25è¿‡æ»¤æŸ¥è¯¢åº”è¯¥è¿”å›ç»“æœ"
        logger.info("âœ… BM25è¿‡æ»¤æŸ¥è¯¢æµ‹è¯•é€šè¿‡: æ‰¾åˆ° %d æ¡ç»“æœ", len(results))

        # æµ‹è¯•6: å¯¹æ¯”BM25æ–¹æ³•å’ŒåŸå§‹æ–¹æ³•çš„ç»“æœä¸€è‡´æ€§
        logger.info("æµ‹è¯•BM25æ–¹æ³•å’ŒåŸå§‹æ–¹æ³•çš„ç»“æœä¸€è‡´æ€§...")
        bm25_results = await repo.multi_search(
            query=["åå¥½", "æµ‹è¯•"], user_id=test_user_id, size=10
        )
        assert len(bm25_results) == 2, "BM25æ–¹æ³•åº”è¯¥è¿”å›ç»“æœ"
        logger.info("âœ… BM25æ–¹æ³•æµ‹è¯•é€šè¿‡: æ‰¾åˆ° %d æ¡ç»“æœ", len(bm25_results))

        # æ¸…ç†æµ‹è¯•æ•°æ®
        await repo.delete_by_event_id(test_event_id, refresh=True)
        await repo.delete_by_event_id(test_event_id_bm25, refresh=True)
        logger.info("âœ… DSLæœç´¢åŠŸèƒ½æµ‹è¯•å®Œæˆ")

    except Exception as e:
        logger.error("âŒ DSLæœç´¢åŠŸèƒ½æµ‹è¯•å¤±è´¥: %s", e)
        # å°è¯•æ¸…ç†
        try:
            await repo.delete_by_event_id(test_event_id, refresh=True)
        except Exception:
            pass
        raise


if __name__ == "__main__":
    asyncio.run(run_all_tests())
