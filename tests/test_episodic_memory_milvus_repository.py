#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯• EpisodicMemoryMilvusRepository çš„åŠŸèƒ½

æµ‹è¯•å†…å®¹åŒ…æ‹¬:
1. åŸºç¡€CRUDæ“ä½œï¼ˆå¢åˆ æ”¹æŸ¥ï¼‰
2. å‘é‡æœç´¢å’Œè¿‡æ»¤åŠŸèƒ½
3. æ‰¹é‡åˆ é™¤åŠŸèƒ½
4. æ—¶åŒºå¤„ç†
"""

import asyncio
from datetime import datetime, timedelta
import json
from zoneinfo import ZoneInfo
import numpy as np
from typing import List
from core.di import get_bean_by_type
from common_utils.datetime_utils import get_now_with_timezone, to_iso_format
from infra_layer.adapters.out.search.repository.episodic_memory_milvus_repository import (
    EpisodicMemoryMilvusRepository,
)
from core.observation.logger import get_logger

logger = get_logger(__name__)


def compare_datetime(dt1: datetime, dt2: datetime) -> bool:
    """æ¯”è¾ƒä¸¤ä¸ªdatetimeå¯¹è±¡ï¼Œåªæ¯”è¾ƒåˆ°ç§’çº§ç²¾åº¦"""
    return dt1.replace(microsecond=0) == dt2.replace(microsecond=0)


def generate_random_vector(dim: int = 1024) -> List[float]:
    """ç”Ÿæˆéšæœºå‘é‡ç”¨äºæµ‹è¯•"""
    return np.random.randn(dim).astype(np.float32).tolist()


def build_episodic_memory_entity(
    event_id: str,
    user_id: str,
    timestamp: datetime,
    episode: str,
    search_content: List[str],
    vector: List[float],
    user_name: str = "",
    title: str = "",
    summary: str = "",
    group_id: str = "",
    participants: List[str] = None,
    event_type: str = "",
    keywords: List[str] = None,
    linked_entities: List[str] = None,
    created_at: datetime = None,
    updated_at: datetime = None,
) -> dict:
    """
    æ„å»ºæƒ…æ™¯è®°å¿†å®ä½“ç”¨äºæµ‹è¯•

    Args:
        event_id: äº‹ä»¶ID
        user_id: ç”¨æˆ·ID
        timestamp: äº‹ä»¶æ—¶é—´æˆ³
        episode: æƒ…æ™¯æè¿°
        search_content: æœç´¢å†…å®¹åˆ—è¡¨
        vector: å‘é‡
        å…¶ä»–å‚æ•°ä¸ºå¯é€‰å‚æ•°

    Returns:
        dict: å¯ä»¥ç›´æ¥æ’å…¥ Milvus çš„å®ä½“å­—å…¸
    """
    now = get_now_with_timezone()
    if created_at is None:
        created_at = now
    if updated_at is None:
        updated_at = now

    # æ„å»º metadata
    metadata = {}
    if user_name:
        metadata["user_name"] = user_name
    if title:
        metadata["title"] = title
    if summary:
        metadata["summary"] = summary
    if participants:
        metadata["participants"] = participants
    if keywords:
        metadata["keywords"] = keywords
    if linked_entities:
        metadata["linked_entities"] = linked_entities

    # æ„å»ºå®ä½“
    entity = {
        "id": event_id,
        "user_id": user_id,
        "group_id": group_id if group_id is not None else "",
        "event_type": event_type if event_type is not None else "",
        "timestamp": int(timestamp.timestamp()),
        "episode": episode,
        "search_content": json.dumps(search_content, ensure_ascii=False),
        "metadata": json.dumps(metadata, ensure_ascii=False),
        "vector": vector,
        "created_at": int(created_at.timestamp()),
        "updated_at": int(updated_at.timestamp()),
    }

    return entity


async def test_crud_operations():
    """æµ‹è¯•åŸºç¡€CRUDæ“ä½œ"""
    logger.info("å¼€å§‹æµ‹è¯•åŸºç¡€CRUDæ“ä½œ...")

    repo = get_bean_by_type(EpisodicMemoryMilvusRepository)
    test_event_id = "test_event_crud_001"
    test_user_id = "test_user_crud_123"
    current_time = get_now_with_timezone()

    try:
        # æµ‹è¯•åˆ›å»ºï¼ˆCreateï¼‰
        entity = build_episodic_memory_entity(
            event_id=test_event_id,
            user_id=test_user_id,
            timestamp=current_time,
            episode="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æƒ…æ™¯è®°å¿†",
            search_content=["æµ‹è¯•", "æƒ…æ™¯", "è®°å¿†", "CRUD"],
            vector=generate_random_vector(),
            user_name="æµ‹è¯•ç”¨æˆ·",
            title="æµ‹è¯•æ ‡é¢˜",
            summary="æµ‹è¯•æ‘˜è¦",
            group_id="test_group_001",
            participants=["user1", "user2"],
            event_type="Test",
            keywords=["æµ‹è¯•", "å•å…ƒæµ‹è¯•"],
            linked_entities=["entity1", "entity2"],
        )

        # æ’å…¥æ–‡æ¡£
        await repo.collection.insert([entity])

        assert entity is not None
        assert entity["id"] == test_event_id
        assert entity["user_id"] == test_user_id
        assert entity["episode"] == "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æƒ…æ™¯è®°å¿†"
        logger.info("âœ… æµ‹è¯•åˆ›å»ºæ“ä½œæˆåŠŸ")

        # ç­‰å¾…æ•°æ®åˆ·æ–°
        await repo.flush()
        await asyncio.sleep(1)

        # æµ‹è¯•è¯»å–ï¼ˆReadï¼‰
        retrieved_doc = await repo.get_by_id(test_event_id)
        assert retrieved_doc is not None
        assert retrieved_doc["id"] == test_event_id
        assert retrieved_doc["user_id"] == test_user_id
        assert retrieved_doc["episode"] == "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æƒ…æ™¯è®°å¿†"
        metadata = json.loads(retrieved_doc["metadata"])
        assert metadata["title"] == "æµ‹è¯•æ ‡é¢˜"
        assert retrieved_doc["group_id"] == "test_group_001"
        logger.info("âœ… æµ‹è¯•è¯»å–æ“ä½œæˆåŠŸ")

        # æµ‹è¯•åˆ é™¤ï¼ˆDeleteï¼‰
        delete_result = await repo.delete_by_event_id(test_event_id)
        assert delete_result is True
        logger.info("âœ… æµ‹è¯•åˆ é™¤æ“ä½œæˆåŠŸ")

        # éªŒè¯åˆ é™¤
        await repo.flush()
        deleted_check = await repo.get_by_id(test_event_id)
        assert deleted_check is None, "æ–‡æ¡£åº”è¯¥å·²è¢«åˆ é™¤"
        logger.info("âœ… éªŒè¯åˆ é™¤ç»“æœæˆåŠŸ")

    except Exception as e:
        logger.error("âŒ æµ‹è¯•åŸºç¡€CRUDæ“ä½œå¤±è´¥: %s", e)
        # æ¸…ç†å¯èƒ½æ®‹ç•™çš„æ•°æ®
        try:
            await repo.delete_by_event_id(test_event_id)
            await repo.flush()
        except Exception:
            pass
        raise

    logger.info("âœ… åŸºç¡€CRUDæ“ä½œæµ‹è¯•å®Œæˆ")


async def test_vector_search():
    """æµ‹è¯•å‘é‡æœç´¢å’Œè¿‡æ»¤åŠŸèƒ½"""
    logger.info("å¼€å§‹æµ‹è¯•å‘é‡æœç´¢å’Œè¿‡æ»¤åŠŸèƒ½...")

    repo = get_bean_by_type(EpisodicMemoryMilvusRepository)
    test_user_id = "test_user_search_456"
    test_group_id = "test_group_search_789"
    base_time = get_now_with_timezone()
    test_event_ids = []
    base_vector = generate_random_vector()  # åŸºå‡†å‘é‡

    try:
        # åˆ›å»ºå¤šä¸ªæµ‹è¯•è®°å¿†
        test_data = [
            {
                "event_id": f"search_test_001_{int(base_time.timestamp())}",
                "episode": "è®¨è®ºäº†å…¬å¸çš„å‘å±•æˆ˜ç•¥",
                "search_content": ["å…¬å¸", "å‘å±•", "æˆ˜ç•¥", "è®¨è®º"],
                "vector": [
                    x + 0.1 * np.random.randn() for x in base_vector
                ],  # ç›¸ä¼¼å‘é‡
                "title": "æˆ˜ç•¥ä¼šè®®",
                "group_id": test_group_id,
                "event_type": "Conversation",
                "keywords": ["ä¼šè®®", "æˆ˜ç•¥"],
                "timestamp": base_time - timedelta(days=1),
            },
            {
                "event_id": f"search_test_002_{int(base_time.timestamp())}",
                "episode": "å­¦ä¹ äº†æ–°çš„æŠ€æœ¯æ¡†æ¶",
                "search_content": ["æŠ€æœ¯", "æ¡†æ¶", "å­¦ä¹ ", "ç¼–ç¨‹"],
                "vector": generate_random_vector(),  # éšæœºå‘é‡
                "title": "æŠ€æœ¯å­¦ä¹ ",
                "group_id": "",
                "event_type": "Learning",
                "keywords": ["æŠ€æœ¯", "å­¦ä¹ "],
                "timestamp": base_time - timedelta(days=2),
            },
            {
                "event_id": f"search_test_003_{int(base_time.timestamp())}",
                "episode": "å‚åŠ äº†å›¢é˜Ÿå»ºè®¾æ´»åŠ¨",
                "search_content": ["å›¢é˜Ÿ", "å»ºè®¾", "æ´»åŠ¨", "å‚åŠ "],
                "vector": [
                    x + 0.2 * np.random.randn() for x in base_vector
                ],  # ç›¸ä¼¼å‘é‡
                "title": "å›¢é˜Ÿæ´»åŠ¨",
                "group_id": test_group_id,
                "event_type": "Activity",
                "keywords": ["å›¢é˜Ÿ", "æ´»åŠ¨"],
                "timestamp": base_time - timedelta(days=3),
            },
        ]

        # æ‰¹é‡åˆ›å»ºæµ‹è¯•æ•°æ®
        for data in test_data:
            entity = build_episodic_memory_entity(
                event_id=data["event_id"],
                user_id=test_user_id,
                timestamp=data["timestamp"],
                episode=data["episode"],
                search_content=data["search_content"],
                vector=data["vector"],
                title=data["title"],
                group_id=data["group_id"],
                event_type=data["event_type"],
                keywords=data["keywords"],
            )
            await repo.collection.insert([entity])
            test_event_ids.append(data["event_id"])

        # åˆ·æ–°é›†åˆ
        await repo.flush()
        await repo.load()  # åŠ è½½åˆ°å†…å­˜ä»¥æé«˜æœç´¢æ€§èƒ½

        logger.info("âœ… åˆ›å»ºäº† %d ä¸ªæµ‹è¯•è®°å¿†", len(test_data))

        # ç­‰å¾…æ•°æ®åŠ è½½
        await asyncio.sleep(2)

        # æµ‹è¯•1: å‘é‡ç›¸ä¼¼åº¦æœç´¢
        logger.info("æµ‹è¯•1: å‘é‡ç›¸ä¼¼åº¦æœç´¢")
        results = await repo.vector_search(
            query_vector=base_vector, user_id=test_user_id, limit=10
        )
        assert len(results) >= 2, f"åº”è¯¥æ‰¾åˆ°è‡³å°‘2æ¡ç›¸ä¼¼è®°å½•ï¼Œå®é™…æ‰¾åˆ°{len(results)}æ¡"
        logger.info("âœ… å‘é‡ç›¸ä¼¼åº¦æœç´¢æµ‹è¯•æˆåŠŸï¼Œæ‰¾åˆ° %d æ¡ç»“æœ", len(results))

        # æµ‹è¯•2: æŒ‰ç”¨æˆ·IDè¿‡æ»¤çš„å‘é‡æœç´¢
        logger.info("æµ‹è¯•2: æŒ‰ç”¨æˆ·IDè¿‡æ»¤çš„å‘é‡æœç´¢")
        user_results = await repo.vector_search(
            query_vector=base_vector, user_id=test_user_id, limit=10
        )
        assert (
            len(user_results) >= 2
        ), f"åº”è¯¥æ‰¾åˆ°è‡³å°‘2æ¡ç”¨æˆ·è®°å½•ï¼Œå®é™…æ‰¾åˆ°{len(user_results)}æ¡"
        logger.info("âœ… ç”¨æˆ·IDè¿‡æ»¤æµ‹è¯•æˆåŠŸï¼Œæ‰¾åˆ° %d æ¡ç»“æœ", len(user_results))

        # æµ‹è¯•3: æŒ‰ç¾¤ç»„IDè¿‡æ»¤çš„å‘é‡æœç´¢
        logger.info("æµ‹è¯•3: æŒ‰ç¾¤ç»„IDè¿‡æ»¤çš„å‘é‡æœç´¢")
        group_results = await repo.vector_search(
            query_vector=base_vector,
            user_id=test_user_id,
            group_id=test_group_id,
            limit=10,
        )
        assert (
            len(group_results) >= 1
        ), f"åº”è¯¥æ‰¾åˆ°è‡³å°‘1æ¡ç¾¤ç»„è®°å½•ï¼Œå®é™…æ‰¾åˆ°{len(group_results)}æ¡"
        logger.info("âœ… ç¾¤ç»„IDè¿‡æ»¤æµ‹è¯•æˆåŠŸï¼Œæ‰¾åˆ° %d æ¡ç»“æœ", len(group_results))

        # æµ‹è¯•4: æŒ‰äº‹ä»¶ç±»å‹è¿‡æ»¤çš„å‘é‡æœç´¢
        logger.info("æµ‹è¯•4: æŒ‰äº‹ä»¶ç±»å‹è¿‡æ»¤çš„å‘é‡æœç´¢")
        type_results = await repo.vector_search(
            query_vector=base_vector,
            user_id=test_user_id,
            event_type="Conversation",
            limit=10,
        )
        assert (
            len(type_results) >= 1
        ), f"åº”è¯¥æ‰¾åˆ°è‡³å°‘1æ¡Conversationç±»å‹è®°å½•ï¼Œå®é™…æ‰¾åˆ°{len(type_results)}æ¡"
        logger.info("âœ… äº‹ä»¶ç±»å‹è¿‡æ»¤æµ‹è¯•æˆåŠŸï¼Œæ‰¾åˆ° %d æ¡ç»“æœ", len(type_results))

        # æµ‹è¯•5: æŒ‰æ—¶é—´èŒƒå›´è¿‡æ»¤çš„å‘é‡æœç´¢
        logger.info("æµ‹è¯•5: æŒ‰æ—¶é—´èŒƒå›´è¿‡æ»¤çš„å‘é‡æœç´¢")
        time_results = await repo.vector_search(
            query_vector=base_vector,
            user_id=test_user_id,
            start_time=base_time - timedelta(days=2),
            end_time=base_time,
            limit=10,
        )
        assert (
            len(time_results) >= 1
        ), f"åº”è¯¥æ‰¾åˆ°è‡³å°‘1æ¡æ—¶é—´èŒƒå›´å†…çš„è®°å½•ï¼Œå®é™…æ‰¾åˆ°{len(time_results)}æ¡"
        logger.info("âœ… æ—¶é—´èŒƒå›´è¿‡æ»¤æµ‹è¯•æˆåŠŸï¼Œæ‰¾åˆ° %d æ¡ç»“æœ", len(time_results))

    except Exception as e:
        logger.error("âŒ æµ‹è¯•å‘é‡æœç´¢å’Œè¿‡æ»¤åŠŸèƒ½å¤±è´¥: %s", e)
        raise
    finally:
        # æ¸…ç†æµ‹è¯•æ•°æ®
        logger.info("æ¸…ç†æœç´¢æµ‹è¯•æ•°æ®...")
        try:
            cleanup_count = await repo.delete_by_filters(user_id=test_user_id)
            await repo.flush()
            logger.info("âœ… æ¸…ç†äº† %d æ¡æœç´¢æµ‹è¯•æ•°æ®", cleanup_count)
        except Exception as cleanup_error:
            logger.error("æ¸…ç†æœç´¢æµ‹è¯•æ•°æ®æ—¶å‡ºç°é”™è¯¯: %s", cleanup_error)

    logger.info("âœ… å‘é‡æœç´¢å’Œè¿‡æ»¤åŠŸèƒ½æµ‹è¯•å®Œæˆ")


async def test_delete_operations():
    """æµ‹è¯•åˆ é™¤åŠŸèƒ½"""
    logger.info("å¼€å§‹æµ‹è¯•åˆ é™¤åŠŸèƒ½...")

    repo = get_bean_by_type(EpisodicMemoryMilvusRepository)
    test_user_id = "test_user_delete_789"
    test_group_id = "test_group_delete_012"
    base_time = get_now_with_timezone()
    test_event_ids = []

    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        for i in range(6):
            event_id = f"delete_test_{i}_{int(base_time.timestamp())}"
            test_event_ids.append(event_id)

            entity = build_episodic_memory_entity(
                event_id=event_id,
                user_id=test_user_id,
                timestamp=base_time - timedelta(days=i),
                episode=f"åˆ é™¤æµ‹è¯•è®°å¿† {i}",
                search_content=["åˆ é™¤", "æµ‹è¯•", f"è®°å¿†{i}"],
                vector=generate_random_vector(),
                title=f"åˆ é™¤æµ‹è¯• {i}",
                group_id=test_group_id if i % 2 == 0 else "",  # éƒ¨åˆ†æœ‰group_id
                event_type="DeleteTest",
            )
            await repo.collection.insert([entity])

        await repo.flush()
        logger.info("âœ… åˆ›å»ºäº† %d ä¸ªåˆ é™¤æµ‹è¯•è®°å¿†", len(test_event_ids))

        # ç­‰å¾…æ•°æ®åˆ·æ–°
        await asyncio.sleep(2)

        # æµ‹è¯•1: æŒ‰event_idåˆ é™¤
        logger.info("æµ‹è¯•1: æŒ‰event_idåˆ é™¤")
        event_id_to_delete = test_event_ids[0]
        delete_result = await repo.delete_by_event_id(event_id_to_delete)
        assert delete_result is True

        # éªŒè¯åˆ é™¤
        await repo.flush()
        deleted_doc = await repo.get_by_id(event_id_to_delete)
        assert deleted_doc is None, "æ–‡æ¡£åº”è¯¥å·²è¢«åˆ é™¤"
        logger.info("âœ… æŒ‰event_idåˆ é™¤æµ‹è¯•æˆåŠŸ")

        # æµ‹è¯•2: æŒ‰è¿‡æ»¤æ¡ä»¶åˆ é™¤ - åªåˆ é™¤æœ‰group_idçš„è®°å¿†
        logger.info("æµ‹è¯•2: æŒ‰è¿‡æ»¤æ¡ä»¶åˆ é™¤ï¼ˆgroup_idï¼‰")
        deleted_count = await repo.delete_by_filters(
            user_id=test_user_id, group_id=test_group_id
        )
        assert (
            deleted_count >= 2
        ), f"åº”è¯¥åˆ é™¤è‡³å°‘2æ¡æœ‰group_idçš„è®°å½•ï¼Œå®é™…åˆ é™¤{deleted_count}æ¡"
        logger.info("âœ… æŒ‰group_idè¿‡æ»¤åˆ é™¤æµ‹è¯•æˆåŠŸï¼Œåˆ é™¤äº† %d æ¡è®°å½•", deleted_count)

        # æµ‹è¯•3: æŒ‰æ—¶é—´èŒƒå›´åˆ é™¤
        logger.info("æµ‹è¯•3: æŒ‰æ—¶é—´èŒƒå›´åˆ é™¤")
        deleted_count = await repo.delete_by_filters(
            user_id=test_user_id,
            start_time=base_time - timedelta(days=2),
            end_time=base_time,
        )
        logger.info("âœ… æŒ‰æ—¶é—´èŒƒå›´åˆ é™¤æµ‹è¯•æˆåŠŸï¼Œåˆ é™¤äº† %d æ¡è®°å½•", deleted_count)

        # æµ‹è¯•4: éªŒè¯å‚æ•°æ£€æŸ¥
        logger.info("æµ‹è¯•4: éªŒè¯å‚æ•°æ£€æŸ¥")
        try:
            await repo.delete_by_filters()  # æ²¡æœ‰æä¾›ä»»ä½•è¿‡æ»¤æ¡ä»¶
            assert False, "åº”è¯¥æŠ›å‡ºå¼‚å¸¸ä½†æ²¡æœ‰"
        except ValueError as e:
            logger.info("âœ… æ­£ç¡®æ•è·å‚æ•°é”™è¯¯: %s", e)

        # æœ€ç»ˆæ¸…ç†å‰©ä½™æ•°æ®
        remaining_count = await repo.delete_by_filters(user_id=test_user_id)
        await repo.flush()
        logger.info("âœ… æœ€ç»ˆæ¸…ç†äº† %d æ¡å‰©ä½™æ•°æ®", remaining_count)

    except Exception as e:
        logger.error("âŒ æµ‹è¯•åˆ é™¤åŠŸèƒ½å¤±è´¥: %s", e)
        raise
    finally:
        # ç¡®ä¿æ¸…ç†æ‰€æœ‰æµ‹è¯•æ•°æ®
        try:
            await repo.delete_by_filters(user_id=test_user_id)
            await repo.flush()
        except Exception:
            pass

    logger.info("âœ… åˆ é™¤åŠŸèƒ½æµ‹è¯•å®Œæˆ")


async def test_timezone_handling():
    """æµ‹è¯•æ—¶åŒºå¤„ç†"""
    logger.info("å¼€å§‹æµ‹è¯•æ—¶åŒºå¤„ç†...")

    repo = get_bean_by_type(EpisodicMemoryMilvusRepository)
    test_event_id = "test_timezone_001"
    test_user_id = "test_user_timezone_999"

    try:
        # åˆ›å»ºä¸åŒæ—¶åŒºçš„æ—¶é—´
        utc_time = datetime.now(ZoneInfo("UTC"))
        tokyo_time = datetime.now(ZoneInfo("Asia/Tokyo"))
        shanghai_time = get_now_with_timezone()  # é»˜è®¤ä¸Šæµ·æ—¶åŒº

        logger.info("åŸå§‹UTCæ—¶é—´: %s", to_iso_format(utc_time))
        logger.info("åŸå§‹ä¸œäº¬æ—¶é—´: %s", to_iso_format(tokyo_time))
        logger.info("åŸå§‹ä¸Šæµ·æ—¶é—´: %s", to_iso_format(shanghai_time))

        # ä½¿ç”¨UTCæ—¶é—´åˆ›å»ºè®°å¿†
        entity = build_episodic_memory_entity(
            event_id=test_event_id,
            user_id=test_user_id,
            timestamp=utc_time,
            episode="æ—¶åŒºæµ‹è¯•è®°å¿†",
            search_content=["æ—¶åŒº", "æµ‹è¯•"],
            vector=generate_random_vector(),
            title="æ—¶åŒºæµ‹è¯•",
            created_at=tokyo_time,
            updated_at=shanghai_time,
        )

        await repo.collection.insert([entity])

        assert entity is not None
        logger.info("âœ… åˆ›å»ºå¸¦æ—¶åŒºä¿¡æ¯çš„è®°å¿†æˆåŠŸ")

        await repo.flush()
        await asyncio.sleep(2)

        # ä»æ•°æ®åº“è·å–å¹¶éªŒè¯
        retrieved_doc = await repo.get_by_id(test_event_id)
        assert retrieved_doc is not None

        # è§£ææ—¶é—´æˆ³
        retrieved_timestamp = datetime.fromtimestamp(retrieved_doc["timestamp"])
        logger.info("ä»æ•°æ®åº“è·å–çš„æ—¶é—´æˆ³: %s", to_iso_format(retrieved_timestamp))

        # éªŒè¯æ—¶é—´è½¬æ¢æ­£ç¡®æ€§ï¼ˆè½¬æ¢åˆ°åŒä¸€æ—¶åŒºååº”è¯¥ç›¸ç­‰ï¼‰
        assert compare_datetime(
            retrieved_timestamp.astimezone(ZoneInfo("UTC")),
            utc_time.astimezone(ZoneInfo("UTC")),
        )
        logger.info("âœ… æ—¶åŒºéªŒè¯æˆåŠŸ")

        # æµ‹è¯•æ—¶é—´èŒƒå›´æŸ¥è¯¢
        results = await repo.vector_search(
            query_vector=generate_random_vector(),
            user_id=test_user_id,
            start_time=shanghai_time - timedelta(hours=2),
            end_time=shanghai_time + timedelta(hours=2),
            limit=10,
        )
        assert len(results) >= 1, "åº”è¯¥æ‰¾åˆ°æ—¶é—´èŒƒå›´å†…çš„è®°å½•"
        logger.info("âœ… æ—¶åŒºæ—¶é—´èŒƒå›´æŸ¥è¯¢æµ‹è¯•æˆåŠŸ")

    except Exception as e:
        logger.error("âŒ æµ‹è¯•æ—¶åŒºå¤„ç†å¤±è´¥: %s", e)
        raise
    finally:
        # æ¸…ç†æµ‹è¯•æ•°æ®
        try:
            await repo.delete_by_event_id(test_event_id)
            await repo.flush()
            logger.info("âœ… æ¸…ç†æ—¶åŒºæµ‹è¯•æ•°æ®æˆåŠŸ")
        except Exception:
            pass

    logger.info("âœ… æ—¶åŒºå¤„ç†æµ‹è¯•å®Œæˆ")


async def test_edge_cases():
    """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
    logger.info("å¼€å§‹æµ‹è¯•è¾¹ç•Œæƒ…å†µ...")

    repo = get_bean_by_type(EpisodicMemoryMilvusRepository)
    test_user_id = "test_user_edge_111"

    try:
        # æµ‹è¯•1: ä¸å­˜åœ¨çš„ç”¨æˆ·
        logger.info("æµ‹è¯•1: ä¸å­˜åœ¨çš„ç”¨æˆ·")
        nonexistent_results = await repo.vector_search(
            query_vector=generate_random_vector(),
            user_id="nonexistent_user_999999",
            limit=10,
        )
        assert len(nonexistent_results) == 0, "ä¸å­˜åœ¨çš„ç”¨æˆ·åº”è¯¥è¿”å›ç©ºç»“æœ"
        logger.info("âœ… ä¸å­˜åœ¨ç”¨æˆ·æµ‹è¯•æˆåŠŸ")

        # æµ‹è¯•2: åˆ é™¤ä¸å­˜åœ¨çš„event_id
        logger.info("æµ‹è¯•2: åˆ é™¤ä¸å­˜åœ¨çš„event_id")
        delete_result = await repo.delete_by_event_id("nonexistent_event_999999")
        assert delete_result is True, "åˆ é™¤ä¸å­˜åœ¨çš„æ–‡æ¡£ä¸çŸ¥é“ä¸ºä»€ä¹ˆä¹Ÿè¿”å›True"
        logger.info("âœ… åˆ é™¤ä¸å­˜åœ¨æ–‡æ¡£æµ‹è¯•æˆåŠŸ")

        # æµ‹è¯•3: ä½¿ç”¨æ— æ•ˆçš„æ—¶é—´èŒƒå›´
        logger.info("æµ‹è¯•3: ä½¿ç”¨æ— æ•ˆçš„æ—¶é—´èŒƒå›´")
        future_time = datetime.now(ZoneInfo("UTC")) + timedelta(days=365)
        future_results = await repo.vector_search(
            query_vector=generate_random_vector(),
            user_id=test_user_id,
            start_time=future_time,
            end_time=future_time + timedelta(days=1),
            limit=10,
        )
        assert len(future_results) == 0, "æœªæ¥æ—¶é—´èŒƒå›´åº”è¯¥è¿”å›ç©ºç»“æœ"
        logger.info("âœ… æ— æ•ˆæ—¶é—´èŒƒå›´æµ‹è¯•æˆåŠŸ")

        # æµ‹è¯•4: å‘é‡ç»´åº¦éªŒè¯
        logger.info("æµ‹è¯•4: å‘é‡ç»´åº¦éªŒè¯")
        try:
            entity = build_episodic_memory_entity(
                event_id="invalid_vector_test",
                user_id=test_user_id,
                timestamp=get_now_with_timezone(),
                episode="æ— æ•ˆå‘é‡æµ‹è¯•",
                search_content=["æµ‹è¯•"],
                vector=[1.0] * 512,  # é”™è¯¯çš„å‘é‡ç»´åº¦
            )
            await repo.collection.insert([entity])
            assert False, "åº”è¯¥å› ä¸ºå‘é‡ç»´åº¦é”™è¯¯è€Œå¤±è´¥"
        except Exception as e:
            assert "the length(512) of float data should divide the dim(1024)" in str(e)
            logger.info("âœ… æ­£ç¡®æ•è·å‘é‡ç»´åº¦é”™è¯¯: %s", e)

    except Exception as e:
        logger.error("âŒ æµ‹è¯•è¾¹ç•Œæƒ…å†µå¤±è´¥: %s", e)
        raise

    logger.info("âœ… è¾¹ç•Œæƒ…å†µæµ‹è¯•å®Œæˆ")


async def test_performance():
    """æµ‹è¯•æ€§èƒ½"""
    logger.info("å¼€å§‹æ€§èƒ½æµ‹è¯•...")

    repo = get_bean_by_type(EpisodicMemoryMilvusRepository)
    test_user_id = "test_user_perf_001"
    current_time = get_now_with_timezone()
    num_docs = 1000

    try:
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_data = []
        base_vector = generate_random_vector()

        for i in range(num_docs):
            # ç”Ÿæˆä¸€ä¸ªä¸åŸºå‡†å‘é‡ç›¸ä¼¼çš„å‘é‡
            noise = np.random.normal(0, 0.1, len(base_vector))
            vector = [x + n for x, n in zip(base_vector, noise)]

            test_data.append(
                {
                    "event_id": f"perf_test_{i}",
                    "user_id": test_user_id,
                    "timestamp": current_time - timedelta(minutes=i),
                    "episode": f"æ€§èƒ½æµ‹è¯•è®°å¿† {i}",
                    "search_content": ["æ€§èƒ½", "æµ‹è¯•", f"è®°å¿†{i}"],
                    "vector": vector,
                    "title": f"æ€§èƒ½æµ‹è¯• {i}",
                    "group_id": "perf_test_group",
                    "event_type": "PerfTest",
                }
            )

        # æµ‹è¯•1: æ‰¹é‡æ’å…¥æ€§èƒ½
        logger.info("æµ‹è¯•1: æ‰¹é‡æ’å…¥æ€§èƒ½ (%d æ¡è®°å½•)...", num_docs)
        insert_times = []
        batch_size = 100

        for i in range(0, num_docs, batch_size):
            batch = test_data[i : i + batch_size]
            start_time = datetime.now()

            for doc in batch:
                entity = build_episodic_memory_entity(**doc)
                await repo.collection.insert([entity])

            end_time = datetime.now()
            insert_time = (end_time - start_time).total_seconds()
            insert_times.append(insert_time)

            logger.info(
                "- æ‰¹æ¬¡ %d/%d: %.3f ç§’ (%.1f æ¡/ç§’)",
                i // batch_size + 1,
                (num_docs + batch_size - 1) // batch_size,
                insert_time,
                len(batch) / insert_time,
            )

        avg_insert_time = sum(insert_times) / len(insert_times)
        min_insert_time = min(insert_times)
        max_insert_time = max(insert_times)
        total_insert_time = sum(insert_times)

        logger.info("æ’å…¥æ€§èƒ½ç»Ÿè®¡:")
        logger.info("- æ€»æ—¶é—´: %.3f ç§’", total_insert_time)
        logger.info(
            "- å¹³å‡æ¯æ‰¹æ¬¡: %.3f ç§’ (%.1f æ¡/ç§’)",
            avg_insert_time,
            batch_size / avg_insert_time,
        )
        logger.info(
            "- æœ€å¿«æ‰¹æ¬¡: %.3f ç§’ (%.1f æ¡/ç§’)",
            min_insert_time,
            batch_size / min_insert_time,
        )
        logger.info(
            "- æœ€æ…¢æ‰¹æ¬¡: %.3f ç§’ (%.1f æ¡/ç§’)",
            max_insert_time,
            batch_size / max_insert_time,
        )

        # æµ‹è¯•2: Flushæ€§èƒ½
        logger.info("æµ‹è¯•2: Flushæ€§èƒ½...")
        start_time = datetime.now()
        await repo.flush()
        flush_time = (datetime.now() - start_time).total_seconds()
        logger.info("Flushè€—æ—¶: %.3f ç§’", flush_time)

        # ç­‰å¾…æ•°æ®åŠ è½½
        await repo.load()
        await asyncio.sleep(2)

        # æµ‹è¯•3: æœç´¢æ€§èƒ½
        logger.info("æµ‹è¯•3: æœç´¢æ€§èƒ½...")
        search_times = []
        num_searches = 10

        for i in range(num_searches):
            # ç”Ÿæˆä¸€ä¸ªä¸åŸºå‡†å‘é‡ç›¸ä¼¼çš„æŸ¥è¯¢å‘é‡
            noise = np.random.normal(0, 0.1, len(base_vector))
            query_vector = [x + n for x, n in zip(base_vector, noise)]

            start_time = datetime.now()
            results = await repo.vector_search(
                query_vector=query_vector, user_id=test_user_id, limit=10
            )
            search_time = (datetime.now() - start_time).total_seconds()
            search_times.append(search_time)

            logger.info(
                "- æœç´¢ %d/%d: %.3f ç§’, æ‰¾åˆ° %d æ¡ç»“æœ",
                i + 1,
                num_searches,
                search_time,
                len(results),
            )

        avg_search_time = sum(search_times) / len(search_times)
        min_search_time = min(search_times)
        max_search_time = max(search_times)

        logger.info("æœç´¢æ€§èƒ½ç»Ÿè®¡:")
        logger.info("- å¹³å‡è€—æ—¶: %.3f ç§’", avg_search_time)
        logger.info("- æœ€å¿«è€—æ—¶: %.3f ç§’", min_search_time)
        logger.info("- æœ€æ…¢è€—æ—¶: %.3f ç§’", max_search_time)

    except Exception as e:
        logger.error("âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: %s", e)
        raise
    finally:
        # æ¸…ç†æµ‹è¯•æ•°æ®
        try:
            cleanup_count = await repo.delete_by_filters(user_id=test_user_id)
            await repo.flush()
            logger.info("âœ… æ¸…ç†äº† %d æ¡æ€§èƒ½æµ‹è¯•æ•°æ®", cleanup_count)
        except Exception as cleanup_error:
            logger.error("æ¸…ç†æ€§èƒ½æµ‹è¯•æ•°æ®æ—¶å‡ºç°é”™è¯¯: %s", cleanup_error)

    logger.info("âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ")


async def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    logger.info("ğŸš€ å¼€å§‹è¿è¡ŒEpisodicMemoryMilvusRepositoryæ‰€æœ‰æµ‹è¯•...")

    try:
        await test_crud_operations()
        await test_vector_search()
        await test_delete_operations()
        await test_timezone_handling()
        await test_edge_cases()
        await test_performance()
        logger.info("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ")
    except Exception as e:
        logger.error("âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: %s", e)
        raise


if __name__ == "__main__":
    asyncio.run(run_all_tests())
