"""å¯¹è¯ä¼šè¯ç®¡ç†

ç®¡ç†å•ä¸ªç¾¤ç»„çš„å¯¹è¯ä¼šè¯ï¼Œæä¾›è®°å¿†æ£€ç´¢å’Œ LLM å¯¹è¯åŠŸèƒ½ã€‚
"""

import json
import httpx
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timedelta
from pathlib import Path

from demo.config import ChatModeConfig, LLMConfig, ScenarioType
from demo.utils import query_memcells_by_group_and_time
from demo.ui import I18nTexts
from memory_layer.llm.llm_provider import LLMProvider
from common_utils.datetime_utils import get_now_with_timezone


class ChatSession:
    """å¯¹è¯ä¼šè¯ç®¡ç†å™¨"""
    
    def __init__(
        self,
        group_id: str,
        config: ChatModeConfig,
        llm_config: LLMConfig,
        scenario_type: ScenarioType,
        retrieval_mode: str,  # "rrf" / "embedding" / "bm25"
        data_source: str,     # "episode" / "event_log"
        texts: I18nTexts,
    ):
        """åˆå§‹åŒ–å¯¹è¯ä¼šè¯
        
        Args:
            group_id: ç¾¤ç»„ ID
            config: å¯¹è¯æ¨¡å¼é…ç½®
            llm_config: LLM é…ç½®
            scenario_type: åœºæ™¯ç±»å‹
            retrieval_mode: æ£€ç´¢æ¨¡å¼ï¼ˆrrf/embedding/bm25ï¼‰
            data_source: æ•°æ®æºï¼ˆepisode/event_logï¼‰
            texts: å›½é™…åŒ–æ–‡æœ¬å¯¹è±¡
        """
        self.group_id = group_id
        self.config = config
        self.llm_config = llm_config
        self.scenario_type = scenario_type
        self.retrieval_mode = retrieval_mode
        self.data_source = data_source
        self.texts = texts
        
        # ä¼šè¯çŠ¶æ€
        self.conversation_history: List[Tuple[str, str]] = []
        self.memcell_count: int = 0
        
        # æœåŠ¡
        self.llm_provider: Optional[LLMProvider] = None
        
        # API é…ç½®
        self.api_base_url = config.api_base_url
        self.retrieve_lightweight_url = f"{self.api_base_url}/api/v3/agentic/retrieve_lightweight"
        self.retrieve_agentic_url = f"{self.api_base_url}/api/v3/agentic/retrieve_agentic"
        
        # æœ€åä¸€æ¬¡æ£€ç´¢å…ƒæ•°æ®
        self.last_retrieval_metadata: Optional[Dict[str, Any]] = None
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–ä¼šè¯
        
        Returns:
            åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        try:
            display_name = "group_chat" if self.group_id == "AIäº§å“ç¾¤" else self.group_id
            print(f"\n[{self.texts.get('loading_label')}] {self.texts.get('loading_group_data', name=display_name)}")
            
            # æ£€æŸ¥ API æœåŠ¡å™¨å¥åº·çŠ¶æ€
            await self._check_api_server()
            
            # ç»Ÿè®¡ MemCell æ•°é‡
            now = get_now_with_timezone()
            start_date = now - timedelta(days=self.config.time_range_days)
            memcells = await query_memcells_by_group_and_time(self.group_id, start_date, now)
            self.memcell_count = len(memcells)
            print(f"[{self.texts.get('loading_label')}] {self.texts.get('loading_memories_success', count=self.memcell_count)} âœ…")
            
            # åŠ è½½å¯¹è¯å†å²
            loaded_history_count = await self.load_conversation_history()
            if loaded_history_count > 0:
                print(f"[{self.texts.get('loading_label')}] {self.texts.get('loading_history_success', count=loaded_history_count)} âœ…")
            else:
                print(f"[{self.texts.get('loading_label')}] {self.texts.get('loading_history_new')} âœ…")
            
            # åˆ›å»º LLM Provider
            self.llm_provider = LLMProvider(
                self.llm_config.provider,
                model=self.llm_config.model,
                api_key=self.llm_config.api_key,
                base_url=self.llm_config.base_url,
                temperature=self.llm_config.temperature,
                max_tokens=self.llm_config.max_tokens,
            )
            
            print(f"\n[{self.texts.get('hint_label')}] {self.texts.get('loading_help_hint')}\n")
            return True
        
        except Exception as e:
            print(f"\n[{self.texts.get('error_label')}] {self.texts.get('session_init_error', error=str(e))}")
            import traceback
            traceback.print_exc()
            return False
    
    async def _check_api_server(self) -> None:
        """æ£€æŸ¥ API æœåŠ¡å™¨æ˜¯å¦è¿è¡Œ
        
        Raises:
            ConnectionError: å¦‚æœæœåŠ¡å™¨æœªè¿è¡Œ
        """
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                # å°è¯•è®¿é—®å¥åº·æ£€æŸ¥ç«¯ç‚¹æˆ–ä»»ä½•ç«¯ç‚¹
                response = await client.get(f"{self.api_base_url}/docs")
                if response.status_code >= 500:
                    raise ConnectionError("API æœåŠ¡å™¨è¿”å›é”™è¯¯")
        except (httpx.ConnectError, httpx.TimeoutException, ConnectionError) as e:
            error_msg = (
                f"\nâŒ æ— æ³•è¿æ¥åˆ° API æœåŠ¡å™¨: {self.api_base_url}\n\n"
                f"è¯·å…ˆå¯åŠ¨ V3 API æœåŠ¡å™¨ï¼š\n"
                f"  uv run python src/bootstrap.py src/run.py --port 8001\n\n"
                f"ç„¶ååœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡ŒèŠå¤©åº”ç”¨ã€‚\n"
            )
            raise ConnectionError(error_msg) from e
    
    async def load_conversation_history(self) -> int:
        """ä»æ–‡ä»¶åŠ è½½å¯¹è¯å†å²
        
        Returns:
            åŠ è½½çš„å¯¹è¯è½®æ•°
        """
        try:
            display_name = "group_chat" if self.group_id == "AIäº§å“ç¾¤" else self.group_id
            history_files = sorted(
                self.config.chat_history_dir.glob(f"{display_name}_*.json"),
                reverse=True
            )
            
            if not history_files:
                return 0
            
            latest_file = history_files[0]
            with latest_file.open("r", encoding="utf-8") as fp:
                data = json.load(fp)
            
            history = data.get("conversation_history", [])
            self.conversation_history = [
                (item["user_input"], item["assistant_response"])
                for item in history[-self.config.conversation_history_size:]
            ]
            
            return len(self.conversation_history)
        
        except Exception as e:
            print(f"[{self.texts.get('warning_label')}] {self.texts.get('loading_history_new')}: {e}")
            return 0
    
    async def save_conversation_history(self) -> None:
        """ä¿å­˜å¯¹è¯å†å²åˆ°æ–‡ä»¶"""
        try:
            display_name = "group_chat" if self.group_id == "AIäº§å“ç¾¤" else self.group_id
            timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M")
            filename = f"{display_name}_{timestamp}.json"
            filepath = self.config.chat_history_dir / filename
            
            data = {
                "group_id": self.group_id,
                "last_updated": datetime.now().isoformat(),
                "conversation_history": [
                    {
                        "timestamp": datetime.now().isoformat(),
                        "user_input": user_q,
                        "assistant_response": assistant_a,
                    }
                    for user_q, assistant_a in self.conversation_history
                ],
            }
            
            with filepath.open("w", encoding="utf-8") as fp:
                json.dump(data, fp, ensure_ascii=False, indent=2)
            
            print(f"[{self.texts.get('save_label')}] {filename} âœ…")
        
        except Exception as e:
            print(f"[{self.texts.get('error_label')}] {e}")
    
    async def retrieve_memories(self, query: str) -> List[Dict[str, Any]]:
        """æ£€ç´¢ç›¸å…³è®°å¿† - é€šè¿‡ HTTP API è°ƒç”¨
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            æ£€ç´¢åˆ°çš„è®°å¿†åˆ—è¡¨
        """
        # ğŸ”¥ æ ¹æ®æ£€ç´¢æ¨¡å¼é€‰æ‹©ä¸åŒçš„ HTTP API ç«¯ç‚¹
        if self.retrieval_mode == "agentic":
            # Agentic æ£€ç´¢ API
            result = await self._call_retrieve_agentic_api(query)
        else:
            # Lightweight æ£€ç´¢ API
            result = await self._call_retrieve_lightweight_api(query)
        
        # æå–ç»“æœå’Œå…ƒæ•°æ®
        memories = result.get("memories", [])
        metadata = result.get("metadata", {})
        
        # ä¿å­˜å…ƒæ•°æ®ï¼ˆç”¨äº UI æ˜¾ç¤ºï¼‰
        self.last_retrieval_metadata = metadata
        
        return memories
    
    async def _call_retrieve_lightweight_api(self, query: str) -> Dict[str, Any]:
        """è°ƒç”¨ Lightweight æ£€ç´¢ APIï¼ˆä¸ test_v3_retrieve_http.py å¯¹é½ï¼‰
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            æ£€ç´¢ç»“æœå­—å…¸
        """
        # ğŸ”¥ å…³é”®ï¼šä¸ test_v3_retrieve_http.py å®Œå…¨å¯¹é½
        payload = {
            "query": query,
            "user_id": "user_001",  # ä¸ test ä¿æŒä¸€è‡´
            "top_k": self.config.top_k_memories,
            "data_source": self.data_source,  # episode / event_log
            "retrieval_mode": self.retrieval_mode,  # rrf / embedding / bm25
            "memory_scope": "all",  # æ£€ç´¢æ‰€æœ‰è®°å¿†ï¼ˆä¸ªäºº + ç¾¤ç»„ï¼‰
        }
        
        # è°ƒè¯•æ—¥å¿—ï¼ˆä»…åœ¨å¼€å‘ç¯å¢ƒæ˜¾ç¤ºï¼‰
        # print(f"\n[DEBUG] Lightweight æ£€ç´¢è¯·æ±‚:")
        # print(f"  - API URL: {self.retrieve_lightweight_url}")
        # print(f"  - query: {query}")
        # print(f"  - user_id: user_001")
        # print(f"  - retrieval_mode: {self.retrieval_mode}")
        # print(f"  - data_source: {self.data_source}")
        # print(f"  - memory_scope: all")
        # print(f"  - top_k: {self.config.top_k_memories}")
        
        try:
            # ğŸ”¥ ä¸ test_v3_retrieve_http.py å®Œå…¨ä¸€è‡´ï¼šverify=False, timeout=30.0
            async with httpx.AsyncClient(timeout=30.0, verify=False) as client:
                response = await client.post(self.retrieve_lightweight_url, json=payload)
                response.raise_for_status()
                api_response = response.json()
                
                # æ£€æŸ¥ API å“åº”çŠ¶æ€
                if api_response.get("status") == "ok":
                    result = api_response.get("result", {"memories": [], "metadata": {}})
                    # memories_count = len(result.get("memories", []))
                    # print(f"  âœ… æ£€ç´¢æˆåŠŸ: {memories_count} æ¡è®°å¿†")
                    return result
                else:
                    error_msg = api_response.get('message', 'æœªçŸ¥é”™è¯¯')
                    # print(f"  âŒ API è¿”å›é”™è¯¯: {error_msg}")
                    raise RuntimeError(f"API è¿”å›é”™è¯¯: {error_msg}")
        
        except httpx.HTTPStatusError as e:
            error_msg = f"HTTP {e.response.status_code}: {e.response.text}"
            raise RuntimeError(error_msg)
        except httpx.TimeoutException:
            error_msg = "è¯·æ±‚è¶…æ—¶ï¼ˆè¶…è¿‡30ç§’ï¼‰"
            raise RuntimeError(error_msg)
        except httpx.ConnectError as e:
            error_msg = f"è¿æ¥å¤±è´¥: æ— æ³•è¿æ¥åˆ° {self.api_base_url}\nè¯·ç¡®ä¿ V3 API æœåŠ¡å·²å¯åŠ¨: uv run python src/bootstrap.py src/run.py --port 8001"
            raise RuntimeError(error_msg) from e
        except Exception as e:
            error_msg = f"æ£€ç´¢å¤±è´¥: {type(e).__name__}: {e}"
            raise RuntimeError(error_msg)
    
    async def _call_retrieve_agentic_api(self, query: str) -> Dict[str, Any]:
        """è°ƒç”¨ Agentic æ£€ç´¢ APIï¼ˆä¸ test_v3_retrieve_http.py å¯¹é½ï¼‰
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            æ£€ç´¢ç»“æœå­—å…¸
        """
        # ğŸ”¥ å…³é”®ï¼šä¸ test_v3_retrieve_http.py å®Œå…¨å¯¹é½
        payload = {
            "query": query,
            "user_id": "user_001",  # ä¸ test ä¿æŒä¸€è‡´
            "top_k": self.config.top_k_memories,
            "time_range_days": self.config.time_range_days,  # ä½¿ç”¨é…ç½®çš„æ—¶é—´èŒƒå›´
        }
        
        # è°ƒè¯•æ—¥å¿—ï¼ˆä»…åœ¨å¼€å‘ç¯å¢ƒæ˜¾ç¤ºï¼‰
        # print(f"\n[DEBUG] Agentic æ£€ç´¢è¯·æ±‚:")
        # print(f"  - API URL: {self.retrieve_agentic_url}")
        # print(f"  - query: {query}")
        # print(f"  - user_id: user_001")
        # print(f"  - top_k: {self.config.top_k_memories}")
        # print(f"  - time_range_days: {self.config.time_range_days}")
        
        # æ˜¾ç¤ºå‹å¥½çš„ç­‰å¾…æç¤º
        print(f"\nâ³ æ­£åœ¨æ£€ç´¢è®°å¿†...")
        # print(f"   æ¶‰åŠï¼šLLM å……åˆ†æ€§åˆ¤æ–­ â†’ å¤šè½®æ£€ç´¢ â†’ ç»“æœèåˆ")
        
        try:
            # ğŸ”¥ Agentic æ£€ç´¢éœ€è¦æ›´é•¿æ—¶é—´ï¼šå¢åŠ åˆ° 180 ç§’ï¼ˆ3åˆ†é’Ÿï¼‰
            # å› ä¸ºæ¶‰åŠ LLM è°ƒç”¨ã€å……åˆ†æ€§åˆ¤æ–­ã€å¤šè½®æ£€ç´¢ç­‰å¤æ‚æ“ä½œ
            async with httpx.AsyncClient(timeout=180.0, verify=False) as client:
                response = await client.post(self.retrieve_agentic_url, json=payload)
                response.raise_for_status()
                api_response = response.json()
                
                # æ£€æŸ¥ API å“åº”çŠ¶æ€
                if api_response.get("status") == "ok":
                    result = api_response.get("result", {"memories": [], "metadata": {}})
                    # memories_count = len(result.get("memories", []))
                    # print(f"  âœ… æ£€ç´¢æˆåŠŸ: {memories_count} æ¡è®°å¿†")
                    return result
                else:
                    error_msg = api_response.get('message', 'æœªçŸ¥é”™è¯¯')
                    # print(f"  âŒ API è¿”å›é”™è¯¯: {error_msg}")
                    raise RuntimeError(f"API è¿”å›é”™è¯¯: {error_msg}")
        
        except httpx.HTTPStatusError as e:
            error_msg = f"HTTP {e.response.status_code}: {e.response.text}"
            raise RuntimeError(error_msg)
        except httpx.TimeoutException:
            error_msg = "è¯·æ±‚è¶…æ—¶ï¼ˆè¶…è¿‡180ç§’ï¼‰\næç¤ºï¼šAgentic æ£€ç´¢æ¶‰åŠ LLM è°ƒç”¨å’Œå¤šè½®æ£€ç´¢ï¼Œè€—æ—¶è¾ƒé•¿\nå»ºè®®ï¼šä½¿ç”¨ RRF/Embedding/BM25 æ£€ç´¢æ¨¡å¼ï¼ˆæ›´å¿«ï¼‰"
            raise RuntimeError(error_msg)
        except httpx.ConnectError as e:
            error_msg = f"è¿æ¥å¤±è´¥: æ— æ³•è¿æ¥åˆ° {self.api_base_url}\nè¯·ç¡®ä¿ V3 API æœåŠ¡å·²å¯åŠ¨: uv run python src/bootstrap.py src/run.py --port 8001"
            raise RuntimeError(error_msg) from e
        except Exception as e:
            error_msg = f"Agentic æ£€ç´¢å¤±è´¥: {type(e).__name__}: {e}"
            raise RuntimeError(error_msg)
    
    def build_prompt(self, user_query: str, memories: List[Dict[str, Any]]) -> List[Dict[str, str]]:
        """æ„å»º Prompt
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            memories: æ£€ç´¢åˆ°çš„è®°å¿†åˆ—è¡¨
            
        Returns:
            Chat Messages åˆ—è¡¨
        """
        messages = []
        
        # System Message
        lang_key = "zh" if self.texts.language == "zh" else "en"
        system_content = self.texts.get(f"prompt_system_role_{lang_key}")
        messages.append({"role": "system", "content": system_content})
        
        # Retrieved Memories
        if memories:
            memory_lines = []
            for i, mem in enumerate(memories, start=1):
                timestamp = mem.get("timestamp", "")[:10]
                subject = mem.get("subject", "")
                summary = mem.get("summary", "")
                episode = mem.get("episode", "")
                
                parts = [f"[{i}] {self.texts.get('prompt_memory_date', date=timestamp)}"]
                if subject:
                    parts.append(self.texts.get("prompt_memory_subject", subject=subject))
                if summary:
                    parts.append(self.texts.get("prompt_memory_content", content=summary))
                if episode:
                    parts.append(self.texts.get("prompt_memory_episode", episode=episode))
                
                memory_lines.append(" | ".join(parts))
            
            memory_content = self.texts.get("prompt_memories_prefix") + "\n".join(memory_lines)
            messages.append({"role": "system", "content": memory_content})
        
        # Conversation History
        for user_q, assistant_a in self.conversation_history[-self.config.conversation_history_size:]:
            messages.append({"role": "user", "content": user_q})
            messages.append({"role": "assistant", "content": assistant_a})
        
        # Current Question
        messages.append({"role": "user", "content": user_query})
        
        return messages
    
    async def chat(self, user_input: str) -> str:
        """æ ¸å¿ƒå¯¹è¯é€»è¾‘
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            
        Returns:
            åŠ©æ‰‹å›ç­”
        """
        from .ui import ChatUI
        
        # æ£€ç´¢è®°å¿†
        memories = await self.retrieve_memories(user_input)
        
        # æ˜¾ç¤ºæ£€ç´¢ç»“æœ
        if self.config.show_retrieved_memories and memories:
            ChatUI.print_retrieved_memories(
                memories[:5],
                texts=self.texts,
                retrieval_metadata=self.last_retrieval_metadata,
            )
        
        # æ„å»º Prompt
        messages = self.build_prompt(user_input, memories)
        
        # æ˜¾ç¤ºç”Ÿæˆè¿›åº¦
        ChatUI.print_generating_indicator(self.texts)
        
        # è°ƒç”¨ LLM
        try:
            if hasattr(self.llm_provider, 'provider') and hasattr(
                self.llm_provider.provider, 'chat_with_messages'
            ):
                raw_response = await self.llm_provider.provider.chat_with_messages(messages)
            else:
                prompt_parts = []
                for msg in messages:
                    role = msg["role"]
                    content = msg["content"]
                    if role == "system":
                        prompt_parts.append(f"System: {content}")
                    elif role == "user":
                        prompt_parts.append(f"User: {content}")
                    elif role == "assistant":
                        prompt_parts.append(f"Assistant: {content}")
                
                prompt = "\n\n".join(prompt_parts)
                raw_response = await self.llm_provider.generate(prompt)
            
            raw_response = raw_response.strip()
            
            # æ¸…é™¤ç”Ÿæˆè¿›åº¦
            ChatUI.print_generation_complete(self.texts)
            
            assistant_response = raw_response
        
        except Exception as e:
            ChatUI.clear_progress_indicator()
            error_msg = f"[{self.texts.get('error_label')}] {self.texts.get('chat_llm_error', error=str(e))}"
            print(f"\n{error_msg}")
            import traceback
            traceback.print_exc()
            return error_msg
        
        # æ›´æ–°å¯¹è¯å†å²
        self.conversation_history.append((user_input, assistant_response))
        
        if len(self.conversation_history) > self.config.conversation_history_size:
            self.conversation_history = self.conversation_history[-self.config.conversation_history_size:]
        
        return assistant_response
    
    def clear_history(self) -> None:
        """æ¸…ç©ºå¯¹è¯å†å²"""
        from .ui import ChatUI
        count = len(self.conversation_history)
        self.conversation_history = []
        ChatUI.print_info(self.texts.get("cmd_clear_done", count=count), self.texts)
    
    async def reload_data(self) -> None:
        """é‡æ–°åŠ è½½è®°å¿†æ•°æ®"""
        from .ui import ChatUI
        from common_utils.cli_ui import CLIUI
        
        display_name = "group_chat" if self.group_id == "AIäº§å“ç¾¤" else self.group_id
        
        ui = CLIUI()
        print()
        ui.note(self.texts.get("cmd_reload_refreshing", name=display_name), icon="ğŸ”„")
        
        # é‡æ–°ç»Ÿè®¡ MemCell æ•°é‡
        now = get_now_with_timezone()
        start_date = now - timedelta(days=self.config.time_range_days)
        memcells = await query_memcells_by_group_and_time(self.group_id, start_date, now)
        self.memcell_count = len(memcells)
        
        print()
        ui.success(f"âœ“ {self.texts.get('cmd_reload_complete', users=0, memories=self.memcell_count)}")
        print()

