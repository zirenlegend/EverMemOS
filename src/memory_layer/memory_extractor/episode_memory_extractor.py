"""
Simple Memory Extraction Base Class for EverMemOS

This module provides a simple base class for extracting memories
from boundary detection results (BoundaryResult).
"""

from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any
from datetime import datetime
import re, json, asyncio, uuid


# ä½¿ç”¨åŠ¨æ€è¯­è¨€æç¤ºè¯å¯¼å…¥ï¼ˆæ ¹æ® MEMORY_LANGUAGE ç¯å¢ƒå˜é‡è‡ªåŠ¨é€‰æ‹©ï¼‰
from ..prompts import (
    EPISODE_GENERATION_PROMPT,
    GROUP_EPISODE_GENERATION_PROMPT,
    DEFAULT_CUSTOM_INSTRUCTIONS,
)

# è¯„ä¼°ä¸“ç”¨æç¤ºè¯
from ..prompts.eval.episode_mem_prompts import (
    EPISODE_GENERATION_PROMPT as EVAL_EPISODE_GENERATION_PROMPT,
    GROUP_EPISODE_GENERATION_PROMPT as EVAL_GROUP_EPISODE_GENERATION_PROMPT,
    DEFAULT_CUSTOM_INSTRUCTIONS as EVAL_DEFAULT_CUSTOM_INSTRUCTIONS,
)


from ..llm.llm_provider import LLMProvider

from .base_memory_extractor import MemoryExtractor, MemoryExtractRequest
from .semantic_memory_extractor import SemanticMemoryExtractor
from ..types import MemoryType, Memory, RawDataType, MemCell

from common_utils.datetime_utils import get_now_with_timezone

from core.observation.logger import get_logger

logger = get_logger(__name__)


@dataclass
class EpisodeMemory(Memory):
    """
    Simple result class for memory extraction.

    Contains the essential information for extracted memories.
    """

    event_id: str = field(default=None)

    def __post_init__(self):
        """Set memory_type to EPISODE_MEMORY and call parent __post_init__."""
        self.memory_type = MemoryType.EPISODE_MEMORY
        super().__post_init__()


@dataclass
class EpisodeMemoryExtractRequest(MemoryExtractRequest):
    pass


class EpisodeMemoryExtractor(MemoryExtractor):
    def __init__(
        self, llm_provider: LLMProvider | None = None, use_eval_prompts: bool = False
    ):
        super().__init__(MemoryType.EPISODE_MEMORY)
        self.llm_provider = llm_provider
        self.semantic_extractor = SemanticMemoryExtractor(self.llm_provider)
        self.use_eval_prompts = use_eval_prompts
        if self.use_eval_prompts:
            self.episode_generation_prompt = EVAL_EPISODE_GENERATION_PROMPT
            self.group_episode_generation_prompt = EVAL_GROUP_EPISODE_GENERATION_PROMPT
            self.default_custom_instructions = EVAL_DEFAULT_CUSTOM_INSTRUCTIONS
        else:
            self.episode_generation_prompt = EPISODE_GENERATION_PROMPT
            self.group_episode_generation_prompt = GROUP_EPISODE_GENERATION_PROMPT
            self.default_custom_instructions = DEFAULT_CUSTOM_INSTRUCTIONS

    def _parse_timestamp(self, timestamp) -> datetime:
        """
        è§£ææ—¶é—´æˆ³ä¸º datetime å¯¹è±¡
        æ”¯æŒå¤šç§æ ¼å¼ï¼šæ•°å­—æ—¶é—´æˆ³ã€ISOæ ¼å¼å­—ç¬¦ä¸²ã€æ•°å­—å­—ç¬¦ä¸²ç­‰
        """
        if isinstance(timestamp, datetime):
            return timestamp
        elif isinstance(timestamp, (int, float)):
            return datetime.fromtimestamp(timestamp)
        elif isinstance(timestamp, str):
            # Handle string timestamps (could be ISO format or timestamp string)
            try:
                if timestamp.isdigit():
                    return datetime.fromtimestamp(int(timestamp))
                else:
                    # Try parsing as ISO format
                    return datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
            except (ValueError, AttributeError):
                # Fallback to current time if parsing fails
                logger.error(f"è§£ææ—¶é—´æˆ³å¤±è´¥: {timestamp}")
                return get_now_with_timezone()
        else:
            # Unknown format, fallback to current time
            logger.error(f"è§£ææ—¶é—´æˆ³å¤±è´¥: {timestamp}")
            return get_now_with_timezone()

    def _format_timestamp(self, dt: datetime) -> str:
        """
        æ ¼å¼åŒ– datetime ä¸ºæ˜“è¯»çš„å­—ç¬¦ä¸²æ ¼å¼
        """
        weekday = dt.strftime("%A")  # Monday, Tuesday, etc.
        month_day = dt.strftime("%B %d, %Y")  # March 14, 2024
        time_of_day = dt.strftime("%I:%M %p")  # 3:00 PM
        return f"{month_day} ({weekday}) at {time_of_day} UTC"

    def get_conversation_text(self, data_list):
        lines = []
        for data in data_list:
            # Handle both RawData objects and dict objects
            if hasattr(data, 'content'):
                # RawData object
                speaker = data.content.get('speaker_name') or data.content.get(
                    'sender', 'Unknown'
                )
                content = data.content['content']
                timestamp = data.content['timestamp']
            else:
                # Dict object
                speaker = data.get('speaker_name') or data.get('sender', 'Unknown')
                content = data['content']
                timestamp = data['timestamp']

            if timestamp:
                lines.append(f"[{timestamp}] {speaker}: {content}")
            else:
                lines.append(f"{speaker}: {content}")
        return "\n".join(lines)

    def get_conversation_json_text(self, data_list):
        lines = []
        for data in data_list:
            # Handle both RawData objects and dict objects
            if hasattr(data, 'content'):
                # RawData object
                speaker = data.content.get('speaker_name') or data.content.get(
                    'sender', 'Unknown'
                )
                content = data.content['content']
                timestamp = data.content['timestamp']
            else:
                # Dict object
                speaker = data.get('speaker_name') or data.get('sender', 'Unknown')
                content = data['content']
                timestamp = data['timestamp']

            if timestamp:
                lines.append(
                    f"""
                {{
                    "timestamp": {timestamp},
                    "speaker": {speaker},
                    "content": {content}
                }}"""
                )
            else:
                lines.append(
                    f"""
                {{
                    "speaker": {speaker},
                    "content": {content}
                }}"""
                )
        return "\n".join(lines)

    def get_speaker_name_map(self, data_list: List[Dict[str, Any]]) -> Dict[str, str]:
        speaker_name_map = {}
        for data in data_list:
            if hasattr(data, 'content'):
                speaker_name_map[data.content.get('speaker_id')] = data.content.get(
                    'speaker_name'
                )
            else:
                speaker_name_map[data.get('speaker_id')] = data.get('speaker_name')
        return speaker_name_map

    def _extract_participant_name_map(
        self, chat_raw_data_list: List[Dict[str, Any]]
    ) -> List[str]:
        participant_name_map = {}
        for raw_data in chat_raw_data_list:
            if 'speaker_name' in raw_data and raw_data['speaker_name']:
                participant_name_map[raw_data['speaker_id']] = raw_data['speaker_name']
            if 'referList' in raw_data and raw_data['referList']:
                for refer_item in raw_data['referList']:
                    if isinstance(refer_item, dict):
                        if 'name' in refer_item and refer_item['_id']:
                            participant_name_map[refer_item['_id']] = refer_item['name']
        return participant_name_map

    async def _trigger_semantic_extraction_async(
        self,
        episode_memories: List[EpisodeMemory],
        request: EpisodeMemoryExtractRequest,
    ):
        """
        å¼‚æ­¥è§¦å‘è¯­ä¹‰è®°å¿†æå–ï¼Œä¸å½±å“ä¸»æµç¨‹
        ä½¿ç”¨å¹¶å‘æ–¹å¼å¤„ç†å¤šä¸ªepisodeçš„è¯­ä¹‰è®°å¿†æå–
        """
        if not self.semantic_extractor:
            logger.debug("è¯­ä¹‰è®°å¿†æå–å™¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡è¯­ä¹‰è®°å¿†æå–")
            return

        logger.info("å¼‚æ­¥è§¦å‘è¯­ä¹‰è®°å¿†æå–...")

        # å®šä¹‰å•ä¸ªepisodeçš„è¯­ä¹‰è®°å¿†æå–å‡½æ•°
        async def extract_semantic_for_episode(episode_memory: EpisodeMemory):
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    logger.debug(
                        f"ğŸ§  è‡ªåŠ¨è§¦å‘è¯­ä¹‰è®°å¿†æå–å¼€å§‹: episode_memory='{episode_memory.subject}' (å°è¯• {attempt + 1}/{max_retries})"
                    )
                    semantic_memories = await self.semantic_extractor.generate_semantic_memories_for_episode(
                        episode_memory
                    )
                    episode_memory.semantic_memories = semantic_memories
                    logger.info(
                        f"âœ… ä¸ºæƒ…æ™¯è®°å¿† '{episode_memory.subject}' ç”Ÿæˆäº† {len(semantic_memories)} æ¡è¯­ä¹‰è®°å¿†"
                    )
                    return True  # æˆåŠŸ
                except Exception as e:
                    logger.error(
                        f"âŒ ä¸ºæƒ…æ™¯è®°å¿† '{episode_memory.subject}' ç”Ÿæˆè¯­ä¹‰è®°å¿†æ—¶å‡ºé”™: {e} (å°è¯• {attempt + 1}/{max_retries})"
                    )

                    if attempt < max_retries - 1:
                        await asyncio.sleep(0.5)
                    else:
                        logger.error(
                            f"âŒ æ‰€æœ‰é‡è¯•æ¬¡æ•°å‡å¤±è´¥ï¼Œæœªèƒ½ä¸ºæƒ…æ™¯è®°å¿† '{episode_memory.subject}' æå–è¯­ä¹‰è®°å¿†"
                        )
                        return False  # å¤±è´¥
            return False

        # å¹¶å‘å¤„ç†æ‰€æœ‰episodeçš„è¯­ä¹‰è®°å¿†æå–
        tasks = [
            extract_semantic_for_episode(episode_memory)
            for episode_memory in episode_memories
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for result in results if result is True)
        logger.info(
            f"è¯­ä¹‰è®°å¿†æå–å®Œæˆ: {success_count}/{len(episode_memories)} ä¸ªepisodeæˆåŠŸ"
        )

    async def _trigger_semantic_extraction_for_memcell_async(
        self, memcell: MemCell, request: EpisodeMemoryExtractRequest
    ):
        """
        å¼‚æ­¥ä¸ºMemCellè§¦å‘è¯­ä¹‰è®°å¿†æå–ï¼Œä¸å½±å“ä¸»æµç¨‹
        """
        if not self.semantic_extractor:
            logger.debug("è¯­ä¹‰è®°å¿†æå–å™¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡è¯­ä¹‰è®°å¿†æå–")
            return

        max_retries = 3
        for attempt in range(max_retries):
            try:
                logger.debug(
                    f"ğŸ§  è‡ªåŠ¨è§¦å‘è¯­ä¹‰è®°å¿†æå–å¼€å§‹: memcell='{memcell.subject}' (å°è¯• {attempt + 1}/{max_retries})"
                )
                semantic_memories = await self.semantic_extractor.generate_semantic_memories_for_memcell(
                    memcell
                )
                memcell.semantic_memories = semantic_memories
                logger.info(
                    f"âœ… ä¸ºMemCell '{memcell.subject}' ç”Ÿæˆäº† {len(semantic_memories)} æ¡è¯­ä¹‰è®°å¿†"
                )
                break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯
            except Exception as e:
                logger.error(
                    f"âŒ ä¸ºMemCell '{memcell.subject}' ç”Ÿæˆè¯­ä¹‰è®°å¿†æ—¶å‡ºé”™: {e} (å°è¯• {attempt + 1}/{max_retries})"
                )

                if attempt < max_retries - 1:
                    await asyncio.sleep(0.5)
                else:
                    logger.error(
                        f"âŒ æ‰€æœ‰é‡è¯•æ¬¡æ•°å‡å¤±è´¥ï¼Œæœªèƒ½ä¸ºMemCell '{memcell.subject}' æå–è¯­ä¹‰è®°å¿†"
                    )

    async def extract_memory(
        self,
        request: EpisodeMemoryExtractRequest,
        use_group_prompt: bool = False,
        use_semantic_extraction: bool = False,
    ) -> Optional[List[EpisodeMemory]] | Optional[MemCell]:
        logger.debug(f"ğŸ“š è‡ªåŠ¨è§¦å‘æƒ…æ™¯è®°å¿†æå–...")

        if not request.memcell_list:
            return None

        # è·å–ç¬¬ä¸€ä¸ª memcell æ¥åˆ¤æ–­ç±»å‹
        first_memcell = request.memcell_list[0]

        # æ ¹æ®ç±»å‹é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼
        if first_memcell.type == RawDataType.CONVERSATION:
            all_content_text = []
            prompt_template = ""
            # å¯¹è¯ç±»å‹å¤„ç†
            for memcell in request.memcell_list:
                # conversation_text = self.get_conversation_text(memcell.original_data)
                conversation_text = self.get_conversation_json_text(
                    memcell.original_data
                )
                all_content_text.append(conversation_text)

            # æ ¹æ®ä½¿ç”¨åœºæ™¯é€‰æ‹©æç¤ºè¯
            if use_group_prompt:
                # ä¸ extract_memcell é…å¥—ä½¿ç”¨
                prompt_template = self.group_episode_generation_prompt
                content_key = "conversation"
                time_key = "conversation_start_time"
            else:
                # å•ç‹¬ä½¿ç”¨
                prompt_template = self.episode_generation_prompt
                content_key = "conversation"
                time_key = "conversation_start_time"
            default_title = "Conversation Episode"
        else:
            pass

        # Extract earliest timestamp for context
        start_time = self._parse_timestamp(first_memcell.timestamp)
        start_time_str = self._format_timestamp(start_time)

        # Combine all content texts
        combined_content = "\n\n".join(all_content_text)

        # æ„å»º prompt
        if use_group_prompt:
            for i in range(5):
                try:
                    format_params = {
                        time_key: start_time_str,
                        content_key: combined_content,
                        "custom_instructions": self.default_custom_instructions,
                    }
                    prompt = prompt_template.format(**format_params)
                    response = await self.llm_provider.generate(prompt)
                    # é¦–å…ˆå°è¯•æå–ä»£ç å—ä¸­çš„JSON
                    if '```json' in response:
                        # æå–ä»£ç å—ä¸­çš„JSONå†…å®¹
                        start = response.find('```json') + 7
                        end = response.find('```', start)
                        if end > start:
                            json_str = response[start:end].strip()
                            data = json.loads(json_str)
                        else:
                            # å°è¯•è§£ææ•´ä¸ªå“åº”ä¸ºJSON
                            data = json.loads(response)
                    else:
                        # å°è¯•åŒ¹é…åŒ…å«titleå’Œcontentçš„JSONå¯¹è±¡
                        json_match = re.search(
                            r'\{[^{}]*"title"[^{}]*"content"[^{}]*\}',
                            response,
                            re.DOTALL,
                        )
                        if json_match:
                            data = json.loads(json_match.group())
                        else:
                            # å°è¯•è§£ææ•´ä¸ªå“åº”ä¸ºJSON
                            data = json.loads(response)
                    break
                except Exception as e:
                    print('retry: ', i)
                    if i == 4:
                        raise Exception("Episode memory extraction failed")
                    continue
            # Ensure we have required fields with fallback defaults
            if "title" not in data:
                data["title"] = default_title
            if "content" not in data:
                data["content"] = combined_content
            if "summary" not in data:
                # Generate a basic summary from content if not provided
                data["summary"] = data["content"]

            title = data["title"]
            content = data["content"]
            summary = data["summary"]

            # GROUP_EPISODE_GENERATION_PROMPT æ¨¡å¼ï¼šå°†æƒ…æ™¯è®°å¿†å­˜å‚¨åˆ° MemCell ä¸­ï¼Œè¿”å› MemCell
            # æ›´æ–° MemCell çš„ episode å­—æ®µ
            for memcell in request.memcell_list:
                memcell.subject = title
                memcell.episode = content

            if use_semantic_extraction:
                await self._trigger_semantic_extraction_for_memcell_async(
                    first_memcell, request
                )

            # è¿”å›ç¬¬ä¸€ä¸ª MemCellï¼ˆå·²ç»åŒ…å«äº†æƒ…æ™¯è®°å¿†å†…å®¹ï¼‰
            return first_memcell
        else:
            format_params = {
                time_key: start_time_str,
                content_key: combined_content,
                "custom_instructions": self.default_custom_instructions,
            }

            participants = []
            [
                participants.extend(memcell.participants)
                for memcell in request.memcell_list
            ]
            if not participants:
                participants = request.participants
            if not participants:
                participants = []

            all_memories = []
            if participants:
                all_original_data = []
                [
                    all_original_data.extend(memcell.original_data)
                    for memcell in request.memcell_list
                ]
                participants_name_map = self.get_speaker_name_map(all_original_data)
                [
                    participants_name_map.update(
                        self._extract_participant_name_map(memcell.original_data)
                    )
                    for memcell in request.memcell_list
                ]

                # å¹¶å‘ç”Ÿæˆæ¯ä¸ªå‚ä¸è€…çš„episode memory
                async def generate_memory_for_user(
                    user_id: str, user_name: str
                ) -> EpisodeMemory:
                    user_format_params = format_params.copy()
                    user_format_params["user_name"] = user_name
                    prompt = prompt_template.format(**user_format_params)
                    response = await self.llm_provider.generate(prompt)

                    # é¦–å…ˆå°è¯•æå–ä»£ç å—ä¸­çš„JSON
                    if '```json' in response:
                        # æå–ä»£ç å—ä¸­çš„JSONå†…å®¹
                        start = response.find('```json') + 7
                        end = response.find('```', start)
                        if end > start:
                            json_str = response[start:end].strip()
                            data = json.loads(json_str)
                        else:
                            # å°è¯•è§£ææ•´ä¸ªå“åº”ä¸ºJSON
                            data = json.loads(response)
                    else:
                        # å°è¯•åŒ¹é…åŒ…å«titleå’Œcontentçš„JSONå¯¹è±¡
                        json_match = re.search(
                            r'\{[^{}]*"title"[^{}]*"content"[^{}]*\}',
                            response,
                            re.DOTALL,
                        )
                        if json_match:
                            data = json.loads(json_match.group())
                        else:
                            # å°è¯•è§£ææ•´ä¸ªå“åº”ä¸ºJSON
                            data = json.loads(response)

                    # Ensure we have required fields with fallback defaults
                    if "title" not in data:
                        data["title"] = default_title
                    if "content" not in data:
                        data["content"] = combined_content
                    if "summary" not in data:
                        # Generate a basic summary from content if not provided
                        data["summary"] = "\n".join(
                            [memcell.summary for memcell in request.memcell_list]
                        )

                    title = data["title"]
                    content = data["content"]
                    summary = data["summary"]

                    return EpisodeMemory(
                        memory_type=MemoryType.EPISODE_MEMORY,
                        user_id=user_id,
                        ori_event_id_list=[
                            memcell.event_id for memcell in request.memcell_list
                        ],
                        timestamp=start_time,
                        subject=title,
                        summary=summary,
                        episode=content,
                        group_id=request.group_id,
                        participants=participants,
                        type=getattr(first_memcell, 'type', None),
                        memcell_event_id_list=[
                            memcell.event_id for memcell in request.memcell_list
                        ],
                    )

                # å¹¶å‘æ‰§è¡Œæ‰€æœ‰å‚ä¸è€…çš„memoryç”Ÿæˆ
                participant_memories = await asyncio.gather(
                    *[
                        generate_memory_for_user(
                            user_id, participants_name_map.get(user_id, user_id)
                        )
                        for user_id in participants
                    ],
                    return_exceptions=True,
                )

                # å¤„ç†ç»“æœï¼Œè¿‡æ»¤æ‰å¼‚å¸¸
                for memory in participant_memories:
                    if isinstance(memory, EpisodeMemory):
                        all_memories.append(memory)
                    else:
                        print(
                            f"[EpisodicMemoryExtractor] Error generating memory: {memory}"
                        )

            for user_id in request.user_id_list:
                if user_id not in participants:
                    memory = EpisodeMemory(
                        memory_type=MemoryType.EPISODE_MEMORY,
                        user_id=user_id,
                        ori_event_id_list=[
                            memcell.event_id for memcell in request.memcell_list
                        ],
                        timestamp=start_time,
                        subject=title,
                        summary="\n".join(
                            [memcell.summary for memcell in request.memcell_list]
                        ),
                        episode="\n".join(
                            [memcell.episode for memcell in request.memcell_list]
                        ),
                        group_id=request.group_id,
                        participants=participants,
                        type=getattr(first_memcell, 'type', None),
                        memcell_event_id_list=[
                            memcell.event_id for memcell in request.memcell_list
                        ],
                    )
                    all_memories.append(memory)
            # å¼‚æ­¥è§¦å‘è¯­ä¹‰è®°å¿†æå–ï¼Œä¸å½±å“ä¸»æµç¨‹
            # if all_memories:
            #     asyncio.create_task(self._trigger_semantic_extraction_async(all_memories, request))
            if use_semantic_extraction:
                await self._trigger_semantic_extraction_async(all_memories, request)
            return all_memories
