<div align="center">

<h1>EverMemOS ğŸ§ </h1>

<p><strong>Let every interaction be driven by understanding.</strong> Â· Enterprise-Grade Intelligent Memory System</p>

<p>
  <img alt="Python" src="https://img.shields.io/badge/Python-3.10+-3776AB?style=flat-square&logo=python&logoColor=white" />
  <img alt="License" src="https://img.shields.io/badge/License-Apache%202.0-D22128?style=flat-square&logo=apache&logoColor=white" />
  <img alt="Docker" src="https://img.shields.io/badge/Docker-Supported-2496ED?style=flat-square&logo=docker&logoColor=white" />
  <img alt="FastAPI" src="https://img.shields.io/badge/FastAPI-Latest-009688?style=flat-square&logo=fastapi&logoColor=white" />
  <img alt="MongoDB" src="https://img.shields.io/badge/MongoDB-7.0+-47A248?style=flat-square&logo=mongodb&logoColor=white" />
  <img alt="Elasticsearch" src="https://img.shields.io/badge/Elasticsearch-8.x-005571?style=flat-square&logo=elasticsearch&logoColor=white" />
  <img alt="Milvus" src="https://img.shields.io/badge/Milvus-2.4+-00A3E0?style=flat-square" />
  <img alt="Redis" src="https://img.shields.io/badge/Redis-7.x-DC382D?style=flat-square&logo=redis&logoColor=white" />
   <a href="https://github.com/EverMind-AI/EverMemOS/releases">
    <img alt="Release" src="https://img.shields.io/badge/release-v1.0.0-4A90E2?style=flat-square" />
  </a>
</p>

<p>
  <a href="README.md">English</a> | <a href="README_zh.md">ç®€ä½“ä¸­æ–‡</a>
</p>

</div>

---

> ğŸ’¬ **More than memory â€” it's foresight.**

**EverMemOS** is a forward-thinking **intelligent system**.  
While traditional AI memory serves merely as a "look-back" database, EverMemOS enables AI not only to "remember" what happened, but also to "understand" the meaning behind these memories and use them to guide current actions and decisions. In the EverMemOS demo tools, you can see how EverMemOS extracts important information from your history, and then remembers your preferences, habits, and history during conversations, just like a **friend** who truly knows you.
On the **LoCoMo** benchmark, our approach built upon EverMemOS achieved a reasoning accuracy of **92.3%** (evaluated by LLM-Judge), outperforming comparable methods in our evaluation.

---

## ğŸ“¢ Latest Updates

<table>
<tr>
<td width="100%" style="border: none;">

**[2025-11-02] ğŸ‰ ğŸ‰ ğŸ‰ EverMemOS v1.0.0 Released!**

- âœ¨ **Stable Version**: AI Memory System officially open sourced  
- ğŸ“š **Complete Documentation**: Quick start guide and comprehensive API documentation  
- ğŸ“ˆ **Benchmark Testing**: LoCoMo dataset benchmark evaluation pipeline
- ğŸ–¥ï¸ **Demo Tools**: Get started quickly with easy-to-use demos

</td>
</tr>
</table>

---

## ğŸ¯ Core Vision  
Build AI memory that never forgets, making every conversation built on previous understanding.

---

## ğŸ’¡ Unique Advantages

### ğŸ”— Coherent Narrative  
Beyond "fragments," connecting "stories": Automatically linking conversation pieces to build clear thematic context, enabling AI to "truly understand."

> When facing multi-threaded conversations, it naturally distinguishes between "Project A progress discussion" and "Team B strategy planning," maintaining coherent contextual logic within each theme.  
> From scattered phrases to complete narratives, AI no longer just "understands one sentence" but "understands the whole story."

---

### ğŸ§  Evidence-Based Perception  
Beyond "retrieval," intelligent "perception": Proactively capturing deep connections between memories and tasks, enabling AI to "think thoroughly" at critical moments.

> Imagine: When a user asks for "food recommendations," the AI proactively recalls "you had dental surgery two days ago" as a key piece of information, automatically adjusting suggestions to avoid unsuitable options.  
> This is **Contextual Awareness** â€” enabling AI thinking to be truly built on understanding rather than isolated responses.

---

### ğŸ’¾ Living Profiles  
Beyond "records," dynamic "growth": Real-time user profile updates that get to know you better with each conversation, enabling AI to "recognize you authentically."

> Every interaction subtly updates the AI's understanding of you â€” preferences, style, and focus points all continuously evolve.  
> As interactions deepen, it doesn't just "remember what you said," but is "learning who you are."

---

<h2>ğŸ“‘ Table of Contents</h2>


<div align="center">
<table>
<tr>
<td width="50%" valign="top">

 - [ğŸ“– Project Introduction](#-project-introduction)
 - [ğŸ¯ System Framework](#-system-framework)
 - [ğŸ“ Project Structure](#-project-structure)
- [ğŸš€ Quick Start](#-quick-start)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation)
  - [How to Use](#how-to-use)
  - [More Details](#more-details)

</td>
<td width="50%" valign="top">

- [ğŸ“š Documentation](#-documentation)
  - [Developer Docs](#developer-docs)
  - [API Documentation](#api-documentation)
  - [Core Framework](#core-framework)
- [ğŸ—ï¸ Architecture Design](#ï¸-architecture-design)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸŒŸ Join Us](#-join-us)
- [ğŸ™ Acknowledgments](#-acknowledgments)

</td>
</tr>
</table>
</div>

---

## ğŸ“– Project Introduction

**EverMemOS** is an open-source project designed to provide long-term memory capabilities to conversational AI agents. This codebase is the official implementation of the paper "EverMemOS". It extracts, structures, and retrieves information from conversations, enabling agents to maintain context, recall past interactions, and progressively build user profiles. This results in more personalized, coherent, and intelligent conversations.

## ğŸ¯ System Framework

EverMemOS operates along two main tracks: **memory construction** and **memory perception**. Together they form a cognitive loop that continuously absorbs, consolidates, and applies past information, so every response is grounded in real context and long-term memory.

<p align="center">
  <img src="figs/overview.png" alt="Overview" />
</p>

### ğŸ§© Memory Construction

Memory construction layer: builds structured, retrievable long-term memory from raw conversation data.

- **Core elements**
  - âš›ï¸ **Atomic memory unit MemCell**: the core structured unit distilled from conversations for downstream organization and reference
  - ğŸ—‚ï¸ **Multi-level memory**: integrate related fragments by theme and storyline to form reusable, hierarchical memories
  - ğŸ·ï¸ **Multiple memory types**: covering episodes, profiles, preferences, relationships, semantic knowledge, basic facts, and core memories

- **Workflow**
  1. **MemCell extraction**: identify key information in conversations to generate atomic memories
  2. **Memory construction**: integrate by theme and participants to form episodes and profiles
  3. **Storage and indexing**: persist data and build keyword and semantic indexes to support fast recall

### ğŸ” Memory Perception

Memory perception layer: quickly recalls relevant memories through multi-round reasoning and intelligent fusion, achieving precise contextual awareness.

#### ğŸ¯ Intelligent Retrieval Tools

- **ğŸ§ª Hybrid Retrieval (RRF Fusion)**  
  Parallel execution of semantic and keyword retrieval, seamlessly fused using Reciprocal Rank Fusion algorithm

- **ğŸ“Š Intelligent Reranking (Reranker)**  
  Batch concurrent processing with exponential backoff retry, maintaining stability under high throughput  
  Reorders candidate memories by deep relevance, prioritizing the most critical information

#### ğŸ¤– Agentic Intelligent Retrieval

- **ğŸ“ LLM-Guided Multi-Round Recall**  
  For insufficient cases, generate 2-3 complementary queries, retrieve and fuse in parallel
  Automatically identifies missing information, proactively filling retrieval blind spots

- **ğŸ”€ Multi-Query Parallel Strategy**  
  When a single query cannot fully express intent, generate multiple complementary perspective queries  
  Enhance coverage of complex intents through multi-path RRF fusion

- **âš¡ Lightweight Fast Mode**  
  For latency-sensitive scenarios, skip LLM calls and use RRF-fused hybrid retrieval  
  Flexibly balance between speed and quality

#### ğŸ§  Reasoning Fusion

- **Context Integration**: Concatenate recalled multi-level memories (episodes, profiles, preferences) with current conversation
- **Traceable Reasoning**: Model generates responses based on explicit memory evidence, avoiding hallucination

ğŸ’¡ Through the cognitive loop of **"Structured Memory â†’ Multi-Strategy Recall â†’ Intelligent Retrieval â†’ Contextual Reasoning"**, the AI always "thinks with memory", achieving true contextual awareness.

## ğŸ“ Project Structure

<details>
<summary>Expand/Collapse Directory Structure</summary>

```
memsys-opensource/
â”œâ”€â”€ src/                              # Source code directory
â”‚   â”œâ”€â”€ agentic_layer/                # Agentic layer - unified memory interface
â”‚   â”œâ”€â”€ memory_layer/                 # Memory layer - memory extraction
â”‚   â”‚   â”œâ”€â”€ memcell_extractor/        # MemCell extractor
â”‚   â”‚   â”œâ”€â”€ memory_extractor/         # Memory extractor
â”‚   â”‚   â””â”€â”€ prompts/                  # LLM prompt templates
â”‚   â”œâ”€â”€ retrieval_layer/              # Retrieval layer - memory retrieval
â”‚   â”œâ”€â”€ biz_layer/                    # Business layer - business logic
â”‚   â”œâ”€â”€ infra_layer/                  # Infrastructure layer
â”‚   â”œâ”€â”€ core/                         # Core functionality (DI/lifecycle/middleware)
â”‚   â”œâ”€â”€ component/                    # Components (LLM adapters, etc.)
â”‚   â””â”€â”€ common_utils/                 # Common utilities
â”œâ”€â”€ demo/                             # Demo code
â”œâ”€â”€ data/                             # Sample conversation data
â”œâ”€â”€ evaluation/                       # Evaluation scripts
â”‚   â””â”€â”€ locomo_evaluation/            # LoCoMo benchmark testing
â”œâ”€â”€ data_format/                      # Data format definitions
â”œâ”€â”€ docs/                             # Documentation
â”œâ”€â”€ config.json                       # Configuration file
â”œâ”€â”€ env.template                      # Environment variable template
â”œâ”€â”€ pyproject.toml                    # Project configuration
â””â”€â”€ README.md                         # Project description
```

</details>

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- uv (recommended package manager)
- Docker and Docker Compose

### Installation

#### Using Docker for Dependency Services â­

Use Docker Compose to start all dependency services (MongoDB, Elasticsearch, Milvus, Redis) with one command:

```bash
# 1. Clone the repository
git clone https://github.com/your-org/memsys_opensource.git
cd memsys_opensource

# 2. Start Docker services
docker-compose up -d

# 3. Verify service status
docker-compose ps

# 4. Install uv (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# 5. Install project dependencies
uv sync

# 6. Configure environment variables
cp env.template .env
# Edit the .env file and fill in the necessary configurations:
#   - LLM_API_KEY: Enter your LLM API Key (for memory extraction)
#   - DEEPINFRA_API_KEY: Enter your DeepInfra API Key (for Embedding and Rerank)
```

**Docker Services**:
- **MongoDB** (27017): Primary database for storing memory cells and profiles
- **Elasticsearch** (19200): Keyword search engine (BM25)
- **Milvus** (19530): Vector database for semantic retrieval
- **Redis** (6479): Cache service

> ğŸ’¡ For detailed Docker configuration and management, see [Docker Deployment Guide](DOCKER_DEPLOYMENT.md)

> ğŸ“– MongoDB detailed installation guide: [MongoDB Installation Guide](docs/usage/MONGODB_GUIDE.md)

---

### How to Use

EverMemOS offers multiple usage methods. Choose the one that best suits your needs:

---

#### ğŸ¯ Run Demo: Memory Extraction and Interactive Chat

The demo showcases the end-to-end functionality of EverMemOS.

---

**ğŸš€ Quick Start: Simple Demo (Recommended)** â­

The fastest way to experience EverMemOS! Just 2 steps to see memory storage and retrieval in action:

```bash
# Step 1: Start the API server (in terminal 1)
uv run python src/bootstrap.py start_server.py

# Step 2: Run the simple demo (in terminal 2)
uv run python src/bootstrap.py demo/simple_demo.py
```

**What it does:**
- Stores 4 conversation messages about sports hobbies
- Waits 10 seconds for indexing
- Searches for relevant memories with 3 different queries
- Shows complete workflow with friendly explanations

**Perfect for:** First-time users, quick testing, understanding core concepts

See the demo code at [`demo/simple_demo.py`](demo/simple_demo.py)

---

We also provide a full-featured experience:

**Prerequisites: Start the API Server**

```bash
# Terminal 1: Start the API server (required)
uv run python src/bootstrap.py start_server.py
```

> ğŸ’¡ **Tip**: Keep the API server running throughout. All following operations should be performed in another terminal.

---

**Step 1: Extract Memories**

Run the memory extraction script to process sample conversation data and build the memory database:

```bash
# Terminal 2: Run the extraction script
uv run python src/bootstrap.py demo/extract_memory.py
```

This script will:
- Read conversation data from the `data/` directory
- Extract MemCells and save them to the configured database (e.g., MongoDB)
- Generate user profiles and save them to `demo/memcell_outputs/` directory

> **ğŸ’¡ Tip**:
> By default, the script extracts memories for the **ASSISTANT** scenario. You can optionally extract memories for the **GROUP_CHAT** scenario:
> 1. Open the `demo/extract_memory.py` file.
> 2. Locate the `EXTRACT_CONFIG` section.
> 3. Change `scenario_type` from `ScenarioType.ASSISTANT` to `ScenarioType.GROUP_CHAT`.
> 4. Run the extraction script again.
>
> You can run either one or both scenarios.

**Step 2: Chat with Memory**

After extracting memories, start the interactive chat demo:

```bash
# Terminal 2: Run the chat program (ensure API server is still running)
uv run python src/bootstrap.py demo/chat_with_memory.py
```

This will launch a command-line interface where you can converse with an agent that utilizes the just-extracted memories. For more details on chat features, tips, and suggested questions, please see the [Demo Guide](demo/README.md).

**Interactive Workflow:**
1. **Select Language**: Choose between Chinese (ä¸­æ–‡) or English interface.
2. **Select Scenario Mode**:
   - **Assistant Mode**: One-on-one conversation with personal memory-based AI assistant.
   - **Group Chat Mode**: Multi-person chat with group memory-based conversation analysis.
3. **Select Conversation Group**: Choose from available groups in your database.
4. **Start Chatting**: Interact with the memory-enhanced AI agent.

---

#### ğŸ“Š Run Evaluation: Performance Testing

The evaluation framework provides a systematic way to measure the performance of the memory system, based on the LoCoMo evaluation dataset.

```bash
# Stage 1: MemCell Extraction
uv run python src/bootstrap.py evaluation/locomo_evaluation/stage1_memcells_extraction.py

# Stage 2: Index Building
uv run python src/bootstrap.py evaluation/locomo_evaluation/stage2_index_building.py

# Stage 3: Memory Retrieval
uv run python src/bootstrap.py evaluation/locomo_evaluation/stage3_memory_retrivel.py

# Stage 4: Response Generation
uv run python src/bootstrap.py evaluation/locomo_evaluation/stage4_response.py

# Stage 5: Evaluation
uv run python src/bootstrap.py evaluation/locomo_evaluation/stage5_eval.py
```

Each script corresponds to a stage in the evaluation pipeline, from data processing to performance scoring.

> **âš™ï¸ Evaluation Configuration**:
> Before running the evaluation, you can modify the `evaluation/locomo_evaluation/config.py` file to adjust the experiment settings:
> - **`ExperimentConfig.experiment_name`**: Change this to alter the save directory for the results.
> - **`ExperimentConfig.llm_service`**: Select the LLM service to use and configure its parameters (e.g., `"openai"` or `"vllm"`).
> - **`ExperimentConfig.llm_config`**: Configure parameters for the selected LLM service in this dictionary, such as the model, `base_url`, and `api_key`.

---

#### ğŸ”Œ Call API Endpoints

Use V3 API to store single message memory:

```bash
curl -X POST http://localhost:1995/api/v3/agentic/memorize \
  -H "Content-Type: application/json" \
  -d '{
    "message_id": "msg_001",
    "create_time": "2025-02-01T10:00:00+08:00",
    "sender": "user_103",
    "sender_name": "Chen",
    "content": "We need to complete the product design this week",
    "group_id": "group_001",
    "group_name": "Project Discussion Group"
  }'
```

---

#### ğŸ“¦ Batch Store Group Chat Memory

EverMemOS supports a standardized group chat data format ([GroupChatFormat](data_format/group_chat/group_chat_format.md)). You can use scripts for batch storage:

```bash
# Use script for batch storage
uv run python src/bootstrap.py src/run_memorize.py \
  --input data/group_chat.json \
  --api-url http://localhost:1995/api/v3/agentic/memorize

# Validate file format
uv run python src/bootstrap.py src/run_memorize.py \
  --input data/group_chat.json \
  --validate-only
```

**GroupChatFormat Example**:

```json
{
  "version": "1.0.0",
  "conversation_meta": {
    "group_id": "group_001",
    "name": "Project Discussion Group",
    "user_details": {
      "user_101": {
        "full_name": "Alice",
        "role": "Product Manager"
      }
    }
  },
  "conversation_list": [
    {
      "message_id": "msg_001",
      "create_time": "2025-02-01T10:00:00+08:00",
      "sender": "user_101",
      "content": "Good morning everyone"
    }
  ]
}
```

For complete format specifications, please refer to [Group Chat Format Specification](data_format/group_chat/group_chat_format.md).

### More Details

For detailed installation, configuration, and usage instructions, please refer to:
- ğŸ“š [Quick Start Guide](docs/dev_docs/getting_started.md) - Complete installation and configuration steps
- ğŸ“– [API Usage Guide](docs/dev_docs/api_usage_guide.md) - API endpoints and data format details
- ğŸ”§ [Development Guide](docs/dev_docs/development_guide.md) - Architecture design and development best practices
- ğŸš€ [Bootstrap Usage](docs/dev_docs/bootstrap_usage.md) - Script runner usage instructions
- ğŸ“ [Group Chat Format Specification](data_format/group_chat/group_chat_format.md) - Standardized data format

## ğŸ“š Documentation

### Developer Docs
- [Quick Start Guide](docs/dev_docs/getting_started.md) - Installation, configuration, and startup
- [Development Guide](docs/dev_docs/development_guide.md) - Architecture design and best practices
- [Bootstrap Usage](docs/dev_docs/bootstrap_usage.md) - Script runner
- [Dependency Management](docs/dev_docs/project_deps_manage.md) - Package management and version control

### API Documentation
- [Agentic V3 API](docs/api_docs/agentic_v3_api.md) - Agentic layer API
- [Agentic V2 API](docs/api_docs/agentic_v2_api.md) - Agentic layer API (legacy)

### Core Framework
- [Dependency Injection Framework](src/core/di/README.md) - DI container usage guide

### Demos & Evaluation
- [ğŸ“– Demo Guide](demo/README.md) - Interactive examples and memory extraction demos
- [ğŸ“Š Data Guide](data/README.md) - Sample conversation data and format specifications
- [ğŸ“Š Evaluation Guide](evaluation/locomo_evaluation/README.md) - Testing EverMemOS-based methods on the public LoCoMo dataset

## ğŸ—ï¸ Architecture Design

EverMemOS adopts a layered architecture design, mainly including:

- **Agentic Layer**: Memory extraction, vectorization, retrieval, and reranking
- **Memory Layer**: MemCell extraction, episodic memory management
- **Retrieval Layer**: Multi-modal retrieval and result ranking
- **Business Layer**: Business logic and data operations
- **Infrastructure Layer**: Database, cache, message queue adapters, etc.
- **Core Framework**: Dependency injection, middleware, queue management, etc.

For more architectural details, please refer to the [Development Guide](docs/dev_docs/development_guide.md).

## ğŸ¤ Contributing

We welcome all forms of contributions! Whether it's reporting bugs, proposing new features, or submitting code improvements.

Before contributing, please read our [Contributing Guide](CONTRIBUTING.md) to learn about:
- Development environment setup
- Code standards and best practices
- Git commit conventions (Gitemoji)
- Pull Request process

## ğŸŒŸ Join Us

<!-- 
This section can include:
- Community communication channels (Discord, Slack, WeChat groups, etc.)
- Technical discussion forums
- Regular meeting information
- Contact email
-->

We are building a vibrant open-source community!

### Contact

- **GitHub Issues**: [Submit issues and suggestions](https://github.com/your-org/memsys_opensource/issues)
- **Discussions**: [Join discussions](https://github.com/your-org/memsys_opensource/discussions)
- **Email**: [Contact email to be added]
- **Community**: [Community link to be added]

### Contributors

Thanks to all the developers who have contributed to this project!

<!-- Can use GitHub Contributors auto-generation -->
<!-- <a href="https://github.com/your-org/memsys_opensource/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=your-org/memsys_opensource" />
</a> -->

## ğŸ“„ License

This project is licensed under the [Apache License 2.0](LICENSE). This means you are free to use, modify, and distribute this project, with the following key conditions:
- You must include a copy of the Apache 2.0 license
- You must state any significant changes made to the code
- You must retain all copyright, patent, trademark, and attribution notices
- If a NOTICE file is included, you must include it in your distribution

## ğŸ™ Acknowledgments

<!-- 
This section can include:
- Projects that inspired us
- Open-source libraries used
- Supporting organizations or individuals
-->

Thanks to the following projects and communities for their inspiration and support:

- (To be added)

---

<div align="center">

**If this project helps you, please give us a â­ï¸**

Made with â¤ï¸ by the EverMemOS Team

</div>
